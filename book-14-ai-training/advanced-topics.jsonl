{"cluster_id": "ai-advanced-001", "question": "What is smoothness regularization in LDA?", "answer": "When clusters have few training examples, projections can overfit. **Smoothness regularization** encourages similar clusters to have similar projections.\n\n**Graph Laplacian approach:**\n```\n# Cluster similarity graph (mathematical notation)\nS[i,j] = cosine_sim(centroid_i, centroid_j)\n\n# Graph Laplacian\nD = diag(sum(S, axis=1))  # Degree matrix\nL = D - S                  # Laplacian\n\n# Regularized loss\nLoss = MSE(pred, target) + λ * sum_ij S[i,j] * ||W_i - W_j||²\n     = MSE(pred, target) + λ * trace(W^T L W)\n```\n\nThis encourages nearby clusters (in embedding space) to have similar answer projections.\n\n**When to use:**\n- Clusters with < 5 training questions\n- High variance in per-cluster performance\n- Domain knowledge suggests cluster similarity", "source_file": "book-14-ai-training/08_advanced_topics.md", "prerequisites": ["ai-multihead-001"]}
{"cluster_id": "ai-advanced-001", "question": "What are hierarchical transformer ensembles?", "answer": "For very large cluster counts (500+), use multiple transformers with a meta-router:\n\n```\n                ┌─────────────────┐\n                │  Meta-Router    │\n                │  (small MLP)    │\n                └────────┬────────┘\n                         │\n       ┌─────────────────┼─────────────────┐\n       ▼                 ▼                 ▼\n┌──────────────┐  ┌──────────────┐  ┌──────────────┐\n│ Transformer  │  │ Transformer  │  │ Transformer  │\n│   Domain A   │  │   Domain B   │  │   Domain C   │\n│ (clusters    │  │ (clusters    │  │ (clusters    │\n│  1-100)      │  │  101-200)    │  │  201-300)    │\n└──────────────┘  └──────────────┘  └──────────────┘\n```\n\n**Domain assignment via:**\n1. K-means on centroids (automatic)\n2. Manual labels (\"auth\", \"data\", \"performance\")\n3. Hierarchical clustering (tree cut)\n\n**Meta-router:**\n```python\ndomain_weights = softmax(meta_router(query))  # [0.8, 0.15, 0.05]\nprojected = sum(domain_weights[d] * transformer_d(query) for d in domains)\n```", "source_file": "book-14-ai-training/08_advanced_topics.md", "prerequisites": ["ai-distill-001"]}
{"cluster_id": "ai-advanced-001", "question": "What scaling strategies exist for different LDA head counts?", "answer": "| Scale | Strategy |\n|-------|---------|\n| Moderate (<100 heads) | Flat LDA multi-head |\n| Large (100-500 heads) | Consider transformer distillation |\n| Very Large (500-2000) | Hierarchical ensemble |\n| Massive (2000+) | ANN + hierarchical + sparse routing |\n\n**Future directions:**\n\n1. **Online Learning:** Update projections incrementally\n2. **Mixture of Experts (MoE):** Sparse routing with top-k selection\n3. **Contrastive Training:** Learn embeddings directly for Q-A matching\n4. **Cross-Lingual Projection:** Work across languages with multilingual embeddings\n5. **Graph-Aware Routing:** Use knowledge graph structure for routing", "source_file": "book-14-ai-training/08_advanced_topics.md", "prerequisites": ["ai-advanced-001"]}
