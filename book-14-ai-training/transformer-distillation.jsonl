{"cluster_id": "ai-distill-001", "question": "What is the H^L = N equivalence conjecture?", "answer": "A transformer with H attention heads per layer and L layers has routing capacity equivalent to H^L flat LDA heads.\n\n```\nFlat LDA:    N independent heads\nTransformer: H heads × L layers → H^L effective routing patterns\n```\n\n**Why H^L?**\nEach layer routes through H attention patterns. Layers compose sequentially:\n```\nLayer 1: H choices\nLayer 2: H choices (for each Layer 1 choice)\n...\nLayer L: H choices\n\nTotal combinations: H × H × ... × H = H^L\n```\n\n**Examples:**\n| Flat LDA Heads (N) | Transformer H | Transformer L | Equivalent |\n|-------------------|---------------|---------------|------------|\n| 16 | 4 | 2 | 4² = 16 |\n| 64 | 4 | 3 | 4³ = 64 |\n| 256 | 4 | 4 | 4⁴ = 256 |\n\n**Optimal H ≈ 4** for practical efficiency.", "source_file": "book-14-ai-training/06_transformer_distillation.md", "prerequisites": ["ai-multihead-001"]}
{"cluster_id": "ai-distill-001", "question": "How do you train a transformer via distillation from LDA?", "answer": "Train the transformer to match LDA outputs using **MSE + cosine loss**:\n\n```\n# Loss function (mathematical notation)\nL = (1 - λ) × MSE(pred, target) + λ × (1 - cosine_sim(pred, target))\n\n# Recommended: λ = 0.7 (cosine-weighted)\n```\n\n**CRITICAL:** MSE alone achieves low error but wrong direction. Cosine loss ensures directional alignment.\n\n```python\nfrom projection_transformer import ProjectionTransformer, train_distillation\n\n# Create transformer matching LDA capacity\ntransformer = ProjectionTransformer(\n    embed_dim=384,\n    num_heads=4,      # H\n    num_layers=2,     # L → 4² = 16 equivalent heads\n    ff_dim=512\n)\n\n# Train via distillation\ntrain_distillation(\n    transformer=transformer,\n    lda_projection=lda,           # Teacher (multi-head LDA)\n    query_embeddings=train_data,\n    num_epochs=200,\n    cosine_weight=0.7             # Essential!\n)\n```\n\n**Validation results (N=18 LDA heads):**\n- Mean Cosine Similarity: 0.9928 ± 0.0027\n- **99.28% cosine similarity** validates the equivalence conjecture.", "source_file": "book-14-ai-training/06_transformer_distillation.md", "prerequisites": ["ai-distill-001"]}
{"cluster_id": "ai-distill-001", "question": "When should I use transformer distillation vs flat LDA?", "answer": "**Use transformer when:**\n- N > 400 heads (single query) or N > 30 (batched)\n- Memory constrained (mobile/edge)\n- Natural domain structure for hierarchical routing\n\n**Stay with LDA when:**\n- N < 400 heads (most cases)\n- Interpretability matters (LDA weights are meaningful)\n- Clusters change frequently (retraining is expensive)\n\n**Latency analysis (N=18 heads, 1000 queries):**\n| Method | Single Query | Batch (32) |\n|--------|-------------|------------|\n| LDA (NumPy CPU) | 0.046 ms | 0.046 ms |\n| Transformer (CUDA) | 1.110 ms | 0.069 ms |\n\n**Crossover points:**\n- Single query: >400 heads (LDA wins below)\n- Batched (32): ~27 heads\n\n**Key insight:** At small scale, LDA wins due to GPU kernel overhead. Transformer benefits emerge at larger scale where O(N) softmax becomes expensive.", "source_file": "book-14-ai-training/06_transformer_distillation.md", "prerequisites": ["ai-distill-001"]}
