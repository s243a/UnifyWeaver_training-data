{"cluster_id": "stream-compilation", "seed_level": 2, "source_files": ["education/book-02-bash-target/02_stream_compilation.md", "src/unifyweaver/core/stream_compiler.pl"], "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "questions": ["How does UnifyWeaver compile non-recursive predicates?", "What is stream compilation in UnifyWeaver?", "How are Prolog joins compiled to Bash pipelines?", "What does the stream_compiler generate?"], "answer": {"text": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "anchor_question_hash": "c4d9e3f2a1b87540"}}
{"cluster_id": "bash-pipeline-patterns", "seed_level": 3, "source_files": ["education/book-02-bash-target/02_stream_compilation.md", "src/unifyweaver/core/stream_compiler.pl"], "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "questions": ["Why does UnifyWeaver use Unix pipelines?", "What are the benefits of stream-based compilation?", "How is memory managed in generated Bash code?", "What makes stream compilation efficient?"], "answer": {"text": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "anchor_question_hash": "d2e8f4a1c3b96750"}}
