{"pair_id": "b4-c3-multi-target-pipelines_p0", "cluster_id": "b4-c3-multi-target-pipelines", "question": "How do multi-target pipelines work in UnifyWeaver?", "question_type": "medium", "answer": "UnifyWeaver's multi-target pipelines allow you to chain stages where each stage compiles to a different target language, letting you leverage the strengths of each language within a single workflow.\n\n### Pipeline Structure\n\nA typical multi-target pipeline might look like:\n```\nAWK \u2192 Python \u2192 Go\n```\n\nEach stage is compiled to the language best suited for its task:\n- **AWK** handles text parsing and field extraction\n- **Python** performs complex transformations and ML integration\n- **Go** executes high-performance aggregation\n\n### Orchestration\n\nDefine your pipeline in Prolog, specifying which target each stage uses:\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow Between Stages\n\n1. Each stage produces output serialized to JSON or CSV\n2. The orchestrator routes this output to the next stage's input\n3. The receiving stage deserializes, processes, and produces its own output\n4. This continues until reaching the final stage\n\n### Key Advantages\n\n- Choose the optimal language for each processing step\n- Write the entire pipeline specification in Prolog\n- Data marshaling between stages happens automatically\n\nFor more on workflow patterns and orchestration, see education/book-04-workflows/.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-orchestration", "multi-target", "stage-selection"], "original_answer": "## Multi-Target Pipelines\n\nMulti-target pipelines combine stages compiled to different languages:\n\n```\nAWK \u2192 Python \u2192 Go\n```\n\n### How It Works\n\nEach stage is compiled to its optimal target:\n- **AWK**: Text parsing, field extraction\n- **Python**: Complex transforms, ML integration\n- **Go**: High-performance aggregation\n\n### Pipeline Orchestration\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow\n\n1. Stage outputs \u2192 serialized (JSON/CSV)\n2. Orchestrator routes to next stage\n3. Next stage reads, processes, outputs\n4. Continues until final stage\n\n### Benefits\n\n- Leverage each language's strengths\n- Unified Prolog specification\n- Automatic data marshaling", "answer_tailored": true}
{"pair_id": "b4-c3-multi-target-pipelines_p1", "cluster_id": "b4-c3-multi-target-pipelines", "question": "How do you combine different target languages in a pipeline?", "question_type": "medium", "answer": "UnifyWeaver allows you to combine different target languages in a pipeline by using a multi-target orchestration approach. Each pipeline stage is compiled to the language best suited for its task, and the orchestrator handles data flow between stages.\n\n## Target Selection Per Stage\n\nChoose the optimal language for each operation:\n- **AWK**: Field extraction and text parsing\n- **Python**: Complex transformations and ML tasks\n- **Go**: High-performance data aggregation\n\n## Orchestration Pattern\n\nDefine your pipeline with target assignments:\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n## Cross-Language Data Flow\n\nThe orchestrator manages inter-stage communication:\n1. Each stage outputs serialized data (JSON/CSV format)\n2. Orchestrator routes output to the next stage's input\n3. Subsequent stage deserializes, processes, and emits results\n4. Process repeats through the final stage\n\n## Key Advantages\n\n- Exploit language-specific strengths for each operation\n- Single Prolog specification for the entire pipeline\n- Automatic data serialization and routing between stages\n\nFor implementation examples, see education/book-04-workflows/03_pipeline_orchestration.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-orchestration", "multi-target", "stage-selection"], "original_answer": "## Multi-Target Pipelines\n\nMulti-target pipelines combine stages compiled to different languages:\n\n```\nAWK \u2192 Python \u2192 Go\n```\n\n### How It Works\n\nEach stage is compiled to its optimal target:\n- **AWK**: Text parsing, field extraction\n- **Python**: Complex transforms, ML integration\n- **Go**: High-performance aggregation\n\n### Pipeline Orchestration\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow\n\n1. Stage outputs \u2192 serialized (JSON/CSV)\n2. Orchestrator routes to next stage\n3. Next stage reads, processes, outputs\n4. Continues until final stage\n\n### Benefits\n\n- Leverage each language's strengths\n- Unified Prolog specification\n- Automatic data marshaling", "answer_tailored": true}
{"pair_id": "b4-c3-multi-target-pipelines_p2", "cluster_id": "b4-c3-multi-target-pipelines", "question": "What is cross-target pipeline orchestration?", "question_type": "medium", "answer": "Cross-target pipeline orchestration in UnifyWeaver enables you to build data processing workflows where different stages execute in different compiled languages, each chosen for its strengths. For example, you might parse text with AWK, apply transformations in Python, and perform aggregations in Go\u2014all from a single Prolog specification.\n\nThe orchestrator manages the entire workflow through a declarative definition:\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\nData flows automatically between stages: each stage's output is serialized (typically as JSON or CSV), routed by the orchestrator to the next stage, which reads and processes it before producing its own output. This continues until the pipeline completes.\n\nKey advantages include leveraging language-specific capabilities (AWK for text processing, Python for ML integration, Go for performance), maintaining a unified Prolog-based definition, and automatic data marshaling between stages. See education/book-04-workflows/03_pipeline_orchestration.md for implementation details.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-orchestration", "multi-target", "stage-selection"], "original_answer": "## Multi-Target Pipelines\n\nMulti-target pipelines combine stages compiled to different languages:\n\n```\nAWK \u2192 Python \u2192 Go\n```\n\n### How It Works\n\nEach stage is compiled to its optimal target:\n- **AWK**: Text parsing, field extraction\n- **Python**: Complex transforms, ML integration\n- **Go**: High-performance aggregation\n\n### Pipeline Orchestration\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow\n\n1. Stage outputs \u2192 serialized (JSON/CSV)\n2. Orchestrator routes to next stage\n3. Next stage reads, processes, outputs\n4. Continues until final stage\n\n### Benefits\n\n- Leverage each language's strengths\n- Unified Prolog specification\n- Automatic data marshaling", "answer_tailored": true}
{"pair_id": "b4-c3-target-selection_p0", "cluster_id": "b4-c3-target-selection", "question": "How do you choose which target language for each pipeline stage?", "question_type": "medium", "answer": "When selecting target languages for pipeline stages, consider these key factors:\n\n**Match Language to Task Type:**\n- **AWK**: Choose for line-by-line text parsing and CSV processing - it's fast and handles streaming data efficiently\n- **Python**: Use when you need complex logic, ML inference, or extensive library support (especially for JSON handling)\n- **Go**: Select for high-performance aggregation and processing of large datasets (1M+ records)\n- **C#**: Ideal for recursive queries and fixpoint computations, particularly when using .NET libraries\n- **Bash**: Best for system integration and gluing pipeline stages together\n\n**Selection Criteria:**\n1. **Data format**: Text/CSV streams \u2192 AWK; JSON/complex structures \u2192 Python\n2. **Performance requirements**: Speed-critical operations \u2192 Go or C#\n3. **Algorithm complexity**: Simple transformations \u2192 AWK; sophisticated logic \u2192 Python\n4. **Library dependencies**: Machine learning \u2192 Python; .NET ecosystem \u2192 C#\n5. **Deployment environment**: Unix/Linux systems \u2192 Bash; cross-platform \u2192 .NET targets\n\n**Quick Decision Guide:**\n- Parsing CSV files? \u2192 AWK\n- Running ML models? \u2192 Python  \n- Processing millions of records? \u2192 Go\n- Implementing recursive queries? \u2192 C# Query Runtime\n- Connecting stages together? \u2192 Bash\n\nFor detailed examples of multi-stage pipelines, see education/book-04-workflows/03_pipeline_orchestration.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["target-selection", "language-strengths", "optimization"], "original_answer": "## Target Selection Criteria\n\n### Language Strengths\n\n| Target | Best For | Characteristics |\n|--------|----------|------------------|\n| AWK | Line parsing | Fast, streaming |\n| Python | Complex logic | Flexible, libraries |\n| Go | Aggregation | Fast, compiled |\n| C# | Recursive queries | Fixpoint, .NET |\n| Bash | Glue code | System integration |\n\n### Selection Factors\n\n1. **Input/Output format**: AWK for text, Python for JSON\n2. **Complexity**: Simple \u2192 AWK, Complex \u2192 Python\n3. **Performance**: Critical \u2192 Go/C#\n4. **Libraries**: ML \u2192 Python, .NET \u2192 C#\n5. **Deployment**: Unix \u2192 Bash, Cross-platform \u2192 .NET\n\n### Decision Example\n\n```\nParsing CSV? \u2192 AWK\nML inference? \u2192 Python\n1M+ records? \u2192 Go\nRecursive? \u2192 C# Query Runtime\n```", "answer_tailored": true}
{"pair_id": "b4-c3-target-selection_p1", "cluster_id": "b4-c3-target-selection", "question": "When should you use AWK vs Python vs Go in a pipeline?", "question_type": "medium", "answer": "The choice between AWK, Python, and Go in a pipeline depends on your data processing needs:\n\n**AWK** excels at line-by-line text parsing and transformation. Choose it when you're working with CSV, TSV, or other line-oriented text formats. It's fast, streams data efficiently, and keeps pipelines lightweight.\n\n**Python** is ideal for complex processing logic, especially when you need extensive libraries (JSON parsing, ML inference, API interactions). It offers flexibility and rich ecosystem support, making it the go-to for sophisticated transformations.\n\n**Go** shines when performance is critical, particularly for aggregation operations on large datasets (1M+ records). As a compiled language, it delivers speed comparable to C# but with simpler deployment in Unix environments.\n\n**Quick decision guide:**\n- Text parsing (CSV/logs) \u2192 AWK\n- JSON processing or ML \u2192 Python  \n- High-volume aggregation \u2192 Go\n- Recursive/fixpoint queries \u2192 C# Query Runtime\n\n**Other considerations:**\n- Deployment context: Unix-centric systems favor AWK/Bash glue, cross-platform needs may require .NET/C#\n- Library requirements: ML workloads need Python, .NET integration requires C#\n\nFor comprehensive pipeline patterns, see education/book-04-workflows/03_pipeline_orchestration.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["target-selection", "language-strengths", "optimization"], "original_answer": "## Target Selection Criteria\n\n### Language Strengths\n\n| Target | Best For | Characteristics |\n|--------|----------|------------------|\n| AWK | Line parsing | Fast, streaming |\n| Python | Complex logic | Flexible, libraries |\n| Go | Aggregation | Fast, compiled |\n| C# | Recursive queries | Fixpoint, .NET |\n| Bash | Glue code | System integration |\n\n### Selection Factors\n\n1. **Input/Output format**: AWK for text, Python for JSON\n2. **Complexity**: Simple \u2192 AWK, Complex \u2192 Python\n3. **Performance**: Critical \u2192 Go/C#\n4. **Libraries**: ML \u2192 Python, .NET \u2192 C#\n5. **Deployment**: Unix \u2192 Bash, Cross-platform \u2192 .NET\n\n### Decision Example\n\n```\nParsing CSV? \u2192 AWK\nML inference? \u2192 Python\n1M+ records? \u2192 Go\nRecursive? \u2192 C# Query Runtime\n```", "answer_tailored": true}
{"pair_id": "b4-c3-target-selection_p2", "cluster_id": "b4-c3-target-selection", "question": "What are the criteria for target selection?", "question_type": "medium", "answer": "When selecting a target language for UnifyWeaver compilation, consider these key factors:\n\n**Language Strengths by Use Case:**\n- **AWK**: Optimal for line-by-line text parsing with fast streaming capabilities\n- **Python**: Best for complex logic and leveraging extensive libraries\n- **Go**: Ideal for aggregation tasks requiring compiled performance\n- **C#**: Suited for recursive queries with fixpoint semantics and .NET integration\n- **Bash**: Perfect for system glue code and integration scripts\n\n**Primary Selection Criteria:**\n\n1. **Data Format**: Choose AWK for text processing, Python for JSON handling\n2. **Query Complexity**: AWK handles simple operations; Python manages intricate logic\n3. **Performance Requirements**: Go or C# deliver speed for performance-critical workloads\n4. **Library Dependencies**: Python provides ML frameworks; C# offers .NET ecosystem access\n5. **Deployment Environment**: Bash targets Unix systems; .NET enables cross-platform deployment\n\n**Quick Decision Guide:**\n- CSV parsing \u2192 AWK\n- Machine learning inference \u2192 Python\n- Processing 1M+ records \u2192 Go\n- Recursive/fixpoint queries \u2192 C# Query Runtime\n\nFor detailed target-specific patterns and examples, see education/book-02-bash-target/ through book-07-sql-target/.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["target-selection", "language-strengths", "optimization"], "original_answer": "## Target Selection Criteria\n\n### Language Strengths\n\n| Target | Best For | Characteristics |\n|--------|----------|------------------|\n| AWK | Line parsing | Fast, streaming |\n| Python | Complex logic | Flexible, libraries |\n| Go | Aggregation | Fast, compiled |\n| C# | Recursive queries | Fixpoint, .NET |\n| Bash | Glue code | System integration |\n\n### Selection Factors\n\n1. **Input/Output format**: AWK for text, Python for JSON\n2. **Complexity**: Simple \u2192 AWK, Complex \u2192 Python\n3. **Performance**: Critical \u2192 Go/C#\n4. **Libraries**: ML \u2192 Python, .NET \u2192 C#\n5. **Deployment**: Unix \u2192 Bash, Cross-platform \u2192 .NET\n\n### Decision Example\n\n```\nParsing CSV? \u2192 AWK\nML inference? \u2192 Python\n1M+ records? \u2192 Go\nRecursive? \u2192 C# Query Runtime\n```", "answer_tailored": true}
{"pair_id": "b4-c3-pipeline-composition_p0", "cluster_id": "b4-c3-pipeline-composition", "question": "How do pipeline stages communicate in UnifyWeaver?", "question_type": "medium", "answer": "Pipeline stages in UnifyWeaver communicate through standard I/O streams and file-based data exchange. The primary method uses pipes where one stage's stdout connects directly to the next stage's stdin, creating a data flow chain. For complex orchestration, stages can also communicate via temporary files.\n\nThe data formats used depend on the complexity: JSON for nested structured data, CSV/TSV for tabular datasets, and line-delimited text for simple record streams. Each stage reads input from stdin (or a specified file), processes it, and writes results to stdout.\n\nThe orchestrator manages this communication by parsing stage outputs, validating data formats, and routing information to subsequent stages. It also handles the error channel (stderr or dedicated error files) separately from the main data flow, enabling proper error detection and recovery without corrupting the pipeline data.\n\nFor error handling, the orchestrator implements retry logic for failed stages, can invoke fallback stages when primary stages fail, and enforces error thresholds to stop execution after reaching a configured number of failures. This ensures robust pipeline execution even when individual stages encounter issues.\n\nFor more details on workflow patterns, see education/book-04-workflows/.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-composition", "data-flow", "serialization"], "original_answer": "## Stage Communication\n\n### Data Formats\n\nStages communicate via standard I/O using:\n- **JSON**: Complex nested data\n- **CSV/TSV**: Tabular data\n- **Line-delimited**: Simple records\n\n### Data Flow\n\n```\nStage A (stdout) \u2192 | pipe | \u2192 Stage B (stdin)\n```\n\nOr via temporary files:\n```\nStage A \u2192 /tmp/stage_a.json \u2192 Stage B\n```\n\n### Error Handling\n\n1. **Error channel**: stderr or error files\n2. **Retry logic**: Orchestrator retries failed stages\n3. **Fallback**: Alternative stage on failure\n4. **Thresholds**: Stop after N errors\n\n### Orchestrator Role\n\n- Parses stage output\n- Validates formats\n- Routes to next stage\n- Manages error recovery", "answer_tailored": true}
{"pair_id": "b4-c3-pipeline-composition_p1", "cluster_id": "b4-c3-pipeline-composition", "question": "What data formats are used between pipeline stages?", "question_type": "medium", "answer": "Pipeline stages in UnifyWeaver communicate using several data formats depending on the complexity of the data being passed:\n\n**JSON** is used for complex nested data structures that need to preserve hierarchy and type information between stages.\n\n**CSV/TSV** (comma or tab-separated values) handles tabular data efficiently when stages exchange structured records.\n\n**Line-delimited formats** work well for simple records where each line represents a single data item.\n\nThese formats flow between stages either through standard I/O pipes (where one stage's stdout connects to the next stage's stdin) or via temporary files (like `/tmp/stage_a.json`) when intermediate persistence is needed. The orchestrator parses and validates these formats to ensure proper data routing between pipeline stages.\n\nFor more details on pipeline orchestration patterns, see `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-composition", "data-flow", "serialization"], "original_answer": "## Stage Communication\n\n### Data Formats\n\nStages communicate via standard I/O using:\n- **JSON**: Complex nested data\n- **CSV/TSV**: Tabular data\n- **Line-delimited**: Simple records\n\n### Data Flow\n\n```\nStage A (stdout) \u2192 | pipe | \u2192 Stage B (stdin)\n```\n\nOr via temporary files:\n```\nStage A \u2192 /tmp/stage_a.json \u2192 Stage B\n```\n\n### Error Handling\n\n1. **Error channel**: stderr or error files\n2. **Retry logic**: Orchestrator retries failed stages\n3. **Fallback**: Alternative stage on failure\n4. **Thresholds**: Stop after N errors\n\n### Orchestrator Role\n\n- Parses stage output\n- Validates formats\n- Routes to next stage\n- Manages error recovery", "answer_tailored": true}
{"pair_id": "b4-c3-pipeline-composition_p2", "cluster_id": "b4-c3-pipeline-composition", "question": "How do you handle errors across pipeline stages?", "question_type": "medium", "answer": "Error handling across pipeline stages in UnifyWeaver involves multiple layers:\n\n**Error Channels**: Stages report errors through stderr or dedicated error files, separate from the main data flow.\n\n**Retry Logic**: The orchestrator automatically retries failed stages based on configurable policies.\n\n**Fallback Stages**: You can define alternative stages that execute when primary stages fail, providing graceful degradation.\n\n**Error Thresholds**: Set limits (e.g., stop pipeline after N consecutive errors) to prevent cascading failures.\n\n**Orchestrator Responsibilities**: The orchestrator monitors stage exit codes, parses error output, validates data formats between stages, and decides whether to retry, fallback, or halt the pipeline.\n\nStages communicate via standard I/O (JSON for complex data, CSV/TSV for tabular, line-delimited for simple records) or temporary files, with the orchestrator managing the error recovery flow. See education/book-04-workflows/03_pipeline_orchestration.md for complete orchestration patterns.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-composition", "data-flow", "serialization"], "original_answer": "## Stage Communication\n\n### Data Formats\n\nStages communicate via standard I/O using:\n- **JSON**: Complex nested data\n- **CSV/TSV**: Tabular data\n- **Line-delimited**: Simple records\n\n### Data Flow\n\n```\nStage A (stdout) \u2192 | pipe | \u2192 Stage B (stdin)\n```\n\nOr via temporary files:\n```\nStage A \u2192 /tmp/stage_a.json \u2192 Stage B\n```\n\n### Error Handling\n\n1. **Error channel**: stderr or error files\n2. **Retry logic**: Orchestrator retries failed stages\n3. **Fallback**: Alternative stage on failure\n4. **Thresholds**: Stop after N errors\n\n### Orchestrator Role\n\n- Parses stage output\n- Validates formats\n- Routes to next stage\n- Manages error recovery", "answer_tailored": true}
