{"pair_id": "ai-lda-001_p0", "cluster_id": "ai-lda-001", "question": "What is LDA projection and how does it work?", "question_type": "medium", "answer": "LDA projection is a technique that learns to transform question embeddings into a space where they're more similar to their corresponding answer embeddings. It works by training a projection matrix W that maps queries closer to relevant documents.\n\n**How it works:**\nThe system learns a matrix W through training, then applies it to transform query embeddings:\n```\nprojected_query = query_embedding @ W\n```\n\n**Matrix dimensions:**\n- Input query embedding: (d,) - typically 384-dimensional vectors\n- Projection matrix W: (d, d) - learned from training data\n- Output projected query: (d,) - transformed to match answer space\n\n**Effectiveness:**\nThis transformation significantly improves retrieval accuracy. For instance, a query like \"How do I read a CSV file?\" might have only 0.45 similarity to a document about \"pandas.read_csv() function\" using raw embeddings, but after LDA projection the similarity jumps to 0.89. In practice, this often increases recall from around 70% to 93%.\n\nFor implementation details, see education/book-14-ai-training/03_lda_projection.md.", "answer_variant": "default", "answer_source": "book-14-ai-training/03_lda_projection.md", "topics": [], "original_answer": "LDA projection learns a matrix W that transforms question embeddings to be closer to their answer embeddings:\n\n```\nprojected_query = query_embedding @ W\n```\n\n**Dimensions:**\n- `query_embedding`: shape (d,) - e.g., 384 dimensions\n- `W`: shape (d, d) - the learned projection\n- `projected_query`: shape (d,) - now closer to answer space\n\n**Example improvement:**\n```\nQuery:    \"How do I read a CSV file?\"\nDocument: \"The pandas.read_csv() function loads comma-separated...\"\n\nDirect similarity: 0.45 (mediocre)\nAfter projection:  0.89 (much better)\n```\n\nTypical recall improvement: 70% \u2192 93%.", "answer_tailored": true}
{"pair_id": "ai-lda-001_p0", "cluster_id": "ai-lda-001", "question": "How is the W matrix computed?", "question_type": "medium", "answer": "The W matrix is computed using the **Moore-Penrose pseudoinverse** to find the optimal linear transformation from question embeddings to answer embeddings.\n\n**Mathematical formulation:**\n```\nQ = question embeddings matrix (n \u00d7 d)\nA = answer embeddings matrix (n \u00d7 d)\nW = pinv(Q) @ A\n```\n\nThis finds W that minimizes the reconstruction error ||Q @ W - A||\u00b2.\n\n**Implementation with regularization:**\n```python\nimport numpy as np\n\ndef compute_W(question_embeddings, answer_embeddings, regularization=1e-6):\n    Q = question_embeddings\n    A = answer_embeddings\n    d = Q.shape[1]\n    \n    # Regularized form: (Q^T Q + \u03bbI)^-1 Q^T\n    QTQ = Q.T @ Q + regularization * np.eye(d)\n    W = np.linalg.solve(QTQ, Q.T @ A)\n    \n    return W\n```\n\n**Advantages of pseudoinverse:**\n- Handles non-square and rank-deficient matrices\n- Provides least-squares optimal solution\n- Numerically stable when regularized\n\nSee education/book-14-ai-training/03_lda_projection.md for the complete LDA projection methodology.", "answer_variant": "default", "answer_source": "book-14-ai-training/03_lda_projection.md", "topics": [], "original_answer": "Using the Moore-Penrose **pseudoinverse**:\n\n```\nQ = stack of question embeddings  (n \u00d7 d)\nA = stack of answer embeddings    (n \u00d7 d)\n\nGoal: Find W such that Q @ W \u2248 A\n\nSolution: W = pinv(Q) @ A\n```\n\n**Why pseudoinverse?**\n- Minimizes ||Q @ W - A||\u00b2 (reconstruction error)\n- Works even when Q is not square or full rank\n- Numerically stable with regularization\n\n```python\nimport numpy as np\n\ndef compute_W(question_embeddings, answer_embeddings, regularization=1e-6):\n    Q = question_embeddings\n    A = answer_embeddings\n    d = Q.shape[1]\n    \n    # Regularized pseudoinverse: (Q^T Q + \u03bbI)^-1 Q^T\n    QTQ = Q.T @ Q + regularization * np.eye(d)\n    W = np.linalg.solve(QTQ, Q.T @ A)\n    \n    return W\n```", "answer_tailored": true}
{"pair_id": "ai-lda-001_p0", "cluster_id": "ai-lda-001", "question": "What's the limitation of single W matrix projection?", "question_type": "medium", "answer": "A single W matrix can only learn one global mapping from question space to answer space, which creates problems when your dataset contains diverse question types.\n\nThis works fine when all questions are semantically similar and follow one dominant pattern. But it struggles when questions span multiple domains - for example, authentication questions need different semantic relationships than CSV format questions or performance debugging questions.\n\nThe core issue: one projection matrix can't capture the different semantic structures needed for \"How do I authenticate?\" (auth domain), \"What's the CSV format?\" (data domain), and \"Why is my query slow?\" (performance domain).\n\nThe solution is multi-head routing with cluster-specific projections, covered in Chapter 4, which allows different W matrices for different question types.", "answer_variant": "default", "answer_source": "book-14-ai-training/03_lda_projection.md", "topics": [], "original_answer": "A single W matrix assumes one global mapping from questions to answers.\n\n**Works well when:**\n- Questions are semantically similar\n- There's one dominant pattern\n\n**Struggles when:**\n- Questions span diverse topics\n- Different question types need different mappings\n\n**Examples of different question types:**\n- \"How do I authenticate?\" \u2192 needs auth-domain knowledge\n- \"What's the CSV format?\" \u2192 needs data-domain knowledge\n- \"Why is my query slow?\" \u2192 needs performance-domain knowledge\n\n**Solution:** Multi-head routing (Chapter 4) with cluster-specific projections.", "answer_tailored": true}
