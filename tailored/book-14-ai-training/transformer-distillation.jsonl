{"pair_id": "ai-distill-001_p0", "cluster_id": "ai-distill-001", "question": "What is the H^L = N equivalence conjecture?", "question_type": "medium", "answer": "The H^L = N equivalence conjecture states that a transformer with H attention heads per layer and L layers has the same routing capacity as N = H^L independent LDA heads in a flat architecture.\n\n**The Mathematical Relationship:**\n```\nN flat LDA heads \u2261 H^L transformer routing patterns\nwhere H = heads per layer, L = number of layers\n```\n\n**Why the Exponential Relationship?**\nTransformer layers compose sequentially, creating exponentially many routing paths:\n- Layer 1 offers H routing choices\n- Layer 2 provides H additional choices for each Layer 1 selection\n- This continues through all L layers\n- Total distinct routing patterns: H \u00d7 H \u00d7 ... \u00d7 H (L times) = H^L\n\n**Equivalence Examples:**\n\n| N (Flat LDA) | H (Heads/Layer) | L (Layers) | H^L |\n|--------------|-----------------|------------|-----|\n| 16           | 4               | 2          | 16  |\n| 64           | 4               | 3          | 64  |\n| 256          | 4               | 4          | 256 |\n\n**Practical Implication:**\nSetting H \u2248 4 provides an efficient sweet spot, allowing deep transformers to match large flat LDA systems with fewer parameters per layer.\n\nFor implementation details, see education/book-14-ai-training/06_transformer_distillation.md.", "answer_variant": "default", "answer_source": "book-14-ai-training/06_transformer_distillation.md", "topics": [], "original_answer": "A transformer with H attention heads per layer and L layers has routing capacity equivalent to H^L flat LDA heads.\n\n```\nFlat LDA:    N independent heads\nTransformer: H heads \u00d7 L layers \u2192 H^L effective routing patterns\n```\n\n**Why H^L?**\nEach layer routes through H attention patterns. Layers compose sequentially:\n```\nLayer 1: H choices\nLayer 2: H choices (for each Layer 1 choice)\n...\nLayer L: H choices\n\nTotal combinations: H \u00d7 H \u00d7 ... \u00d7 H = H^L\n```\n\n**Examples:**\n| Flat LDA Heads (N) | Transformer H | Transformer L | Equivalent |\n|-------------------|---------------|---------------|------------|\n| 16 | 4 | 2 | 4\u00b2 = 16 |\n| 64 | 4 | 3 | 4\u00b3 = 64 |\n| 256 | 4 | 4 | 4\u2074 = 256 |\n\n**Optimal H \u2248 4** for practical efficiency.", "answer_tailored": true}
{"pair_id": "ai-distill-001_p0", "cluster_id": "ai-distill-001", "question": "How do you train a transformer via distillation from LDA?", "question_type": "medium", "answer": "To train a transformer via distillation from LDA, use a **combined MSE + cosine similarity loss** with the LDA model as the teacher:\n\n```\nL = (1 - \u03bb) \u00d7 MSE(pred, target) + \u03bb \u00d7 (1 - cosine_sim(pred, target))\n```\n\nSet **\u03bb = 0.7** to weight cosine similarity heavily. This is critical because MSE alone can minimize error while producing outputs in the wrong direction\u2014cosine loss ensures directional alignment with LDA.\n\n```python\nfrom projection_transformer import ProjectionTransformer, train_distillation\n\n# Configure transformer to match LDA dimensions\ntransformer = ProjectionTransformer(\n    embed_dim=384,\n    num_heads=4,\n    num_layers=2,     # Gives 4\u00b2 = 16 effective heads\n    ff_dim=512\n)\n\n# Distill from LDA teacher model\ntrain_distillation(\n    transformer=transformer,\n    lda_projection=lda,           # LDA teacher\n    query_embeddings=train_data,\n    num_epochs=200,\n    cosine_weight=0.7\n)\n```\n\n**Performance:** With 18 LDA heads, distillation achieves **mean cosine similarity of 0.9928 \u00b1 0.0027** (99.28%), confirming the transformer successfully learns LDA's projection behavior.\n\nSee `education/book-14-ai-training/06_transformer_distillation.md` for implementation details.", "answer_variant": "default", "answer_source": "book-14-ai-training/06_transformer_distillation.md", "topics": [], "original_answer": "Train the transformer to match LDA outputs using **MSE + cosine loss**:\n\n```\n# Loss function (mathematical notation)\nL = (1 - \u03bb) \u00d7 MSE(pred, target) + \u03bb \u00d7 (1 - cosine_sim(pred, target))\n\n# Recommended: \u03bb = 0.7 (cosine-weighted)\n```\n\n**CRITICAL:** MSE alone achieves low error but wrong direction. Cosine loss ensures directional alignment.\n\n```python\nfrom projection_transformer import ProjectionTransformer, train_distillation\n\n# Create transformer matching LDA capacity\ntransformer = ProjectionTransformer(\n    embed_dim=384,\n    num_heads=4,      # H\n    num_layers=2,     # L \u2192 4\u00b2 = 16 equivalent heads\n    ff_dim=512\n)\n\n# Train via distillation\ntrain_distillation(\n    transformer=transformer,\n    lda_projection=lda,           # Teacher (multi-head LDA)\n    query_embeddings=train_data,\n    num_epochs=200,\n    cosine_weight=0.7             # Essential!\n)\n```\n\n**Validation results (N=18 LDA heads):**\n- Mean Cosine Similarity: 0.9928 \u00b1 0.0027\n- **99.28% cosine similarity** validates the equivalence conjecture.", "answer_tailored": true}
{"pair_id": "ai-distill-001_p0", "cluster_id": "ai-distill-001", "question": "When should I use transformer distillation vs flat LDA?", "question_type": "medium", "answer": "Choose **transformer distillation** when you have:\n- More than 400 routing heads for single queries (or >30 for batched queries)\n- Memory-constrained environments like mobile or edge devices\n- Clear domain hierarchies that map naturally to tree-structured routing\n\nStick with **flat LDA** when:\n- You're working with fewer than 400 heads (typical for most deployments)\n- You need interpretable routing weights for debugging or analysis\n- Your cluster definitions evolve frequently (transformer retraining is costly)\n\n**Performance comparison (18 heads, 1000 queries):**\n| Approach | Single Query | Batch-32 |\n|----------|-------------|----------|\n| LDA (NumPy CPU) | 0.046 ms | 0.046 ms |\n| Transformer (CUDA) | 1.110 ms | 0.069 ms |\n\n**Decision thresholds:**\n- Single-query workloads: transformer becomes faster beyond ~400 heads\n- Batched workloads (batch size 32): crossover at ~27 heads\n\n**Bottom line:** For smaller head counts, LDA's simplicity beats GPU overhead. Transformer advantages appear at scale where linear softmax costs dominate. See education/book-14-ai-training/06_transformer_distillation.md for implementation details.", "answer_variant": "default", "answer_source": "book-14-ai-training/06_transformer_distillation.md", "topics": [], "original_answer": "**Use transformer when:**\n- N > 400 heads (single query) or N > 30 (batched)\n- Memory constrained (mobile/edge)\n- Natural domain structure for hierarchical routing\n\n**Stay with LDA when:**\n- N < 400 heads (most cases)\n- Interpretability matters (LDA weights are meaningful)\n- Clusters change frequently (retraining is expensive)\n\n**Latency analysis (N=18 heads, 1000 queries):**\n| Method | Single Query | Batch (32) |\n|--------|-------------|------------|\n| LDA (NumPy CPU) | 0.046 ms | 0.046 ms |\n| Transformer (CUDA) | 1.110 ms | 0.069 ms |\n\n**Crossover points:**\n- Single query: >400 heads (LDA wins below)\n- Batched (32): ~27 heads\n\n**Key insight:** At small scale, LDA wins due to GPU kernel overhead. Transformer benefits emerge at larger scale where O(N) softmax becomes expensive.", "answer_tailored": true}
