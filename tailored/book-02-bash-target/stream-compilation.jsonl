{"pair_id": "stream-compilation_p0", "cluster_id": "stream-compilation", "question": "How does UnifyWeaver compile non-recursive predicates?", "question_type": "medium", "answer": "UnifyWeaver compiles non-recursive predicates using the `stream_compiler`, which transforms them into Unix pipeline code. This approach converts Prolog logic into efficient data processing pipelines where:\n\n- Data flows line by line through the pipeline (memory efficient)\n- Joins are implemented as nested loops over streams\n- OR conditions (multiple rules) generate concatenated outputs\n\nFor example, the predicate `grandparent(GP, GC) :- parent(GP, P), parent(P, GC)` compiles to a Bash pipeline where `parent_stream` generates all parent:child pairs, a join function matches children to parents, and `sort -u` deduplicates results. Each conjunctive goal in the rule body becomes a pipeline stage.\n\nWhen predicates have multiple rules (representing OR conditions), the compiler generates separate pipeline branches for each rule, then concatenates and deduplicates their outputs using `{ rule1_output; rule2_output; } | sort -u`.\n\nThe complete compilation strategy is detailed in `education/book-02-bash-target/02_stream_compilation.md`, with the core implementation in `src/unifyweaver/targets/bash/stream_compiler.pl`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "stream-compilation_p1", "cluster_id": "stream-compilation", "question": "What is stream compilation in UnifyWeaver?", "question_type": "medium", "answer": "Stream compilation is UnifyWeaver's approach to translating non-recursive Prolog predicates into Unix pipeline code. It transforms logical rules into data processing pipelines where information flows line by line through Unix utilities.\n\nThe core concept: each Prolog predicate becomes a stream-generating function. Joins are implemented as nested loops over streams, and OR conditions (multiple rules) become concatenated outputs that are deduplicated.\n\nFor example, the `grandparent/2` predicate:\n\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\nCompiles to a Bash pipeline where `parent_stream` outputs all parent-child pairs, a join function matches children to parents (if b==c, output a:d), and `sort -u` removes duplicates. The entire query becomes a composable stream processor.\n\nWhen predicates have multiple rules (OR logic), the stream compiler generates separate output streams for each rule body, concatenates them using brace grouping `{ rule1_output; rule2_output; }`, then deduplicates the combined result.\n\nThis streaming approach is memory-efficient since data flows incrementally rather than materializing entire result sets. The `stream_compiler` module in `src/unifyweaver/` handles this translation. For implementation details and more examples, see education/book-02-bash-target/02_stream_compilation.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "stream-compilation_p2", "cluster_id": "stream-compilation", "question": "How are Prolog joins compiled to Bash pipelines?", "question_type": "medium", "answer": "Prolog joins compile to Bash pipelines through nested loop structures that process streams line by line. Here's how it works:\n\nWhen compiling a join like `grandparent(GP, GC) :- parent(GP, P), parent(P, GC)`, the stream compiler generates:\n\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\nThe join operation works by:\n1. Streaming the first relation (`parent_stream`) through a pipeline\n2. For each tuple (a:b) from the stream, iterating over all tuples (c:d) in the second relation\n3. Matching on the join variable (b==c in this case)\n4. Outputting the combined result (a:d) when matches occur\n5. Deduplicating with `sort -u`\n\nThis approach is memory-efficient since data flows line by line rather than loading everything into memory. Each join becomes a filter function in the pipeline that matches and combines tuples based on shared variables.\n\nFor multiple rules (OR conditions), the compiler concatenates outputs from each rule and deduplicates at the end. See education/book-02-bash-target/02_stream_compilation.md for complete details on stream compilation patterns.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "stream-compilation_p3", "cluster_id": "stream-compilation", "question": "What does the stream_compiler generate?", "question_type": "medium", "answer": "The `stream_compiler` generates Unix pipeline code for non-recursive Prolog predicates. It transforms Prolog logic into Bash functions that process data line by line through streams.\n\nFor example, a simple join predicate like `grandparent(GP, GC)` becomes a pipeline with these components:\n\n**Generated Output:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\nThe generated code creates stream-processing functions where:\n- Data flows through pipes one line at a time (memory efficient)\n- Joins are implemented as nested loops over data streams\n- Deduplication happens via `sort -u`\n\nFor predicates with multiple rules (OR conditions), the compiler generates functions that concatenate outputs from each rule path, then deduplicate:\n\n```bash\nchild_stream() {\n    {\n        # Output from first rule\n        generate_children_from_mothers;\n        # Output from second rule\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nSee education/book-02-bash-target/02_stream_compilation.md for complete details on the compilation process.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p0", "cluster_id": "bash-pipeline-patterns", "question": "Why does UnifyWeaver use Unix pipelines?", "question_type": "medium", "answer": "UnifyWeaver uses Unix pipelines because they provide critical advantages for query compilation:\n\n**Lazy Evaluation and Parallelism**\nPipeline stages run concurrently and process data on-demand rather than loading everything into memory first. This allows UnifyWeaver to handle large datasets efficiently.\n\n**Memory Efficiency**\nStreaming avoids O(n) memory consumption:\n\n```bash\n# Memory-inefficient approach (not used)\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# UnifyWeaver's streaming approach\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n**Core Pipeline Patterns**\n\nUnifyWeaver leverages standard Unix patterns for query operations:\n\n- **Filter:** `stream | grep pattern`\n- **Transform:** `stream | while IFS=: read a b; do echo \"$b:$a\"; done`\n- **Join:** `stream1 | while read key; do lookup_in_stream2 \"$key\"; done`\n- **Aggregate:** `stream | sort | uniq -c`\n\n**Composability and Tooling**\nPipelines let UnifyWeaver combine battle-tested Unix tools (grep, awk, sort) into complex query operations without reinventing functionality.\n\n**Memory Profile**\n\n| Technique | Memory | UnifyWeaver Usage |\n|-----------|--------|-------------------|\n| Stream processing | O(1) | Default for queries |\n| Associative arrays | O(n) | Facts only |\n| Full data loading | O(n) | Avoided |\n\nSee education/book-02-bash-target/02_stream_compilation.md for implementation details.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p1", "cluster_id": "bash-pipeline-patterns", "question": "What are the benefits of stream-based compilation?", "question_type": "medium", "answer": "Stream-based compilation offers several key advantages:\n\n**1. Memory Efficiency**\nProcessing data as a stream avoids loading entire datasets into memory. Instead of accumulating all data first, UnifyWeaver's Bash target processes records one at a time:\n\n```bash\n# Memory-inefficient approach\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# UnifyWeaver's stream approach - O(1) memory\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n**2. Lazy Evaluation**\nData is processed only as needed, reducing unnecessary computation and enabling work with infinite or very large datasets.\n\n**3. Built-in Parallelism**\nPipeline stages execute concurrently - while one stage processes a record, the next stage can already be working on the previous record, improving throughput.\n\n**4. Composability**\nComplex operations emerge from combining simple, single-purpose tools (grep, awk, sort, uniq). This Unix philosophy aligns well with Prolog's logical composition.\n\n**5. Battle-Tested Tooling**\nStream processing leverages standard Unix utilities that are mature, optimized, and widely understood.\n\n**Memory Comparison:**\n- Load-all-data: O(n) memory - not used by UnifyWeaver\n- Stream processing: O(1) memory - UnifyWeaver's default\n- Associative arrays: O(n) memory - only for fact storage when needed\n\nFor detailed pipeline patterns (filter, transform, join, aggregate), see education/book-02-bash-target/02_stream_compilation.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p2", "cluster_id": "bash-pipeline-patterns", "question": "How is memory managed in generated Bash code?", "question_type": "medium", "answer": "UnifyWeaver's Bash code generator manages memory through stream-based processing rather than loading entire datasets. The core principle is to process data as it flows through pipelines, maintaining O(1) memory usage instead of O(n).\n\n**Stream Processing Approach:**\n\nUnifyWeaver avoids memory-intensive patterns like `all_data=$(cat large_file.csv)` and instead generates pipeline-based code:\n\n```bash\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\nThis approach processes one record at a time, keeping memory footprint constant regardless of data size.\n\n**Memory Usage by Pattern:**\n\n- **Stream processing** (default): O(1) memory - data flows through pipelines\n- **Associative arrays** (for facts): O(n) memory - used only when needed for lookups\n- **Loading all data**: O(n) memory - avoided in generated code\n\n**Why This Works:**\n\n1. **Lazy evaluation** - Records processed on demand\n2. **Pipeline concurrency** - Stages run in parallel without buffering entire dataset\n3. **Standard Unix tools** - grep, awk, sort handle streaming natively\n4. **Composability** - Complex queries built from streaming primitives\n\nThe generated code uses standard pipeline patterns (filter with `grep`, transform with `while IFS=`, aggregate with `sort | uniq -c`) that maintain constant memory usage even on large datasets.\n\nFor detailed stream compilation patterns, see `education/book-02-bash-target/02_stream_compilation.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p3", "cluster_id": "bash-pipeline-patterns", "question": "What makes stream compilation efficient?", "question_type": "medium", "answer": "Stream compilation achieves efficiency through several key mechanisms:\n\n**Constant Memory Usage**\n\nThe primary efficiency gain comes from O(1) memory consumption - data flows through the pipeline without being stored in memory. UnifyWeaver's stream-based approach processes one item at a time:\n\n```bash\n# Inefficient: O(n) memory\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Efficient: O(1) memory (UnifyWeaver default)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n**Concurrent Pipeline Execution**\n\nPipeline stages run in parallel, maximizing throughput. While one stage filters data, the next stage transforms it simultaneously:\n\n```bash\nstream | grep pattern | while IFS=: read a b; do\n    echo \"$b:$a\"\ndone\n```\n\n**Lazy Evaluation**\n\nData is only processed when needed. If you only need the first 10 results, the pipeline stops early:\n\n```bash\nstream | grep pattern | head -10  # Stops after 10 matches\n```\n\n**Composable Operations**\n\nCommon patterns reuse proven Unix utilities (grep, awk, sort) which are highly optimized:\n\n- **Filter:** `stream | grep pattern`\n- **Transform:** `stream | awk '{print $2\":\"$1}'`\n- **Aggregate:** `stream | sort | uniq -c`\n- **Join:** Combine streams with lookups\n\n**Memory Usage Comparison**\n\n| Method | Memory Footprint | UnifyWeaver Usage |\n|--------|-----------------|-------------------|\n| Load entire dataset | O(n) | Avoided |\n| Stream processing | O(1) | Default mode |\n| Fact tables (arrays) | O(n) | Only for stored facts |\n\nFor detailed examples and patterns, see education/book-02-bash-target/02_stream_compilation.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
