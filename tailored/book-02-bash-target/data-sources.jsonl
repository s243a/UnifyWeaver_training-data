{"pair_id": "data-sources-overview_p0", "cluster_id": "data-sources-overview", "question": "What data sources does UnifyWeaver support?", "question_type": "medium", "answer": "UnifyWeaver supports four main data source types for external data integration:\n\n1. **CSV/TSV files** (`csv`) - For structured tabular data with options like `csv_file`, `has_header`, and `delimiter`\n2. **Web APIs** (`http`) - For RESTful endpoints with configurable `url`, `headers`, and `cache_duration`\n3. **JSON files** (`json`) - For JSON data with `json_file` and `json_path` options\n4. **Python scripts** (`python`) - For complex transformations using `python_inline` or `python_file`\n\nData sources are defined in Prolog using the `source(Type, Name, Options)` predicate. For example, a CSV source:\n\n```prolog\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n```\n\nThis automatically creates a `users/4` predicate that compiles to efficient Bash functions with both lookup and streaming modes. The compiler generates optimized code using tools like `awk` for CSV processing.\n\nFor complete details on configuring and using each source type, see education/book-02-bash-target/data_sources_pipeline_guide.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-overview_p1", "cluster_id": "data-sources-overview", "question": "How do I use external data in UnifyWeaver?", "question_type": "medium", "answer": "UnifyWeaver allows you to integrate external data using the `source/3` predicate, which compiles data source definitions from Prolog into efficient target language code (like Bash).\n\n### Defining Data Sources\n\nUse the `source/3` predicate with the format:\n```prolog\nsource(SourceType, SourceName, OptionsList)\n```\n\n### Available Source Types\n\n- **csv** - CSV/TSV files (`csv_file`, `has_header`, `delimiter`)\n- **http** - REST APIs and web endpoints (`url`, `headers`, `cache_duration`)\n- **json** - JSON data files (`json_file`, `json_path`)\n- **python** - Custom transformations (`python_inline`, `python_file`)\n\n### Example: Loading CSV Data\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Automatically creates: users/4 predicate (id, name, role, department)\n```\n\n### Compiling to Target Language\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\nThis generates optimized Bash functions that support both streaming all records and key-based lookups. You can then query the data in your compiled output:\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\nFor comprehensive examples and advanced usage patterns, see education/book-02-bash-target/data_sources_pipeline_guide.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-overview_p2", "cluster_id": "data-sources-overview", "question": "What is the source/3 predicate?", "question_type": "medium", "answer": "The `source/3` predicate is UnifyWeaver's declarative way to define external data sources that can be compiled into target languages like Bash.\n\n### Syntax\n\n```prolog\nsource(Type, Name, Options)\n```\n\nWhere:\n- **Type**: The data source type (`csv`, `http`, `json`, or `python`)\n- **Name**: The identifier for your data source\n- **Options**: Configuration list specific to the source type\n\n### Example Usage\n\n```prolog\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n```\n\nThis declaration creates a `users/4` predicate that UnifyWeaver can compile into native Bash functions. The predicate automatically handles both streaming all records and key-based lookups.\n\n### Common Source Types\n\n- **csv**: For CSV/TSV files (options: `csv_file`, `has_header`, `delimiter`)\n- **http**: For REST APIs (options: `url`, `headers`, `cache_duration`)\n- **json**: For JSON data (options: `json_file`, `json_path`)\n- **python**: For complex transformations (options: `python_inline`, `python_file`)\n\nTo compile a declared source, use:\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\nFor complete examples and pipeline patterns, see `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-overview_p3", "cluster_id": "data-sources-overview", "question": "How do I compile CSV data to Bash?", "question_type": "medium", "answer": "To compile CSV data to Bash in UnifyWeaver, you define a CSV data source using Prolog predicates and then compile it to executable Bash code.\n\n### Define the CSV Source\n\nUse the `source/3` predicate to declare your CSV file:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% This creates a users/4 predicate (id, name, role, department)\n```\n\n### Compile to Bash\n\nExecute the compilation:\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Result\n\nUnifyWeaver generates a Bash function that reads and processes the CSV:\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup by key\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Use the Generated Code\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\nThe `source/3` predicate supports other data types too: `http` for APIs, `json` for JSON files, and `python` for custom transformations. For comprehensive coverage, see education/book-02-bash-target/data_sources_pipeline_guide.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p0", "cluster_id": "data-sources-pipeline", "question": "How do I build ETL pipelines with UnifyWeaver?", "question_type": "medium", "answer": "UnifyWeaver enables ETL pipeline construction by combining Prolog data sources with standard Unix pipeline tools for streaming, filtering, transforming, and aggregating data.\n\n### ETL Pipeline Patterns\n\n**Stream Data:**\n```bash\nusers_stream  # Output all records\n```\n\n**Filter Records:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transform Output:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregate Results:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Full ETL Implementation\n\n```prolog\n% Configure data source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Generate and assemble pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Multi-Plugin Loading\n\nLoad multiple source plugins without namespace conflicts using `load_files` with `imports([])`:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```\n\nSee education/book-02-bash-target/data_sources_pipeline_guide.md for comprehensive pipeline examples.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p1", "cluster_id": "data-sources-pipeline", "question": "How do I filter and transform data sources?", "question_type": "medium", "answer": "## Filtering and Transforming Data Sources\n\nUnifyWeaver data sources can be filtered and transformed using standard Unix pipeline tools after compilation to Bash.\n\n### Basic Filtering\n\nOnce you've compiled a source to a stream function, pipe it through `awk` or `grep`:\n\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'  # Filter by role\nusers_stream | grep \"Engineering\"            # Filter by pattern\n```\n\n### Transforming Records\n\nUse shell loops to reshape data:\n\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\nOr `awk` for field manipulation:\n\n```bash\nusers_stream | awk -F: '{print $2 \" (\" $3 \")\"}'  # Extract name and role\n```\n\n### Aggregating Results\n\nGroup and count with `awk` associative arrays:\n\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete Example\n\n```prolog\n% Define your data source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile to Bash stream\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    build_etl_pipeline.\n\nbuild_etl_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\n# Filter developers and transform output\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"$name is a $role in $dept\"\n    done\n\n# Aggregate by department\nusers_stream | awk -F: \\'{count[$4]++} END {\n    for (d in count) print d \": \" count[d]\n}\\'',\n    open('output/etl.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\nFor complete pipeline patterns and advanced techniques, see education/book-02-bash-target/data_sources_pipeline_guide.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p2", "cluster_id": "data-sources-pipeline", "question": "What pipeline operations are available?", "question_type": "medium", "answer": "UnifyWeaver provides several standard Unix pipeline operations for processing data sources in Bash:\n\n**Streaming** - Output all records from a source:\n```bash\nusers_stream  # Emits all records\n```\n\n**Filtering** - Select records matching criteria using awk:\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation** - Reshape or format records:\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation** - Compute summaries across records:\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\nThese operations can be chained together to build ETL pipelines. After compiling a UnifyWeaver source definition to Bash, the generated streaming function integrates naturally with standard Unix tools. For complete ETL pipeline examples combining source compilation with multi-stage processing, see education/book-02-bash-target/data_sources_pipeline_guide.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p3", "cluster_id": "data-sources-pipeline", "question": "How do I aggregate data in Bash pipelines?", "question_type": "medium", "answer": "To aggregate data in Bash pipelines with UnifyWeaver, use `awk` to accumulate values and print summaries:\n\n**Basic aggregation pattern:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\nThis groups records by department (field 4) and counts occurrences. The `END` block executes after processing all input, printing the aggregated results.\n\n**How it works:**\n- Stream data from your compiled source (e.g., `users_stream`)\n- Use `awk` with field separator (`-F:`) matching your source format\n- Accumulate values in associative arrays during processing\n- Print aggregated results in the `END` block\n\n**Other common aggregations:**\n\nSumming values:\n```bash\nsales_stream | awk -F: '{total+=$3} END {print \"Total: \" total}'\n```\n\nAveraging:\n```bash\nscores_stream | awk -F: '{sum+=$2; count++} END {print \"Average: \" sum/count}'\n```\n\nMultiple aggregates:\n```bash\nusers_stream | awk -F: '{\n    dept[$4]++\n    role_count[$3]++\n} END {\n    print \"By Department:\"\n    for (d in dept) print \"  \" d\": \"dept[d]\n    print \"By Role:\"\n    for (r in role_count) print \"  \" r\": \"role_count[r]\n}'\n```\n\nYou can also combine filtering before aggregation:\n```bash\nusers_stream | awk -F: '$3 == \"Developer\" {dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\nFor complete ETL pipeline examples with aggregation, see education/book-02-bash-target/data_sources_pipeline_guide.md.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
