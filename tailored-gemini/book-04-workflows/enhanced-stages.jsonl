{"pair_id": "b4-c6-enhanced-pipeline-stages_p0", "cluster_id": "b4-c6-enhanced-pipeline-stages", "question": "What are enhanced pipeline stages?", "question_type": "medium", "answer": "Enhanced pipeline stages are powerful building blocks designed for processing data within a single target language. While cross-target pipelines manage data flow between different environments (such as transitioning from AWK to Python), enhanced stages provide granular control over stream processing within the same language context.\n\n### Functional Categories\nThese stages are organized into several key categories to handle complex data workflows:\n* **Stream Combination**: Methods like `interleave`, `concat`, and `zip` for merging data flows.\n* **Flow & Execution**: Tools such as `fan_out`, `parallel`, and `merge` for managing concurrent processing.\n* **Data Transformation**: Operations like `filter_by`, `batch`, `group_by`, `reduce`, and `distinct` for refining and aggregating results.\n* **Resiliency & Performance**: Features like `try_catch` and `retry` for error handling, alongside `rate_limit` and `buffer` for throughput management.\n\n### Implementation\nIn UnifyWeaver, these stages are implemented using the `compile_enhanced_pipeline/3` predicate. For example:\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```\n\nFor more information on configuring these stages, see `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["enhanced-stages", "stream-processing", "within-target"], "original_answer": "## Enhanced Pipeline Stages\n\nPowerful building blocks for within-target data processing.\n\n### vs Cross-Target Pipelines\n\n| Aspect | Cross-Target | Enhanced Stages |\n|--------|--------------|------------------|\n| Scope | AWK \u2192 Python \u2192 Go | Within one target |\n| Use | Different languages | Same language |\n| Chapter | Chapter 3 | Chapter 6 |\n\n### Stage Categories\n\n| Category | Stages |\n|----------|--------|\n| Stream combination | interleave, concat, zip |\n| Flow control | fan_out, parallel, merge |\n| Filtering | filter_by, batch, unbatch |\n| Aggregation | group_by, reduce, distinct |\n| Error handling | try_catch, retry, timeout |\n| Throughput | rate_limit, throttle, buffer |\n\n### Usage\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_tailored": true}
{"pair_id": "b4-c6-enhanced-pipeline-stages_p1", "cluster_id": "b4-c6-enhanced-pipeline-stages", "question": "How do enhanced stages differ from cross-target pipelines?", "question_type": "medium", "answer": "Enhanced stages differ from cross-target pipelines primarily in their execution scope: enhanced stages optimize data processing within a single target language, whereas cross-target pipelines coordinate data flow between different environments such as AWK, Python, or Go.\n\nWhile cross-target pipelines focus on multi-language orchestration, enhanced stages provide granular building blocks for internal logic within the same target environment. These stages include:\n\n*   **Stream Combination**: `interleave`, `concat`, `zip`\n*   **Flow Control**: `fan_out`, `parallel`, `merge`\n*   **Filtering & Aggregation**: `filter_by`, `batch`, `unbatch`, `group_by`, `reduce`, `distinct`\n*   **Operations & Throughput**: `try_catch`, `retry`, `timeout`, `rate_limit`, `throttle`, `buffer`\n\nEnhanced pipelines are implemented via the `compile_enhanced_pipeline/3` predicate:\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```\n\nFor more details on within-target processing, see `education/book-04-workflows/06_enhanced_pipeline_stages.md`. For information on polyglot orchestration, refer to the cross-target pipeline documentation in `education/book-04-workflows/`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["enhanced-stages", "stream-processing", "within-target"], "original_answer": "## Enhanced Pipeline Stages\n\nPowerful building blocks for within-target data processing.\n\n### vs Cross-Target Pipelines\n\n| Aspect | Cross-Target | Enhanced Stages |\n|--------|--------------|------------------|\n| Scope | AWK \u2192 Python \u2192 Go | Within one target |\n| Use | Different languages | Same language |\n| Chapter | Chapter 3 | Chapter 6 |\n\n### Stage Categories\n\n| Category | Stages |\n|----------|--------|\n| Stream combination | interleave, concat, zip |\n| Flow control | fan_out, parallel, merge |\n| Filtering | filter_by, batch, unbatch |\n| Aggregation | group_by, reduce, distinct |\n| Error handling | try_catch, retry, timeout |\n| Throughput | rate_limit, throttle, buffer |\n\n### Usage\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_tailored": true}
{"pair_id": "b4-c6-enhanced-pipeline-stages_p2", "cluster_id": "b4-c6-enhanced-pipeline-stages", "question": "What types of enhanced stages exist?", "question_type": "medium", "answer": "Enhanced pipeline stages in UnifyWeaver provide powerful building blocks for data processing within a single target environment. These stages are categorized based on their functional role in the pipeline:\n\n- **Stream Combination:** Includes `interleave`, `concat`, and `zip` for merging multiple data sources.\n- **Flow Control:** Provides `fan_out`, `parallel`, and `merge` to manage execution paths and concurrency.\n- **Filtering:** Features `filter_by`, `batch`, and `unbatch` for data selection and windowing.\n- **Aggregation:** Includes `group_by`, `reduce`, and `distinct` for summarizing and condensing data.\n- **Error Handling:** Ensures resiliency through `try_catch`, `retry`, and `timeout` mechanisms.\n- **Throughput Management:** Uses `rate_limit`, `throttle`, and `buffer` to control data flow rates.\n\nThese stages are typically invoked via the `compile_enhanced_pipeline/3` predicate. For a deeper dive into within-target processing, refer to `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["enhanced-stages", "stream-processing", "within-target"], "original_answer": "## Enhanced Pipeline Stages\n\nPowerful building blocks for within-target data processing.\n\n### vs Cross-Target Pipelines\n\n| Aspect | Cross-Target | Enhanced Stages |\n|--------|--------------|------------------|\n| Scope | AWK \u2192 Python \u2192 Go | Within one target |\n| Use | Different languages | Same language |\n| Chapter | Chapter 3 | Chapter 6 |\n\n### Stage Categories\n\n| Category | Stages |\n|----------|--------|\n| Stream combination | interleave, concat, zip |\n| Flow control | fan_out, parallel, merge |\n| Filtering | filter_by, batch, unbatch |\n| Aggregation | group_by, reduce, distinct |\n| Error handling | try_catch, retry, timeout |\n| Throughput | rate_limit, throttle, buffer |\n\n### Usage\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_tailored": true}
{"pair_id": "b4-c6-stream-combination_p0", "cluster_id": "b4-c6-stream-combination", "question": "What is the difference between interleave and concat?", "question_type": "medium", "answer": "In UnifyWeaver's stream combination stages, the difference between `interleave` and `concat` is defined by the order in which they process and merge data from multiple sources.\n\n*   **interleave(Stages)**: Employs a round-robin alternating strategy. When merging streams like `[A1, A2, A3]` and `[B1, B2]`, the output is interleaved as `[A1, B1, A2, B2, A3]`. This is ideal for fair merging where you want to balance processing across all input streams.\n*   **concat(Stages)**: Performs a sequential append. It processes the entirety of the first stream before starting the next. For example, combining `[A1, A2]` and `[B1, B2, B3]` results in `[A1, A2, B1, B2, B3]`. Use this for simple unions or when the order of data sets is critical.\n\n| Feature | interleave | concat |\n| :--- | :--- | :--- |\n| **Merging Pattern** | Alternating / Round-robin | Sequential / Append |\n| **Data Priority** | Balanced across streams | First stream is priority |\n| **Primary Goal** | Fair distribution | Union of datasets |\n\nFor a deeper dive into stream management and pipeline architecture, refer to the documentation in `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["interleave", "concat", "zip", "stream-merging"], "original_answer": "## Stream Combination Stages\n\n### interleave vs concat\n\n**interleave(Stages)**: Round-robin alternating\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)**: Sequential append\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\n| Aspect | interleave | concat |\n|--------|------------|--------|\n| Order | Alternating | Sequential |\n| Fairness | Balanced | First priority |\n| Use case | Fair merge | Union/append |\n\n### zip(Stages)\n\nRuns multiple stages on same input, combines outputs:\n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach record gets all three enrichments.", "answer_tailored": true}
{"pair_id": "b4-c6-stream-combination_p1", "cluster_id": "b4-c6-stream-combination", "question": "How does the zip stage work?", "question_type": "medium", "answer": "The `zip(Stages)` stage works by running multiple processing stages concurrently on the same input and combining their resulting outputs. This allows each record in the stream to be processed by every stage listed in the zip block. For example, if you define `zip([geocode/1, sentiment/1, categorize/1])`, every record will undergo geocoding, sentiment analysis, and categorization simultaneously. This stage is ideal for complex data enrichment where multiple independent attributes need to be added to a single data point. You can find more information on pipeline orchestration in `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["interleave", "concat", "zip", "stream-merging"], "original_answer": "## Stream Combination Stages\n\n### interleave vs concat\n\n**interleave(Stages)**: Round-robin alternating\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)**: Sequential append\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\n| Aspect | interleave | concat |\n|--------|------------|--------|\n| Order | Alternating | Sequential |\n| Fairness | Balanced | First priority |\n| Use case | Fair merge | Union/append |\n\n### zip(Stages)\n\nRuns multiple stages on same input, combines outputs:\n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach record gets all three enrichments.", "answer_tailored": true}
{"pair_id": "b4-c6-stream-combination_p2", "cluster_id": "b4-c6-stream-combination", "question": "When should you use interleave vs concat?", "question_type": "medium", "answer": "Use **interleave** when you need to merge multiple streams fairly using a round-robin approach, while **concat** should be used for sequential appending where one stream must finish before the next begins.\n\n### interleave(Stages)\nThis stage alternates between inputs to ensure balanced processing. For example, merging `[A1, A2, A3]` and `[B1, B2]` results in `[A1, B1, A2, B2, A3]`. Use this when you want to prevent a single large stream from blocking others.\n\n### concat(Stages)\nThis stage appends streams sequentially. It processes every record from the first stage before moving to the second. For example, `[A1, A2]` followed by `[B1, B2, B3]` results in `[A1, A2, B1, B2, B3]`. This is the standard choice for simple data unions.\n\n| Feature | interleave | concat |\n| :--- | :--- | :--- |\n| **Order** | Alternating (Round-robin) | Sequential |\n| **Fairness** | Balanced across all stages | Priority to earlier stages |\n| **Primary Use** | Fair stream merging | Union and appending |\n\nFor more details on pipeline stage configurations, refer to `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["interleave", "concat", "zip", "stream-merging"], "original_answer": "## Stream Combination Stages\n\n### interleave vs concat\n\n**interleave(Stages)**: Round-robin alternating\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)**: Sequential append\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\n| Aspect | interleave | concat |\n|--------|------------|--------|\n| Order | Alternating | Sequential |\n| Fairness | Balanced | First priority |\n| Use case | Fair merge | Union/append |\n\n### zip(Stages)\n\nRuns multiple stages on same input, combines outputs:\n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach record gets all three enrichments.", "answer_tailored": true}
{"pair_id": "b4-c6-flow-control_p0", "cluster_id": "b4-c6-flow-control", "question": "How do fan_out and parallel stages work?", "question_type": "medium", "answer": "UnifyWeaver uses `fan_out` and `parallel` stages to manage how records are distributed across multiple processing paths within a pipeline.\n\n### fan_out(Stages)\nThe `fan_out` stage implements a broadcast pattern where every input record is sent to every stage defined in the list. This allows you to perform disparate tasks like validation, enrichment, and logging on the same record stream simultaneously.\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\nThe `parallel` stage is designed for performance optimization by executing multiple stages concurrently. It is best used for heavy transformations that can benefit from multi-threaded or multi-process execution to increase overall throughput.\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### Related Flow Control Stages\n- **route_by(Pred, Routes):** This stage provides conditional routing, sending records to specific handlers based on a predicate (e.g., routing by message type like `error` or `info`).\n- **merge:** Both `fan_out` and `parallel` typically conclude with a `merge` stage, which synchronizes the outputs from various branches back into a single stream while maintaining record ordering where possible.\n\nDetailed documentation on these flow control patterns and other enhanced pipeline building blocks can be found in `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["fan-out", "parallel", "route-by", "flow-control"], "original_answer": "## Flow Control Stages\n\n### fan_out(Stages)\n\nBroadcasts each record to multiple stages:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\n\nExecutes stages concurrently for performance:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### route_by(Pred, Routes)\n\nConditional routing based on record type:\n\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\n### merge\n\nCombines results from fan_out or parallel:\n- Collects outputs from all branches\n- Maintains record order where possible", "answer_tailored": true}
{"pair_id": "b4-c6-flow-control_p1", "cluster_id": "b4-c6-flow-control", "question": "What is conditional routing with route_by?", "question_type": "medium", "answer": "Conditional routing with `route_by(Pred, Routes)` is a flow control mechanism in UnifyWeaver that dispatches records to specific processing paths based on their attributes. It evaluates a predicate (`Pred`) against each record to determine a routing key, which is then matched against the entries in the `Routes` list to select the appropriate destination stage.\n\nExample implementation:\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\nThis allows for declarative branching within a workflow, ensuring that different record types (such as logs of varying severity) are processed by the correct specialized handlers. For a comprehensive overview of pipeline orchestration, see `education/book-04-workflows/`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["fan-out", "parallel", "route-by", "flow-control"], "original_answer": "## Flow Control Stages\n\n### fan_out(Stages)\n\nBroadcasts each record to multiple stages:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\n\nExecutes stages concurrently for performance:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### route_by(Pred, Routes)\n\nConditional routing based on record type:\n\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\n### merge\n\nCombines results from fan_out or parallel:\n- Collects outputs from all branches\n- Maintains record order where possible", "answer_tailored": true}
{"pair_id": "b4-c6-flow-control_p2", "cluster_id": "b4-c6-flow-control", "question": "How do you merge results from parallel stages?", "question_type": "medium", "answer": "To merge results from parallel stages in UnifyWeaver, you use the `merge` operator following a `parallel(Stages)` or `fan_out(Stages)` block. The `merge` stage consolidates outputs from all concurrent branches back into a single stream, maintaining the original record order where possible.\n\nExample implementation:\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\nThe `merge` operator is also used with `fan_out([Stage1, Stage2])` to collect broadcasted results. For more details on pipeline orchestration and flow control, see `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["fan-out", "parallel", "route-by", "flow-control"], "original_answer": "## Flow Control Stages\n\n### fan_out(Stages)\n\nBroadcasts each record to multiple stages:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\n\nExecutes stages concurrently for performance:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### route_by(Pred, Routes)\n\nConditional routing based on record type:\n\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\n### merge\n\nCombines results from fan_out or parallel:\n- Collects outputs from all branches\n- Maintains record order where possible", "answer_tailored": true}
{"pair_id": "b4-c6-error-throughput_p0", "cluster_id": "b4-c6-error-throughput", "question": "How do you handle errors in enhanced pipelines?", "question_type": "medium", "answer": "In UnifyWeaver, error handling in enhanced pipelines is implemented through specialized resilient stages such as `try_catch/2`, `retry/3`, and `timeout/3`. These allow you to define granular recovery strategies for individual pipeline steps.\n\n### Key Error Handling Stages\n*   **`try_catch(Stage, Handler)`**: Executes a handler predicate if the primary stage fails (e.g., `try_catch(risky_transform/1, error_fallback/1)`).\n*   **`retry(Stage, N, Options)`**: Attempts to re-run a stage up to N times. It supports configurations like `delay(1000)` and `backoff(exponential)` for intelligent retry logic.\n*   **`timeout(Stage, Ms, Fallback)`**: Wraps a potentially slow operation with a millisecond limit, falling back to an alternative if the operation exceeds the time limit.\n\n### Stability and Throughput Controls\nTo prevent errors related to resource exhaustion or rate limits, pipelines also support throughput management:\n*   **`rate_limit(N, Per)`**: Limits processing to a specific rate (e.g., `rate_limit(100, second)`).\n*   **`throttle(Ms)`**: Adds a fixed delay between records.\n*   **`buffer(N)` / `debounce(Ms)`**: Manages record accumulation and quiet periods.\n\nA robust production configuration often chains these together to ensure reliability:\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```\n\nFor more details on pipeline orchestration and stage options, see `education/book-04-workflows/`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["error-handling", "retry", "rate-limit", "resilience"], "original_answer": "## Error Handling Stages\n\n### try_catch(Stage, Handler)\n\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n### retry(Stage, N, Opts)\n\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n### timeout(Stage, Ms, Fallback)\n\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\n## Throughput Control\n\n### rate_limit(N, Per)\n\n```prolog\nrate_limit(100, second)  % 100 records/sec\n```\n\n### Other Controls\n\n| Stage | Effect |\n|-------|--------|\n| `throttle(Ms)` | Fixed delay |\n| `buffer(N)` | Collect N records |\n| `debounce(Ms)` | Wait for quiet period |\n\n### Production Pipeline\n\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```", "answer_tailored": true}
{"pair_id": "b4-c6-error-throughput_p1", "cluster_id": "b4-c6-error-throughput", "question": "What retry and timeout options are available?", "question_type": "medium", "answer": "UnifyWeaver provides dedicated pipeline stages for managing execution failures and duration limits, specifically through the `retry/3` and `timeout/3` constructs.\n\n### Retry Capabilities\nThe `retry(Stage, N, Opts)` stage attempts to re-execute a failing operation up to `N` times. You can configure the retry behavior using the following options in the `Opts` list:\n- **delay(Ms)**: Specifies the initial wait time (in milliseconds) between retries.\n- **backoff(Type)**: Determines the retry strategy, such as `exponential` or `fixed` backoff.\n\nExample:\n```prolog\nretry(send_request/1, 3, [delay(1000), backoff(exponential)])\n```\n\n### Timeout Capabilities\nThe `timeout(Stage, Ms, Fallback)` stage enforces a maximum execution time for a given operation. If the `Stage` does not complete within the specified `Ms` limit, the pipeline diverts to the `Fallback` stage to ensure the process continues or fails gracefully.\n\nExample:\n```prolog\ntimeout(process_data/1, 5000, error_handler/1)\n```\n\n### Related Error and Flow Controls\nFor broader error management, `try_catch(Stage, Handler)` can be used to wrap risky operations. Additionally, throughput can be managed using:\n- **rate_limit(N, Per)**: Limits records per time unit (e.g., `second`).\n- **throttle(Ms)**: Introduces a fixed delay.\n- **debounce(Ms)**: Delays execution until a quiet period is detected.\n\nDetailed documentation on these features is available in `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["error-handling", "retry", "rate-limit", "resilience"], "original_answer": "## Error Handling Stages\n\n### try_catch(Stage, Handler)\n\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n### retry(Stage, N, Opts)\n\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n### timeout(Stage, Ms, Fallback)\n\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\n## Throughput Control\n\n### rate_limit(N, Per)\n\n```prolog\nrate_limit(100, second)  % 100 records/sec\n```\n\n### Other Controls\n\n| Stage | Effect |\n|-------|--------|\n| `throttle(Ms)` | Fixed delay |\n| `buffer(N)` | Collect N records |\n| `debounce(Ms)` | Wait for quiet period |\n\n### Production Pipeline\n\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```", "answer_tailored": true}
{"pair_id": "b4-c6-error-throughput_p2", "cluster_id": "b4-c6-error-throughput", "question": "How do you control throughput in pipelines?", "question_type": "medium", "answer": "In UnifyWeaver, throughput in pipelines is controlled through specialized stages designed to regulate data flow and ensure system stability. These mechanisms allow you to manage processing rates, buffer records, and prevent downstream service exhaustion.\n\nThe primary stages for throughput management include:\n\n*   **`rate_limit(N, Per)`**: Limits processing to a maximum number of records (N) over a specific timeframe (e.g., `rate_limit(100, second)`).\n*   **`throttle(Ms)`**: Enforces a fixed delay (in milliseconds) between processing individual records.\n*   **`buffer(N)`**: Accumulates up to N records before passing them to the next stage, often used to optimize bulk operations.\n*   **`debounce(Ms)`**: Only emits a record after a specified quiet period has passed without any new incoming data.\n\nThese controls are typically integrated into a pipeline definition alongside error handling logic:\n\n```prolog\n[\n    try_catch(retry(ingest_data/1, 3), log_failure/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_load/1\n]\n```\n\nIn this configuration, the `rate_limit` ensures the pipeline does not exceed 50 operations per second, while `batch` groups those records for more efficient output. Detailed documentation on these stages can be found in `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["error-handling", "retry", "rate-limit", "resilience"], "original_answer": "## Error Handling Stages\n\n### try_catch(Stage, Handler)\n\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n### retry(Stage, N, Opts)\n\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n### timeout(Stage, Ms, Fallback)\n\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\n## Throughput Control\n\n### rate_limit(N, Per)\n\n```prolog\nrate_limit(100, second)  % 100 records/sec\n```\n\n### Other Controls\n\n| Stage | Effect |\n|-------|--------|\n| `throttle(Ms)` | Fixed delay |\n| `buffer(N)` | Collect N records |\n| `debounce(Ms)` | Wait for quiet period |\n\n### Production Pipeline\n\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```", "answer_tailored": true}
