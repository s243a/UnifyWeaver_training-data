{"pair_id": "b4-c3-multi-target-pipelines_p0", "cluster_id": "b4-c3-multi-target-pipelines", "question": "How do multi-target pipelines work in UnifyWeaver?", "question_type": "medium", "answer": "In UnifyWeaver, multi-target pipelines function by chaining together processing stages that are each compiled into a different optimal target language from a single Prolog source.\n\nThe system handles orchestration through the following mechanism:\n- **Prolog Definition**: Pipelines are declared using `pipeline_orchestrator/3`, where you define the sequence of stages and their corresponding targets (e.g., `stage(parse, awk)` followed by `stage(transform, python)`).\n- **Language Specialization**: Each part of the workflow uses the language best suited for the task\u2014such as AWK for field extraction, Python for complex business logic, or Go for high-speed data aggregation.\n- **Serialized Data Flow**: The orchestrator manages data movement between stages by serializing outputs into intermediate formats (typically JSON or CSV). Each stage reads from the previous stage's output, processes the data, and passes it forward.\n- **Unified Logic**: Despite running in different environments, the entire workflow remains managed by a central UnifyWeaver specification, ensuring consistent data marshaling and error handling.\n\nFor detailed implementation examples, refer to `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-orchestration", "multi-target", "stage-selection"], "original_answer": "## Multi-Target Pipelines\n\nMulti-target pipelines combine stages compiled to different languages:\n\n```\nAWK \u2192 Python \u2192 Go\n```\n\n### How It Works\n\nEach stage is compiled to its optimal target:\n- **AWK**: Text parsing, field extraction\n- **Python**: Complex transforms, ML integration\n- **Go**: High-performance aggregation\n\n### Pipeline Orchestration\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow\n\n1. Stage outputs \u2192 serialized (JSON/CSV)\n2. Orchestrator routes to next stage\n3. Next stage reads, processes, outputs\n4. Continues until final stage\n\n### Benefits\n\n- Leverage each language's strengths\n- Unified Prolog specification\n- Automatic data marshaling", "answer_tailored": true}
{"pair_id": "b4-c3-multi-target-pipelines_p1", "cluster_id": "b4-c3-multi-target-pipelines", "question": "How do you combine different target languages in a pipeline?", "question_type": "medium", "answer": "To combine different target languages in a pipeline, you define multi-stage workflows in a unified Prolog specification where each stage is compiled to the language best suited for its specific task. This approach allows you to leverage individual language strengths, such as using AWK for efficient text parsing, Python for complex transformations, and Go for high-performance data aggregation.\n\nThe orchestration is handled through the `pipeline_orchestrator/3` predicate:\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\nUnifyWeaver manages the data flow between these heterogeneous stages by automatically serializing stage outputs into a common format like JSON or CSV. The orchestrator then routes this data to the next stage's input, handling all necessary data marshaling and process management.\n\nFor more information on orchestrating multi-language workflows, see education/book-04-workflows/03_pipeline_orchestration.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-orchestration", "multi-target", "stage-selection"], "original_answer": "## Multi-Target Pipelines\n\nMulti-target pipelines combine stages compiled to different languages:\n\n```\nAWK \u2192 Python \u2192 Go\n```\n\n### How It Works\n\nEach stage is compiled to its optimal target:\n- **AWK**: Text parsing, field extraction\n- **Python**: Complex transforms, ML integration\n- **Go**: High-performance aggregation\n\n### Pipeline Orchestration\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow\n\n1. Stage outputs \u2192 serialized (JSON/CSV)\n2. Orchestrator routes to next stage\n3. Next stage reads, processes, outputs\n4. Continues until final stage\n\n### Benefits\n\n- Leverage each language's strengths\n- Unified Prolog specification\n- Automatic data marshaling", "answer_tailored": true}
{"pair_id": "b4-c3-multi-target-pipelines_p2", "cluster_id": "b4-c3-multi-target-pipelines", "question": "What is cross-target pipeline orchestration?", "question_type": "medium", "answer": "Cross-target pipeline orchestration is the management of multi-stage workflows where each stage is compiled from a unified Prolog specification into the specific target language most optimal for its task. This allows a single pipeline to leverage different language strengths, such as using AWK for initial text parsing, Python for complex logic or transformations, and Go for high-performance data aggregation.\n\nThe process is defined via the `pipeline_orchestrator/3` predicate:\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\nThe orchestrator facilitates data flow by automatically marshaling data between stages using serialized formats like JSON or CSV. Each stage processes its input and passes the results to the next runtime in the sequence until the final output is produced. This approach ensures that developers can utilize the best features of various languages without manually managing the glue code or data transitions between them.\n\nFor more details on implementing these workflows, see `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-orchestration", "multi-target", "stage-selection"], "original_answer": "## Multi-Target Pipelines\n\nMulti-target pipelines combine stages compiled to different languages:\n\n```\nAWK \u2192 Python \u2192 Go\n```\n\n### How It Works\n\nEach stage is compiled to its optimal target:\n- **AWK**: Text parsing, field extraction\n- **Python**: Complex transforms, ML integration\n- **Go**: High-performance aggregation\n\n### Pipeline Orchestration\n\n```prolog\npipeline_orchestrator([\n    stage(parse, awk),\n    stage(transform, python),\n    stage(aggregate, go)\n], Options, Output).\n```\n\n### Data Flow\n\n1. Stage outputs \u2192 serialized (JSON/CSV)\n2. Orchestrator routes to next stage\n3. Next stage reads, processes, outputs\n4. Continues until final stage\n\n### Benefits\n\n- Leverage each language's strengths\n- Unified Prolog specification\n- Automatic data marshaling", "answer_tailored": true}
{"pair_id": "b4-c3-target-selection_p0", "cluster_id": "b4-c3-target-selection", "question": "How do you choose which target language for each pipeline stage?", "question_type": "medium", "answer": "Choosing the optimal target language for each stage of your UnifyWeaver pipeline depends on balancing data format, logic complexity, and performance requirements.\n\n### Target Performance and Capabilities\n\n| Target | Primary Use Case | Key Characteristics |\n|--------|------------------|---------------------|\n| **AWK** | Line-based text parsing | High-speed streaming for simple patterns. |\n| **Python** | Complex business logic | Flexible logic with extensive JSON and ML libraries. |\n| **Go** | High-speed aggregation | Compiled performance for data-intensive tasks. |\n| **C#** | Recursive/Fixpoint queries | Optimized for .NET and complex recursive logic. |\n| **Bash** | System orchestration | Lightweight glue code for system integration. |\n\n### Key Selection Factors\n\n1.  **Data Format**: Use **AWK** for raw text/CSV and **Python** or **Go** for structured JSON data.\n2.  **Logic Complexity**: Simple filters are best handled by **AWK**, while sophisticated algorithms or external library needs favor **Python**.\n3.  **Performance Demands**: For processing millions of records or high-concurrency requirements, **Go** or **C#** are the preferred compiled targets.\n4.  **Runtime Environment**: Choose **Bash** for native Unix tool integration and **C#** for cross-platform .NET deployments.\n\n### Quick Decision Guide\n*   **Log/CSV Parsing?** \u2192 AWK\n*   **JSON Processing or ML?** \u2192 Python\n*   **Large-Scale Aggregations?** \u2192 Go\n*   **Recursive Graph Logic?** \u2192 C# Query Runtime\n\nFor more detailed strategies on pipeline design, see the documentation in `education/book-04-workflows/`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["target-selection", "language-strengths", "optimization"], "original_answer": "## Target Selection Criteria\n\n### Language Strengths\n\n| Target | Best For | Characteristics |\n|--------|----------|------------------|\n| AWK | Line parsing | Fast, streaming |\n| Python | Complex logic | Flexible, libraries |\n| Go | Aggregation | Fast, compiled |\n| C# | Recursive queries | Fixpoint, .NET |\n| Bash | Glue code | System integration |\n\n### Selection Factors\n\n1. **Input/Output format**: AWK for text, Python for JSON\n2. **Complexity**: Simple \u2192 AWK, Complex \u2192 Python\n3. **Performance**: Critical \u2192 Go/C#\n4. **Libraries**: ML \u2192 Python, .NET \u2192 C#\n5. **Deployment**: Unix \u2192 Bash, Cross-platform \u2192 .NET\n\n### Decision Example\n\n```\nParsing CSV? \u2192 AWK\nML inference? \u2192 Python\n1M+ records? \u2192 Go\nRecursive? \u2192 C# Query Runtime\n```", "answer_tailored": true}
{"pair_id": "b4-c3-target-selection_p1", "cluster_id": "b4-c3-target-selection", "question": "When should you use AWK vs Python vs Go in a pipeline?", "question_type": "medium", "answer": "Choosing between AWK, Python, and Go in a UnifyWeaver pipeline depends on your data format, the complexity of your logic, and your performance requirements.\n\n*   **AWK** is ideal for fast, streaming line parsing. Use it when processing text or CSV files that require simple transformations. (See `education/book-05-awk-target/` for implementation details).\n*   **Python** should be used for pipelines requiring complex logic, JSON handling, or integration with external libraries (e.g., ML inference). It offers the most flexibility for sophisticated workflows. (See `education/book-03-python-target/` for more).\n*   **Go** is the best choice for high-performance aggregation and processing large datasets, such as those with over 1 million records. Its compiled nature provides the speed necessary for performance-critical tasks. (See `education/book-06-go-target/` for more).\n\n### Selection Criteria\n\n| Factor | AWK | Python | Go |\n| :--- | :--- | :--- | :--- |\n| **Input Format** | Text / CSV | JSON | High-volume streams |\n| **Logic Complexity** | Simple | Complex | Performance-focused |\n| **Speed** | Fast (Streaming) | Moderate | Very Fast (Compiled) |\n| **Use Case** | Line parsing | Library integration | Heavy aggregation |\n\nFor more information on coordinating these targets, see the orchestration guide in `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["target-selection", "language-strengths", "optimization"], "original_answer": "## Target Selection Criteria\n\n### Language Strengths\n\n| Target | Best For | Characteristics |\n|--------|----------|------------------|\n| AWK | Line parsing | Fast, streaming |\n| Python | Complex logic | Flexible, libraries |\n| Go | Aggregation | Fast, compiled |\n| C# | Recursive queries | Fixpoint, .NET |\n| Bash | Glue code | System integration |\n\n### Selection Factors\n\n1. **Input/Output format**: AWK for text, Python for JSON\n2. **Complexity**: Simple \u2192 AWK, Complex \u2192 Python\n3. **Performance**: Critical \u2192 Go/C#\n4. **Libraries**: ML \u2192 Python, .NET \u2192 C#\n5. **Deployment**: Unix \u2192 Bash, Cross-platform \u2192 .NET\n\n### Decision Example\n\n```\nParsing CSV? \u2192 AWK\nML inference? \u2192 Python\n1M+ records? \u2192 Go\nRecursive? \u2192 C# Query Runtime\n```", "answer_tailored": true}
{"pair_id": "b4-c3-target-selection_p2", "cluster_id": "b4-c3-target-selection", "question": "What are the criteria for target selection?", "question_type": "medium", "answer": "UnifyWeaver selects compilation targets by matching the specific technical requirements of a task\u2014such as performance, data format, and logic complexity\u2014to the inherent strengths of each supported language.\n\n### Core Selection Factors\n- **Data Format & I/O**: Use AWK for streaming text and line-based parsing; select Python for complex JSON processing.\n- **Logic Complexity**: Simple filters and transformations favor AWK, while Python is the standard for intricate business logic and external library usage.\n- **Performance Requirements**: For high-throughput aggregation or processing over 1 million records, Go or C# are the preferred high-performance targets.\n- **Query Characteristics**: C# is specifically utilized for recursive queries and fixpoint calculations via its specialized query runtime.\n- **Environment & Deployment**: Bash provides lightweight system glue for Unix environments, while C# offers robust cross-platform support within the .NET ecosystem.\n\n### Target Mapping Summary\n| Target | Best For | Key Characteristic |\n| :--- | :--- | :--- |\n| **AWK** | CSV/Text parsing | High-speed streaming |\n| **Python** | ML and complex logic | Flexible library ecosystem |\n| **Go** | Data aggregation | Fast, compiled execution |\n| **C#** | Recursive queries | Fixpoint and .NET integration |\n| **Bash** | System glue code | Native Unix integration |\n\nFor detailed examples of decision logic in pipeline orchestration, refer to the documentation in `education/book-04-workflows/`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["target-selection", "language-strengths", "optimization"], "original_answer": "## Target Selection Criteria\n\n### Language Strengths\n\n| Target | Best For | Characteristics |\n|--------|----------|------------------|\n| AWK | Line parsing | Fast, streaming |\n| Python | Complex logic | Flexible, libraries |\n| Go | Aggregation | Fast, compiled |\n| C# | Recursive queries | Fixpoint, .NET |\n| Bash | Glue code | System integration |\n\n### Selection Factors\n\n1. **Input/Output format**: AWK for text, Python for JSON\n2. **Complexity**: Simple \u2192 AWK, Complex \u2192 Python\n3. **Performance**: Critical \u2192 Go/C#\n4. **Libraries**: ML \u2192 Python, .NET \u2192 C#\n5. **Deployment**: Unix \u2192 Bash, Cross-platform \u2192 .NET\n\n### Decision Example\n\n```\nParsing CSV? \u2192 AWK\nML inference? \u2192 Python\n1M+ records? \u2192 Go\nRecursive? \u2192 C# Query Runtime\n```", "answer_tailored": true}
{"pair_id": "b4-c3-pipeline-composition_p0", "cluster_id": "b4-c3-pipeline-composition", "question": "How do pipeline stages communicate in UnifyWeaver?", "question_type": "medium", "answer": "In UnifyWeaver, pipeline stages communicate by passing structured data through standard I/O streams or temporary storage. Data is typically piped directly from one stage's stdout to the next stage's stdin, though intermediate temporary files are used when persistence or large data volumes are required.\n\nCommunication relies on three primary data formats:\n- **JSON**: For complex, nested data structures.\n- **CSV/TSV**: For tabular information.\n- **Line-delimited**: For simple, streaming records.\n\nThe orchestrator manages this communication by validating stage outputs, routing data to downstream components, and handling errors. Error management includes monitoring stderr, executing retry logic, and managing fallback stages when error thresholds are met. For more information on configuring these flows, see `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-composition", "data-flow", "serialization"], "original_answer": "## Stage Communication\n\n### Data Formats\n\nStages communicate via standard I/O using:\n- **JSON**: Complex nested data\n- **CSV/TSV**: Tabular data\n- **Line-delimited**: Simple records\n\n### Data Flow\n\n```\nStage A (stdout) \u2192 | pipe | \u2192 Stage B (stdin)\n```\n\nOr via temporary files:\n```\nStage A \u2192 /tmp/stage_a.json \u2192 Stage B\n```\n\n### Error Handling\n\n1. **Error channel**: stderr or error files\n2. **Retry logic**: Orchestrator retries failed stages\n3. **Fallback**: Alternative stage on failure\n4. **Thresholds**: Stop after N errors\n\n### Orchestrator Role\n\n- Parses stage output\n- Validates formats\n- Routes to next stage\n- Manages error recovery", "answer_tailored": true}
{"pair_id": "b4-c3-pipeline-composition_p1", "cluster_id": "b4-c3-pipeline-composition", "question": "What data formats are used between pipeline stages?", "question_type": "medium", "answer": "UnifyWeaver pipelines utilize several standard data formats for communication between stages, primarily leveraging standard I/O or temporary files for the transfer. The specific formats used are:\n\n*   **JSON**: For complex, nested data structures.\n*   **CSV/TSV**: For tabular data.\n*   **Line-delimited**: For simple, individual records.\n\nData typically flows between stages via pipes (mapping Stage A\u2019s stdout to Stage B\u2019s stdin) or through intermediate files (e.g., `/tmp/stage_a.json`). The system's orchestrator is responsible for parsing these outputs, validating the formats, and routing the data to the next stage while managing any necessary error recovery. For a deeper look into how these formats are managed within a workflow, see `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-composition", "data-flow", "serialization"], "original_answer": "## Stage Communication\n\n### Data Formats\n\nStages communicate via standard I/O using:\n- **JSON**: Complex nested data\n- **CSV/TSV**: Tabular data\n- **Line-delimited**: Simple records\n\n### Data Flow\n\n```\nStage A (stdout) \u2192 | pipe | \u2192 Stage B (stdin)\n```\n\nOr via temporary files:\n```\nStage A \u2192 /tmp/stage_a.json \u2192 Stage B\n```\n\n### Error Handling\n\n1. **Error channel**: stderr or error files\n2. **Retry logic**: Orchestrator retries failed stages\n3. **Fallback**: Alternative stage on failure\n4. **Thresholds**: Stop after N errors\n\n### Orchestrator Role\n\n- Parses stage output\n- Validates formats\n- Routes to next stage\n- Manages error recovery", "answer_tailored": true}
{"pair_id": "b4-c3-pipeline-composition_p2", "cluster_id": "b4-c3-pipeline-composition", "question": "How do you handle errors across pipeline stages?", "question_type": "medium", "answer": "In UnifyWeaver, error handling across pipeline stages is managed by the orchestrator using a multi-layered approach to ensure data integrity and system resilience. Errors are primarily isolated using the `stderr` stream or dedicated error logs, keeping them separate from the main data flow (which uses JSON, CSV, or line-delimited formats via standard I/O).\n\nKey error management strategies include:\n*   **Retry Mechanisms**: The orchestrator automatically retries stages that fail due to transient issues.\n*   **Fallback Routing**: If a primary stage fails, the system can divert processing to an alternative stage.\n*   **Error Thresholds**: Pipelines can be configured to terminate after reaching a specific count of failures (N errors) to prevent cascading issues.\n*   **Recovery Management**: The orchestrator actively monitors stage outputs, validates data formats, and executes recovery logic to maintain the pipeline's state.\n\nFor comprehensive details on managing these flows, refer to the documentation in `education/book-04-workflows/03_pipeline_orchestration.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/03_pipeline_orchestration.md", "topics": ["pipeline-composition", "data-flow", "serialization"], "original_answer": "## Stage Communication\n\n### Data Formats\n\nStages communicate via standard I/O using:\n- **JSON**: Complex nested data\n- **CSV/TSV**: Tabular data\n- **Line-delimited**: Simple records\n\n### Data Flow\n\n```\nStage A (stdout) \u2192 | pipe | \u2192 Stage B (stdin)\n```\n\nOr via temporary files:\n```\nStage A \u2192 /tmp/stage_a.json \u2192 Stage B\n```\n\n### Error Handling\n\n1. **Error channel**: stderr or error files\n2. **Retry logic**: Orchestrator retries failed stages\n3. **Fallback**: Alternative stage on failure\n4. **Thresholds**: Stop after N errors\n\n### Orchestrator Role\n\n- Parses stage output\n- Validates formats\n- Routes to next stage\n- Manages error recovery", "answer_tailored": true}
