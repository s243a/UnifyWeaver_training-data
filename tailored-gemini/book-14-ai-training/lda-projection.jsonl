{"pair_id": "ai-lda-001_p0", "cluster_id": "ai-lda-001", "question": "What is LDA projection and how does it work?", "question_type": "medium", "answer": "LDA (Linear Discriminant Analysis) projection is a supervised alignment technique that transforms query embeddings to better match the semantic space of their corresponding answers. It works by applying a learned projection matrix, **W**, to a raw query embedding via matrix multiplication:\n\n```\nprojected_query = query_embedding @ W\n```\n\n**How it works:**\n*   **Dimensions:** A query embedding (e.g., shape 384,) is multiplied by the learned matrix **W** (shape 384x384) to produce a `projected_query`.\n*   **Alignment:** The matrix is trained to minimize the distance between a question's vector and the vector of its correct answer.\n*   **Impact:** By mapping the \"question space\" into the \"answer space,\" retrieval recall typically improves from around 70% to 93%. This ensures that a user query like \"How do I read CSV files?\" achieves a high similarity score with technical documentation like \"The `read_csv` function loads...\", even if the literal wording differs significantly.\n\nFor more details on training these projections, refer to `education/book-14-ai-training/03_lda_projection.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/03_lda_projection.md", "topics": [], "original_answer": "LDA projection learns a matrix W that transforms question embeddings to be closer to their answer embeddings:\n\n```\nprojected_query = query_embedding @ W\n```\n\n**Dimensions:**\n- `query_embedding`: shape (d,) - e.g., 384 dimensions\n- `W`: shape (d, d) - the learned projection\n- `projected_query`: shape (d,) - now closer to answer space\n\n**Example improvement:**\n```\nQuery:    \"How do I read a CSV file?\"\nDocument: \"The pandas.read_csv() function loads comma-separated...\"\n\nDirect similarity: 0.45 (mediocre)\nAfter projection:  0.89 (much better)\n```\n\nTypical recall improvement: 70% \u2192 93%.", "answer_tailored": true}
{"pair_id": "ai-lda-001_p0", "cluster_id": "ai-lda-001", "question": "How is the W matrix computed?", "question_type": "medium", "answer": "The $W$ matrix is computed using the Moore-Penrose **pseudoinverse** to establish an optimal linear mapping between question and answer embeddings. Given a set of question embeddings ($Q$) and corresponding answer embeddings ($A$), the objective is to find $W$ such that $QW \\approx A$, minimizing the squared reconstruction error.\n\nThe formal solution is:\n**$W = \\text{pinv}(Q) \\times A$**\n\nIn the UnifyWeaver training pipeline, this is typically implemented with a regularization term to ensure numerical stability when $Q$ is not full rank:\n\n```python\nimport numpy as np\n\ndef calculate_projection_W(Q, A, alpha=1e-6):\n    # Q: stack of question embeddings (n x d)\n    # A: stack of answer embeddings (n x d)\n    d = Q.shape[1]\n    \n    # Solve for W using the regularized normal equation:\n    # (Q.T @ Q + alpha * I) @ W = Q.T @ A\n    lhs = Q.T @ Q + alpha * np.eye(d)\n    rhs = Q.T @ A\n    W = np.linalg.solve(lhs, rhs)\n    \n    return W\n```\n\nThis computation allows the system to project a user's query into the semantic space of potential answers during retrieval. For further technical details on LDA projection, see `education/book-14-ai-training/03_lda_projection.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/03_lda_projection.md", "topics": [], "original_answer": "Using the Moore-Penrose **pseudoinverse**:\n\n```\nQ = stack of question embeddings  (n \u00d7 d)\nA = stack of answer embeddings    (n \u00d7 d)\n\nGoal: Find W such that Q @ W \u2248 A\n\nSolution: W = pinv(Q) @ A\n```\n\n**Why pseudoinverse?**\n- Minimizes ||Q @ W - A||\u00b2 (reconstruction error)\n- Works even when Q is not square or full rank\n- Numerically stable with regularization\n\n```python\nimport numpy as np\n\ndef compute_W(question_embeddings, answer_embeddings, regularization=1e-6):\n    Q = question_embeddings\n    A = answer_embeddings\n    d = Q.shape[1]\n    \n    # Regularized pseudoinverse: (Q^T Q + \u03bbI)^-1 Q^T\n    QTQ = Q.T @ Q + regularization * np.eye(d)\n    W = np.linalg.solve(QTQ, Q.T @ A)\n    \n    return W\n```", "answer_tailored": true}
{"pair_id": "ai-lda-001_p0", "cluster_id": "ai-lda-001", "question": "What's the limitation of single W matrix projection?", "question_type": "medium", "answer": "The main limitation of using a single W matrix projection is the assumption of a single global mapping between questions and answers. This \"one-size-fits-all\" approach struggles with diverse datasets where different question types\u2014such as those focusing on authentication, CSV formats, or performance optimization\u2014require distinct semantic mappings to retrieve the best results. Because a single matrix cannot accommodate these varying domain-specific needs, the system instead moves toward multi-head routing and cluster-specific projections. Detailed information on this solution can be found in `education/book-14-ai-training/04_multi_head_routing.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/03_lda_projection.md", "topics": [], "original_answer": "A single W matrix assumes one global mapping from questions to answers.\n\n**Works well when:**\n- Questions are semantically similar\n- There's one dominant pattern\n\n**Struggles when:**\n- Questions span diverse topics\n- Different question types need different mappings\n\n**Examples of different question types:**\n- \"How do I authenticate?\" \u2192 needs auth-domain knowledge\n- \"What's the CSV format?\" \u2192 needs data-domain knowledge\n- \"Why is my query slow?\" \u2192 needs performance-domain knowledge\n\n**Solution:** Multi-head routing (Chapter 4) with cluster-specific projections.", "answer_tailored": true}
