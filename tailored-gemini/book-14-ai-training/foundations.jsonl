{"pair_id": "ai-foundations-001_p0", "cluster_id": "ai-foundations-001", "question": "What is the asymmetric semantics problem?", "question_type": "medium", "answer": "The asymmetric semantics problem is the linguistic mismatch between informal user queries and formal technical documentation. Queries are typically short, exploratory questions (e.g., \"What's the syntax for loops?\"), whereas documents are detailed, authoritative answers (e.g., \"The `for` statement iterates over...\"). This \"asymmetry\" means that direct cosine similarity between raw embeddings often fails to find the best match because the vector for a question doesn't naturally cluster with the vector for its answer. UnifyWeaver addresses this by learning a projection matrix $W$ that transforms question embeddings to align more closely with the document's answer embeddings: `question_emb \u2500\u2500\u25ba W \u2500\u2500\u25ba projected_emb \u2248 answer_emb`. For more information on this retrieval strategy, refer to `education/book-14-ai-training/01_introduction.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/01_introduction.md", "topics": [], "original_answer": "Users phrase queries differently than how answers are written:\n\n| User Query | Document Text |\n|------------|---------------|\n| \"How do I log in?\" | \"Authentication requires valid credentials...\" |\n| \"What's the syntax for loops?\" | \"The `for` statement iterates over...\" |\n| \"Why is my code slow?\" | \"Performance optimization involves...\" |\n\n**Key insight:**\n- Queries are **questions** - short, informal, seeking\n- Documents are **answers** - detailed, formal, providing\n\nDirect cosine similarity between query and document embeddings often misses the best matches.\n\n**Solution:** Learn a projection matrix W that maps question embeddings toward their corresponding answer embeddings:\n\n```\nquestion_emb \u2500\u2500\u25ba W \u2500\u2500\u25ba projected_emb \u2248 answer_emb\n```", "answer_tailored": true}
{"pair_id": "ai-foundations-001_p0", "cluster_id": "ai-foundations-001", "question": "What embedding models does UnifyWeaver support?", "question_type": "medium", "answer": "UnifyWeaver supports two primary embedding models for its semantic search and retrieval systems, catering to different performance and context requirements:\n\n**1. all-MiniLM-L6-v2 (Default)**\nThis is the standard model used for most general-purpose tasks due to its high efficiency and speed.\n- **Dimensions:** 384\n- **Context Window:** 256 tokens\n- **Best Use:** Rapid processing of short queries and general search tasks.\n\n```python\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembedding = model.encode(\"How do I log in?\")\n```\n\n**2. nomic-embed-text-v1.5 (High Context)**\nThis model is preferred for complex retrieval scenarios involving large documents or specialized search patterns.\n- **Dimensions:** 768\n- **Context Window:** 8192 tokens\n- **Key Feature:** Supports asymmetric prefixes to distinguish between queries and documents.\n\n```python\nmodel = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\nquery_emb = model.encode(\"search_query: How do I log in?\")\ndoc_emb = model.encode(\"search_document: Authentication requires...\")\n```\n\n| Requirement | Recommended Model |\n|-------------|-------------------|\n| Fast, short queries | all-MiniLM-L6-v2 |\n| Extensive documentation | nomic-embed-text-v1.5 |\n| Asymmetric retrieval | nomic-embed-text-v1.5 |\n\nFor comprehensive details on configuring these providers, refer to the documentation in `education/book-14-ai-training/`.", "answer_variant": "default", "answer_source": "book-14-ai-training/02_embedding_providers.md", "topics": [], "original_answer": "Two main embedding models:\n\n**all-MiniLM-L6-v2 (Default):**\n- 384 dimensions\n- 256 tokens context\n- Very fast\n- Good for general use\n\n```python\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembedding = model.encode(\"How do I log in?\")\n```\n\n**nomic-embed-text-v1.5 (Large Context):**\n- 768 dimensions\n- 8192 tokens context\n- Supports asymmetric prefixes\n\n```python\nmodel = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\nquery_emb = model.encode(\"search_query: How do I log in?\")\ndoc_emb = model.encode(\"search_document: Authentication requires...\")\n```\n\n| Use Case | Recommended |\n|----------|-------------|\n| Short queries | all-MiniLM-L6-v2 |\n| Long documents | nomic-embed-text-v1.5 |\n| Asymmetric search | nomic with prefixes |", "answer_tailored": true}
{"pair_id": "ai-foundations-001_p0", "cluster_id": "ai-foundations-001", "question": "How do I choose between embedding models?", "question_type": "medium", "answer": "Selecting the appropriate embedding model depends on your specific use case, data volume, and deployment environment. \n\n**Model Selection Criteria:**\n*   **all-MiniLM-L6-v2**: Choose this for short queries (under 256 tokens) or when operating in resource-constrained environments. It offers the fastest processing speeds with solid quality.\n*   **nomic-embed-text-v1.5**: Opt for this model when handling long documents (up to 8K context) or when you need maximum representation quality. It is also the best choice for asymmetric search, using specific prefixes to distinguish between queries and documents.\n\n**Target Implementation:**\nUnifyWeaver supports model execution across different runtimes:\n*   **Python**: Best for full-featured implementations and GPU acceleration via `sentence-transformers`.\n*   **Go & Rust**: Utilize ONNX Runtime for high performance without Python dependencies. Rust specifically offers excellent zero-copy efficiency for CLI tools.\n\nAll targets maintain cross-compatibility by reading and writing NPY files. For more details on model configuration and provider setup, refer to `education/book-14-ai-training/02_embedding_providers.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/02_embedding_providers.md", "topics": [], "original_answer": "| Use Case | Recommended Model | Why |\n|----------|------------------|-----|\n| Short queries (<256 tokens) | all-MiniLM-L6-v2 | Fast, good quality |\n| Long documents | nomic-embed-text-v1.5 | 8K context |\n| Asymmetric search | nomic with prefixes | Query/doc distinction |\n| Resource constrained | all-MiniLM-L6-v2 | Smaller, faster |\n| Maximum quality | nomic-embed-text-v1.5 | Better representations |\n\n**Multi-target support:**\n- **Python**: Full-featured with GPU (sentence-transformers)\n- **Go**: ONNX Runtime (pure Go, no Python dependency)\n- **Rust**: ONNX (zero-copy, excellent for CLI tools)\n\nAll targets can read NPY files for cross-target compatibility.", "answer_tailored": true}
