{"id": "skill_synthetic_data_001", "question": "How do I generate Q&A training pairs from my documentation?", "question_variants": ["How can I create Q&A pairs from skills or docs?", "How do I turn documentation into training data?"], "level": 2, "tree_path": ["Quickstart", "Synthetic Data"], "answer": "Use the Q&A generation tool to automatically create training pairs from your skill documents:\n\n```bash\npython training-data/scripts/generate_qa_from_skills.py --skill skill_mindmap_linking.md --model haiku\n```\n\nOr generate from all skills at once:\n\n```bash\npython training-data/scripts/generate_qa_from_skills.py --all --provider gemini\n```\n\nThe tool outputs JSONL format with questions, answers, related skills, and tags. You can then review and edit the generated pairs. See skill_synthetic_data.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_qa_from_skills.py", "training-data/scripts/generate_quickstart_qa.py"], "tags": ["training data", "Q&A generation", "documentation", "LLM"]}
{"id": "skill_synthetic_data_002", "question": "How do I expand and reword training answers with an LLM?", "question_variants": ["How can I create variations of my answers?", "How do I tailor answers for training data?"], "level": 2, "tree_path": ["Quickstart", "Synthetic Data"], "answer": "Use the answer tailoring tool to generate reworded versions of existing answers:\n\n```bash\npython scripts/generate_tailored_answers.py --input training-data/expanded --model sonnet\n```\n\nOr tailor answers from a specific file:\n\n```bash\npython scripts/generate_tailored_answers.py --file training-data/expanded/mindmap/pairs.jsonl --batch-size 5\n```\n\nYou can use Claude (sonnet, opus, haiku) or Gemini models. This expands your training dataset with natural answer variations. See skill_synthetic_data.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_answer_tailoring.md"], "related_docs": ["scripts/generate_tailored_answers.py"], "tags": ["training data", "answer expansion", "LLM", "augmentation"]}
{"id": "skill_synthetic_data_003", "question": "What's the workflow for bootstrapping training data from a new feature?", "question_variants": ["How do I create training data for a new capability?", "How do I set up training data from scratch?"], "level": 2, "tree_path": ["Quickstart", "Synthetic Data"], "answer": "Follow this pipeline: (1) Create a skill document (skills/skill_new_feature.md), (2) Generate Q&A pairs using `generate_qa_from_skills.py`, (3) Review and edit the generated pairs in training-data/by-topic/, (4) Expand clusters to individual pairs with `expand_clusters_to_pairs.py`, (5) Generate tailored answers using `generate_tailored_answers.py`. All tools support Claude and Gemini models. The output is JSONL format ready for training. See skill_synthetic_data.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_qa_generation.md"], "related_docs": ["scripts/expand_clusters_to_pairs.py"], "tags": ["training data", "workflow", "bootstrap", "feature"]}
{"id": "skill_synthetic_data_004", "question": "How do I generate training data from Pearltrees exports?", "question_variants": ["How can I create training datasets from Pearltrees RDF?", "How do I extract training targets from my Pearltrees data?"], "level": 2, "tree_path": ["Quickstart", "Synthetic Data"], "answer": "Use the Pearltrees dataset generation tool to convert RDF exports into training targets:\n\n```bash\npython scripts/generate_pearl_dataset.py --rdf data/s243a.rdf --output reports/pearltrees_targets.jsonl --query-style locate\n```\n\nThis generates structured JSONL training data from your Pearltrees collections. You can adjust the query-style parameter to customize how locations and relationships are extracted. The output integrates with your training pipeline. See skill_synthetic_data.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_pearl_dataset.md"], "related_docs": ["scripts/generate_pearl_dataset.py"], "tags": ["training data", "Pearltrees", "RDF", "dataset generation"]}
{"id": "skill_qa_generation_001", "question": "How do I create Q&A training data from my skill documentation?", "question_variants": ["How do I generate question-answer pairs from skill files?", "What's the process for creating training data from documentation?"], "level": 2, "tree_path": ["Quickstart", "Q&A Generation"], "answer": "Use `generate_qa_from_skills.py` to generate Q&A pairs from skill markdown files. Run `python training-data/scripts/generate_qa_from_skills.py --skill skill_mindmap_linking.md` for a single skill, or use `--all` to process all skill files. The tool extracts the 'When to Use' section for question ideas and examples for answers, then uses an LLM to generate structured JSONL output to `by-topic/<topic>/skills-generated.jsonl`. You can customize the LLM provider, model, and number of pairs with options like `--provider gemini --pairs 4`. See skill_qa_generation.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_answer_tailoring.md", "skill_qa_generation.md"], "related_docs": ["training-data/by-topic/SOURCE_MAPPING.md", "training-data/scripts/generate_qa_from_skills.py"], "tags": ["training-data", "qa-generation", "documentation", "llm"]}
{"id": "skill_qa_generation_002", "question": "What's the difference between the two Q&A generation tools?", "question_variants": ["When should I use generate_qa_from_skills vs generate_quickstart_qa?", "Which tool should I use for my training data?"], "level": 2, "tree_path": ["Quickstart", "Q&A Generation"], "answer": "Use `generate_qa_from_skills.py` for task-oriented Q&A from individual skill markdown files—it's best for how-to training data. Use `generate_quickstart_qa.py` for capability-based Q&A from SOURCE_MAPPING.md—it reads referenced source files and generates training data organized by topics. Choose `generate_qa_from_skills.py` if you're creating data from new skills, or `generate_quickstart_qa.py` if you're mapping existing source files to capabilities. Both output JSONL format with configurable LLM providers and models. See skill_qa_generation.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_answer_tailoring.md", "skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_quickstart_qa.py", "training-data/by-topic/SOURCE_MAPPING.md"], "tags": ["training-data", "qa-generation", "tools", "workflow"]}
{"id": "skill_qa_generation_003", "question": "How do I check what skill coverage gaps exist in my training data?", "question_variants": ["How do I find out which capabilities don't have corresponding skills?", "Can I analyze which skills are missing?"], "level": 3, "tree_path": ["Quickstart", "Q&A Generation"], "answer": "Run `python training-data/scripts/generate_qa_from_skills.py --coverage` to analyze skill coverage gaps. This command identifies which capabilities in your capability tree have corresponding skills (marked with checkmarks) and which are missing. The output shows suggested new skills to create, helping you prioritize what documentation to write next. This is useful for identifying gaps in your quickstart agent's training data. See skill_qa_generation.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_qa_from_skills.py"], "tags": ["training-data", "qa-generation", "coverage-analysis", "planning"]}
{"id": "skill_qa_generation_004", "question": "What should good Q&A pairs look like for training data?", "question_variants": ["How do I write quality questions and answers for training?", "What makes a good training question?"], "level": 2, "tree_path": ["Quickstart", "Q&A Generation"], "answer": "Good questions should be user-centric (what a new user would ask), avoid mentioning skill names since users don't know them, and include 2-3 variants for training diversity. Answers should be concise but complete, include relevant code examples and commands, and reference documentation paths for 'learn more'. The output JSONL format includes levels (0=identity, 1=capabilities, 2=general task, 3=specific task, 4=details), tree paths for organization, related skills/docs, and tags for discoverability. Focus on practical, actionable Q&A that helps new users accomplish real tasks. See skill_qa_generation.md for details.", "related_skills": ["skill_synthetic_data.md", "skill_answer_tailoring.md", "skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_qa_from_skills.py"], "tags": ["training-data", "qa-generation", "best-practices", "quality"]}
{"id": "skill_answer_tailoring_001", "question": "How do I create variations of my training data answers?", "question_variants": ["How can I generate different versions of my Q&A answers?", "What's the best way to reword answers for training data diversity?", "How do I augment my training data with answer variations?"], "level": 2, "tree_path": ["Quickstart", "Answer Tailoring"], "answer": "Use the generate_tailored_answers.py script to automatically reword your Q&A pairs. Run `python scripts/generate_tailored_answers.py --input training-data/expanded` to process all JSONL files in a directory. The script rewrites answers to directly address the question while maintaining semantic equivalence. Each pair gets a `answer_tailored: true` flag and the original answer is preserved as `original_answer`. See skill_answer_tailoring.md for details.", "related_skills": ["skill_answer_tailoring.md", "skill_synthetic_data.md", "skill_qa_generation.md"], "related_docs": ["scripts/generate_tailored_answers.py", "scripts/expand_clusters_to_pairs.py"], "tags": ["training-data", "data-augmentation", "llm", "answers"]}
{"id": "skill_answer_tailoring_002", "question": "How do I choose between different LLM providers for tailoring answers?", "question_variants": ["Which model should I use for answer rewriting?", "What's faster - Claude or Gemini for answer tailoring?", "Should I use haiku, sonnet, or opus for generating answer variations?"], "level": 3, "tree_path": ["Quickstart", "Answer Tailoring"], "answer": "Use `--provider` and `--model` flags to select your LLM. For bulk processing, use haiku or gemini-2.5-flash-preview (fast, low cost). For quality-sensitive data, use sonnet. Run `python scripts/generate_tailored_answers.py --input training-data/expanded --provider gemini --model gemini-2.5-flash-preview`. Claude providers (haiku, sonnet, opus) offer increasing quality at higher cost. See skill_answer_tailoring.md for details.", "related_skills": ["skill_answer_tailoring.md"], "related_docs": ["scripts/generate_tailored_answers.py"], "tags": ["llm-selection", "performance", "cost-optimization"]}
{"id": "skill_answer_tailoring_003", "question": "How do I handle rate limiting when processing large datasets?", "question_variants": ["My answer tailoring keeps timing out - how do I fix it?", "How can I safely process thousands of Q&A pairs without hitting API limits?", "What batch size and delay should I use?"], "level": 3, "tree_path": ["Quickstart", "Answer Tailoring"], "answer": "Adjust `--batch-size` and `--delay` parameters to control API rate. For rate limit safety, use smaller batches with longer delays: `python scripts/generate_tailored_answers.py --input training-data/expanded --batch-size 5 --delay 1.0`. For faster processing, increase batch size and decrease delay: `--batch-size 20 --delay 0.2`. If timeouts occur, reduce batch size further and increase delay to 2.0 seconds. See skill_answer_tailoring.md for details.", "related_skills": ["skill_answer_tailoring.md"], "related_docs": ["scripts/generate_tailored_answers.py"], "tags": ["rate-limiting", "api-optimization", "batch-processing"]}
{"id": "skill_answer_tailoring_004", "question": "How do I resume an interrupted answer tailoring job?", "question_variants": ["Can I restart my tailoring script without reprocessing everything?", "How do I continue where I left off?", "Does the script skip already-processed pairs?"], "level": 3, "tree_path": ["Quickstart", "Answer Tailoring"], "answer": "The script automatically resumes from where it stopped. Just run the same command again: `python scripts/generate_tailored_answers.py --input training-data/expanded`. It checks the output file for already-processed `pair_id` values and skips pairs with `answer_tailored: true`. To reprocess everything from scratch, use the `--no-skip` flag: `python scripts/generate_tailored_answers.py --input training-data/expanded --no-skip`. See skill_answer_tailoring.md for details.", "related_skills": ["skill_answer_tailoring.md"], "related_docs": ["scripts/generate_tailored_answers.py"], "tags": ["resumable-processing", "workflow", "error-recovery"]}
{"id": "skill_pearl_dataset_001", "question": "How do I create training data from Pearltrees?", "question_variants": ["How do I export Pearltrees for machine learning?", "How do I generate a dataset from my Pearltrees bookmarks?", "Can I use Pearltrees exports for semantic search training?"], "level": 2, "tree_path": ["Quickstart", "Synthetic Data", "Pearl Dataset Generation"], "answer": "Export your Pearltrees as RDF (from Settings > Export), then run:\n\n```bash\npython scripts/generate_pearl_dataset.py \\\n  --rdf data/export.rdf \\\n  --output reports/pearltrees_targets.jsonl\n```\n\nThis generates JSONL with materialized paths (e.g., `account > Root > Science > Physics`) suitable for semantic search training. Use `--query-style locate` for Prolog-style queries or `--query-style file` for bookmark filing. Add `--trees-only` or `--pearls-only` to filter content type.\n\nSee skill_pearl_dataset.md for details.", "related_skills": ["skill_pearl_dataset.md", "skill_synthetic_data.md"], "related_docs": ["scripts/generate_pearl_dataset.py", "scripts/generate_tree_refpearls.py", "scripts/generate_account_training_data.py"], "tags": ["pearltrees", "dataset", "rdf", "training-data", "semantic-search"]}
{"id": "skill_pearl_dataset_002", "question": "How do I generate targets for semantic search from my bookmarks?", "question_variants": ["How do I prepare bookmark data for embedding models?", "What's the format for semantic search training data?", "How do I create hierarchical training targets?"], "level": 2, "tree_path": ["Quickstart", "Synthetic Data", "Pearl Dataset Generation"], "answer": "Use `generate_pearl_dataset.py` to transform Pearltrees RDF into JSONL targets:\n\n```bash\npython scripts/generate_pearl_dataset.py \\\n  --rdf data/s243a.rdf \\\n  --query-style locate \\\n  --output reports/targets.jsonl\n```\n\nEach entry includes:\n- `target_text`: Materialized path (e.g., `s243a > Root > Science > Physics`)\n- `query`: Formatted query based on style (`locate_node(\"Physics\")`)\n- `cluster_id`: Parent folder URI\n- `type`: Tree, PagePearl, NotePearl, or RefPearl\n\nQuery styles: `raw` (title only), `locate` (Prolog), `file` (bookmark filing), `similar` (similarity search). Use `--pearls-only` for bookmarks only.\n\nSee skill_pearl_dataset.md for details.", "related_skills": ["skill_pearl_dataset.md", "skill_train_model.md"], "related_docs": ["scripts/generate_pearl_dataset.py"], "tags": ["semantic-search", "embeddings", "targets", "pearltrees", "bookmarks"]}
{"id": "skill_pearl_dataset_003", "question": "How do I handle multiple Pearltrees accounts in my dataset?", "question_variants": ["Can I combine datasets from different Pearltrees accounts?", "How do I track cross-account references in Pearltrees?", "How do I process multiple RDF files together?"], "level": 3, "tree_path": ["Quickstart", "Synthetic Data", "Pearl Dataset Generation", "Multi-Account"], "answer": "Process multiple RDF files with `--cross-account` to track references between accounts:\n\n```bash\npython scripts/generate_pearl_dataset.py \\\n  --rdf data/s243a.rdf data/groups.rdf \\\n  --cross-account \\\n  --output reports/multi_targets.jsonl\n```\n\nCross-account RefPearls show account boundaries: `s243a > Root > Shared > Physics → @other_account`\n\nTo extract single account:\n```bash\npython scripts/generate_account_training_data.py \\\n  --input reports/multi_targets.jsonl \\\n  --account s243a \\\n  --output reports/s243a_only.jsonl\n```\n\nSee skill_pearl_dataset.md for details.", "related_skills": ["skill_pearl_dataset.md"], "related_docs": ["scripts/generate_pearl_dataset.py", "scripts/generate_account_training_data.py", "scripts/pearltrees_multi_account_generator.py"], "tags": ["multi-account", "cross-references", "pearltrees", "rdf"]}
{"id": "skill_pearl_dataset_004", "question": "How do I add folder relationships to my Pearltrees dataset?", "question_variants": ["How do I generate RefPearls from hierarchy?", "Can I infer parent-child relationships from my data?", "How do I enrich my Pearltrees dataset with synthetic references?"], "level": 3, "tree_path": ["Quickstart", "Synthetic Data", "Pearl Dataset Generation", "RefPearl Generation"], "answer": "Generate synthetic RefPearls from existing hierarchy using `cluster_id` relationships:\n\n```bash\npython scripts/generate_tree_refpearls.py \\\n  --input reports/pearltrees_targets.jsonl \\\n  --output reports/targets_with_refs.jsonl\n```\n\nThis infers parent-child relationships and creates RefPearl entries marked with `_source: \"hierarchy_inference\"`. RefPearls represent folder references and cross-account links, enriching the dataset for training folder suggestion models.\n\nFull workflow:\n1. Generate base dataset with `generate_pearl_dataset.py`\n2. Add synthetic refs with `generate_tree_refpearls.py`\n3. Generate embeddings with `generate_embeddings.py`\n4. Train model with `train_pearltrees_federated.py`\n\nSee skill_pearl_dataset.md for details.", "related_skills": ["skill_pearl_dataset.md", "skill_train_model.md", "skill_bookmark_filing.md"], "related_docs": ["scripts/generate_tree_refpearls.py", "scripts/train_pearltrees_federated.py"], "tags": ["refpearls", "hierarchy", "synthetic-data", "relationships", "pearltrees"]}
{"id": "skill_synthetic_data_001", "question": "How do I generate training data from skill files?", "question_variants": ["Can I create Q&A pairs automatically?", "How do I bootstrap training data?"], "level": 2, "tree_path": ["Data", "Synthetic Data", "Q&A Generation"], "answer": "Use generate_qa_from_skills.py: `python training-data/scripts/generate_qa_from_skills.py --skill skill_mindmap_linking.md --model haiku`. For all skills: `--all`. Supports Claude and Gemini providers. Output goes to by-topic/<topic>/skills-generated.jsonl with question, variants, level, tree_path, answer, and related_skills fields.", "related_skills": ["skill_synthetic_data.md", "skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_qa_from_skills.py"], "tags": ["training", "qa", "generation", "skills"]}
{"id": "skill_synthetic_data_002", "question": "How do I reword or tailor generated answers?", "question_variants": ["Can I improve generated answers?", "How do I create answer variations?"], "level": 3, "tree_path": ["Data", "Synthetic Data", "Answer Tailoring"], "answer": "Use generate_tailored_answers.py: `python scripts/generate_tailored_answers.py --input training-data/expanded --model sonnet`. This uses LLMs to create reworded versions of existing answers for training diversity. Supports both Claude (--provider claude) and Gemini (--provider gemini) backends.", "related_skills": ["skill_synthetic_data.md", "skill_answer_tailoring.md"], "related_docs": ["scripts/generate_tailored_answers.py"], "tags": ["training", "tailoring", "answers", "llm"]}
{"id": "skill_synthetic_data_003", "question": "How do I generate Pearltrees training datasets?", "question_variants": ["Can I create training data from my Pearltrees?", "How do I export Pearltrees for training?"], "level": 3, "tree_path": ["Data", "Synthetic Data", "Pearltrees"], "answer": "Use generate_pearl_dataset.py: `python scripts/generate_pearl_dataset.py --rdf data/s243a.rdf --output reports/pearltrees_targets.jsonl --query-style locate`. This extracts hierarchical paths and generates JSONL with question, tree_path, and answer fields suitable for training embedding models or fine-tuning.", "related_skills": ["skill_synthetic_data.md", "skill_pearl_dataset.md"], "related_docs": ["scripts/generate_pearl_dataset.py"], "tags": ["pearltrees", "rdf", "training", "dataset"]}
{"id": "skill_qa_generation_001", "question": "What's the difference between generate_qa_from_skills and generate_quickstart_qa?", "question_variants": ["Which Q&A generator should I use?", "How do the Q&A tools differ?"], "level": 3, "tree_path": ["Data", "Synthetic Data", "Q&A Tools"], "answer": "generate_qa_from_skills.py reads skill markdown files and generates task-oriented Q&A. generate_quickstart_qa.py reads SOURCE_MAPPING.md and generates capability-based Q&A from source code references. Use skills for 'how do I' questions, quickstart for 'what can it do' questions.", "related_skills": ["skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_qa_from_skills.py", "training-data/scripts/generate_quickstart_qa.py"], "tags": ["qa", "generation", "comparison"]}
{"id": "skill_qa_generation_002", "question": "How do I check which skills are missing training data?", "question_variants": ["How do I find gaps in training coverage?", "Which skills need Q&A pairs?"], "level": 3, "tree_path": ["Data", "Synthetic Data", "Coverage"], "answer": "Run coverage analysis: `python training-data/scripts/generate_qa_from_skills.py --coverage`. This shows existing skills with checkmarks, missing skills (capability tree items without skill files), and suggests new skills to create. Use to prioritize Q&A generation efforts.", "related_skills": ["skill_qa_generation.md"], "related_docs": ["training-data/scripts/generate_qa_from_skills.py"], "tags": ["coverage", "analysis", "skills", "gaps"]}
{"id": "skill_qa_generation_003", "question": "What do the level numbers mean in training data?", "question_variants": ["How are Q&A levels defined?", "What's level 2 vs level 3?"], "level": 4, "tree_path": ["Data", "Synthetic Data", "Levels"], "answer": "Levels indicate question specificity: 0=Identity ('What is UnifyWeaver?'), 1=Capabilities ('What can it compile to?'), 2=General task ('How do I organize mindmaps?'), 3=Specific task ('How do I use MST clustering?'), 4=Details ('What's the --target-size parameter?'). Match level to question type for consistent training data.", "related_skills": ["skill_qa_generation.md"], "related_docs": ["training-data/by-topic/SOURCE_MAPPING.md"], "tags": ["levels", "specificity", "training"]}
{"id": "skill_json_sources_001", "question": "How do I read JSON data in UnifyWeaver?", "question_variants": ["Can I use JSON files as data sources?", "How do I query JSON with Prolog?"], "level": 2, "tree_path": ["Data", "Sources", "JSON"], "answer": "Declare a JSON source: `:- source(json, order_totals, [json_file('data/orders.json'), columns(['order.customer.name', 'items[0].product', 'items[0].total'])])`. Column names support dot notation and array indices. Use jsonpath('$.orders[*].total') for complex selectors. The predicate arity matches the column count.", "related_skills": ["skill_json_sources.md", "skill_transpiler_extension.md"], "related_docs": ["src/unifyweaver/sources/json_source.pl"], "tags": ["json", "source", "data", "query"]}
{"id": "skill_json_sources_002", "question": "How do I handle JSON Lines (JSONL) files?", "question_variants": ["Can I read JSONL files?", "How do I stream newline-delimited JSON?"], "level": 3, "tree_path": ["Data", "Sources", "JSON Lines"], "answer": "Use record_format(jsonl): `:- source(json, events, [json_file('data/events.jsonl'), record_format(jsonl), columns([jsonpath('$.type'), jsonpath('$.timestamp')])])`. This treats each line as an independent object. Control null handling with null_policy(fail), null_policy(skip), or null_policy(default('N/A')).", "related_skills": ["skill_json_sources.md"], "related_docs": ["src/unifyweaver/sources/json_source.pl"], "tags": ["jsonl", "streaming", "source"]}
{"id": "skill_json_sources_003", "question": "How do I get typed records from JSON?", "question_variants": ["Can I generate POCOs from JSON?", "How do I use schema with JSON sources?"], "level": 3, "tree_path": ["Data", "Sources", "JSON Schema"], "answer": "Use schema/1 for typed records: `:- source(json, products, [json_file('products.json'), schema([field(id, 'id', string), field(name, 'name', string), field(price, 'price', double)]), record_type('ProductRecord')])`. The C# backend generates a POCO; predicate arity must be 1 with return_object(true) implied.", "related_skills": ["skill_json_sources.md"], "related_docs": ["src/unifyweaver/sources/json_source.pl"], "tags": ["json", "schema", "record", "typed"]}
