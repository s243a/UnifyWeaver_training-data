{"pair_id": "stream-compilation_p0", "cluster_id": "stream-compilation", "question": "How does UnifyWeaver compile non-recursive predicates?", "question_type": "medium", "answer": "UnifyWeaver compiles non-recursive predicates into Unix-native data processing pipelines. This \"stream compilation\" strategy transforms Prolog's logical relationships into memory-efficient Bash code where data flows line-by-line between processes.\n\n### Core Compilation Principles\n- **Stream-Based Processing:** Logical conjunctions (AND) are implemented as nested loops within pipelines. Data is passed as line-delimited records, ensuring low memory overhead even for large datasets.\n- **Join Implementation:** For predicates requiring joins, the compiler generates `while read` loops that ingest an input stream and perform lookups against secondary data sources to find matching tuples.\n- **Rule Aggregation (OR Logic):** Multiple rules defining the same predicate are handled by concatenating the output of each rule block within a subshell `{ rule1; rule2; }` and piping the combined stream to `sort -u` for deduplication.\n\n### Technical Example: grandparent/2\nIn a Bash target, the predicate `grandparent(GP, GC) :- parent(GP, P), parent(P, GC)` is compiled into a pipeline:\n1. `parent_stream` outputs all `GP:P` pairs.\n2. The stream is piped into a `parent_join` function.\n3. The join function iterates over the input, matching the child `P` against the parent key in the second relation to produce `GP:GC`.\n4. `sort -u` ensures the final set contains no duplicates.\n\n### Handling Disjunctions\nWhen a predicate has multiple definitions, they are executed sequentially to simulate backtracking:\n```bash\nchild_stream() {\n    {\n        # Logic for Rule 1\n        find_via_mother;\n        # Logic for Rule 2\n        find_via_father;\n    } | sort -u\n}\n```\n\nFor a comprehensive guide on how logical predicates are mapped to shell pipelines, see the documentation in `education/book-02-bash-target/`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "stream-compilation_p1", "cluster_id": "stream-compilation", "question": "What is stream compilation in UnifyWeaver?", "question_type": "medium", "answer": "Stream compilation in UnifyWeaver is a technique used by the `stream_compiler` to transform non-recursive Prolog predicates into efficient Unix pipelines. This approach treats logical declarations as a data processing flow, where information moves line-by-line through shell functions to maintain a low memory footprint.\n\nKey aspects of this compilation model include:\n- **Pipeline Architecture:** Prolog logic is mapped to Bash pipes. Data flows sequentially, allowing the system to handle large datasets without loading them entirely into memory.\n- **Join Operations:** Logical AND conditions are implemented as nested loops. For example, a `grandparent` predicate is compiled into a function that reads a stream of `parent(GP, P)` pairs and joins them against a `parent(P, GC)` data source by matching the common variable.\n- **Disjunctions (OR):** When a predicate has multiple rules, the compiler concatenates the output of each rule branch.\n- **Deduplication:** To match Prolog's behavior, pipelines frequently use `sort -u` to remove redundant results from the final output.\n\nFor a detailed breakdown of how Prolog logic is converted into Bash streams, refer to `education/book-02-bash-target/02_stream_compilation.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "stream-compilation_p2", "cluster_id": "stream-compilation", "question": "How are Prolog joins compiled to Bash pipelines?", "question_type": "medium", "answer": "UnifyWeaver compiles Prolog joins into Bash by translating logical relationships into memory-efficient Unix pipelines. The `stream_compiler` handles non-recursive predicates by mapping them to functions that process data line-by-line, mimicking Prolog\u2019s backtracking through shell-based loops and stream processing.\n\nIn practice, a join like `grandparent(GP, GC) :- parent(GP, P), parent(P, GC)` is transformed into a pipeline where the first relation acts as a data source (`parent_stream`) and the subsequent relation acts as a joiner (`parent_join`). The joiner reads from standard input and, for each record, performs a nested lookup or loop against the second dataset to find matches where variables unify (e.g., `[[ \"$b\" == \"$c\" ]]`). Results are then output as colon-separated values.\n\nIf a predicate has multiple rules (representing OR conditions), the generated Bash code executes each rule's logic sequentially within a subshell or block and pipes the combined output to `sort -u` for deduplication. This architecture ensures that complex joins remain memory-efficient by leveraging standard Unix pipes rather than loading entire datasets into memory. For further details on stream compilation and Bash-specific implementations, see `education/book-02-bash-target/`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "stream-compilation_p3", "cluster_id": "stream-compilation", "question": "What does the stream_compiler generate?", "question_type": "medium", "answer": "The `stream_compiler` generates Unix pipeline code for non-recursive Prolog predicates, transforming logical relations into memory-efficient data processing streams. It translates Prolog logic into Bash functions where data flows line-by-line, ensuring that complex queries can be executed using standard shell utilities.\n\nKey components of the generated code include:\n- **Join Implementation:** The compiler generates nested loops to handle joins. For a predicate like `grandparent(GP, GC) :- parent(GP, P), parent(P, GC)`, it creates a join function that reads from a parent stream and matches the intermediate variable `P` against the keys of a second parent data source.\n- **OR Logic (Disjunctions):** For predicates defined by multiple rules, the compiler generates a code block that concatenates the outputs of each individual rule stream and pipes the combined result through `sort -u` for deduplication.\n- **Stream Processing:** The generated Bash utilizes `while IFS= read` loops and string splitting (e.g., `read -r a b <<< \"$input\"`) to process data records delimited by colons, maintaining a low memory footprint.\n\nFor more information on the compilation strategy and generated Bash structures, see `education/book-02-bash-target/02_stream_compilation.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["stream", "pipeline", "join", "non-recursive", "unix-pipes"], "original_answer": "## Stream Compilation\n\nThe `stream_compiler` handles non-recursive predicates by generating Unix pipeline code.\n\n### The Philosophy\n\nProlog logic becomes a data processing pipeline:\n- Data flows line by line (memory efficient)\n- Joins become nested loops over streams\n- OR conditions become concatenated outputs\n\n### Example: grandparent/2\n\n**Prolog:**\n```prolog\ngrandparent(GP, GC) :-\n    parent(GP, P),\n    parent(P, GC).\n```\n\n**Generated Bash:**\n```bash\nparent_join() {\n    while IFS= read -r input; do\n        IFS=\":\" read -r a b <<< \"$input\"\n        for key in \"${!parent_data[@]}\"; do\n            IFS=\":\" read -r c d <<< \"$key\"\n            [[ \"$b\" == \"$c\" ]] && echo \"$a:$d\"\n        done\n    done\n}\n\ngrandparent() {\n    parent_stream | parent_join | sort -u\n}\n```\n\n### How It Works\n\n1. `parent_stream` outputs all parent:child pairs\n2. Each line flows through `parent_join`\n3. For each (a:b), check all parent data (c:d)\n4. If b==c (child matches parent), output a:d\n5. `sort -u` removes duplicates\n\n### Handling Multiple Rules (OR)\n\n```prolog\nchild(C) :- parent(M, C), female(M).\nchild(C) :- parent(F, C), male(F).\n```\n\n**Generated:**\n```bash\nchild_stream() {\n    {\n        # First rule (mother)\n        generate_children_from_mothers;\n        # Second rule (father)\n        generate_children_from_fathers;\n    } | sort -u\n}\n```\n\nBoth rule outputs are concatenated, then deduplicated.", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p0", "cluster_id": "bash-pipeline-patterns", "question": "Why does UnifyWeaver use Unix pipelines?", "question_type": "medium", "answer": "UnifyWeaver utilizes Unix pipelines to implement a stream-based compilation model that prioritizes memory efficiency and modularity. By piping data between stages, the system avoids the O(n) memory cost associated with loading large datasets into memory, instead maintaining a constant O(1) footprint for most operations.\n\n### Core Benefits of the Pipeline Approach\n- **Lazy Evaluation:** Data is processed only as it is needed by the next stage of the pipeline, reducing unnecessary computation.\n- **Parallelism:** Unix pipelines inherently execute different stages concurrently, allowing the system to leverage multiple CPU cores.\n- **Composability:** Complex Prolog queries are decomposed into a series of small, battle-tested tools like `awk`, `grep`, and `sort`.\n- **Streaming Patterns:** The compiler maps logic into standard Unix patterns:\n    - **Filter:** Using `grep` to select specific records.\n    - **Transform:** Utilizing `while read` loops or `awk` to modify columns.\n    - **Join:** Performing stream-based lookups between different data sources.\n    - **Aggregate:** Combining `sort` and `uniq -c` for efficient counting and summarization.\n\n### Memory Management Strategy\n| Strategy | Memory Profile | UnifyWeaver Usage |\n| :--- | :--- | :--- |\n| Load-all | O(n) | Avoided to prevent memory exhaustion |\n| Stream-processing | O(1) | The default approach for query execution |\n| Fact-caching | O(n) | Reserved for associative arrays when handling static facts |\n\nFor detailed implementation patterns and Bash target specifics, refer to the documentation in `education/book-02-bash-target/02_stream_compilation.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p1", "cluster_id": "bash-pipeline-patterns", "question": "What are the benefits of stream-based compilation?", "question_type": "medium", "answer": "Stream-based compilation in UnifyWeaver offers significant advantages in memory management, execution speed, and architectural modularity. By leveraging Unix-style pipelines, the compiler ensures that data processing remains efficient even when handling massive datasets.\n\n### Key Benefits\n\n*   **Memory Efficiency:** Unlike traditional approaches that load entire datasets into memory (O(n) complexity), UnifyWeaver\u2019s stream-based approach maintains an **O(1) memory footprint**. By processing records sequentially rather than buffering them, the system can handle files much larger than the available RAM.\n*   **Pipeline Patterns:** Logic is compiled into standard Unix pipeline structures, enabling efficient operations:\n    *   **Filtering:** Narrowing data streams using patterns (e.g., via `grep`).\n    *   **Transformation:** Reformatting or restructuring fields on the fly (e.g., swapping columns).\n    *   **Joining:** Correlating data from multiple sources through sequential lookups.\n    *   **Aggregation:** Calculating summaries or counts using standard utilities like `sort` and `uniq`.\n*   **Concurrent Parallelism:** Because pipeline stages run concurrently, the system naturally benefits from multi-core processors without requiring complex threading logic in the source code.\n*   **Lazy Evaluation:** Data is only processed as downstream components request it, which optimizes resource usage and reduces unnecessary computation.\n*   **Composability:** Complex logic is decomposed into combinations of small, battle-tested tools, ensuring reliability and making the generated code easier to debug.\n\n| Approach | Memory Complexity | UnifyWeaver Usage |\n|----------|-------------------|-------------------|\n| Load all data | O(n) | Generally avoided |\n| Stream processing | O(1) | Default compilation mode |\n| Associative arrays | O(n) | Reserved for facts and lookups |\n\nFor a deeper dive into stream-based compilation and Bash pipeline implementation, see the documentation in `education/book-02-bash-target/`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p2", "cluster_id": "bash-pipeline-patterns", "question": "How is memory managed in generated Bash code?", "question_type": "medium", "answer": "UnifyWeaver manages memory in generated Bash code by prioritizing **stream-based compilation**, which ensures data is processed record-by-record rather than being loaded entirely into memory. This methodology typically maintains **O(1) memory complexity** for most operations, avoiding the performance degradation and shell limits associated with O(n) bulk loading.\n\nKey memory management techniques include:\n\n*   **Lazy Evaluation:** Utilizing Unix pipes (`|`) and `while read` loops to process data only as needed.\n*   **Pipeline Parallelism:** Allowing different stages of a query (filtering, transformation, and joining) to run concurrently as separate processes, each with its own minimal memory footprint.\n*   **Native Tooling Integration:** Leveraging battle-tested utilities like `grep`, `awk`, and `sort` which are optimized for stream processing.\n\n| Approach | Memory Profile | UnifyWeaver Usage |\n|----------|----------------|-------------------|\n| Bulk Variable Loading | O(n) | Strictly avoided |\n| Stream Processing | O(1) | Primary default |\n| Associative Arrays | O(n) | Reserved for facts and state tracking |\n\nWhile most streaming operations are O(1), memory usage may scale to O(n) when the compiler utilizes associative arrays to store specific facts or lookup tables. For a comprehensive technical breakdown of these patterns, see `education/book-02-bash-target/02_stream_compilation.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
{"pair_id": "bash-pipeline-patterns_p3", "cluster_id": "bash-pipeline-patterns", "question": "What makes stream compilation efficient?", "question_type": "medium", "answer": "Stream compilation achieves efficiency primarily through **$O(1)$ memory management** and **pipeline parallelism**. By processing data record-by-record rather than loading entire datasets into memory ($O(n)$), UnifyWeaver ensures that even massive files can be handled with minimal resource overhead.\n\n### Key Drivers of Efficiency\n\n1.  **Lazy Evaluation:** Data is only retrieved and processed as required by subsequent stages, preventing unnecessary computation.\n2.  **Pipeline Concurrency:** In the Bash target, stages run simultaneously as separate processes. For example, a `stream | grep pattern` operation allows the producer and the filter to work in parallel.\n3.  **Standard Tool Integration:** Performance is boosted by leveraging highly optimized, battle-tested utilities like `grep`, `awk`, and `sort` for filtering and aggregation.\n4.  **Minimal Memory Footprint:** While facts might be stored in associative arrays ($O(n)$), the primary data flow remains $O(1)$ through sequential processing loops.\n\n### Efficiency Patterns\n\n*   **Filtering:** Using tools like `grep` to discard irrelevant records early in the pipeline.\n*   **Transformation:** Utilizing `while read` loops to modify fields on the fly without intermediate storage.\n*   **Joins and Aggregates:** Implementing stream-based lookups and sorted counting (`sort | uniq -c`) to maintain high throughput.\n\n| Strategy | Memory Complexity | UnifyWeaver Usage |\n| :--- | :--- | :--- |\n| **Load-All Approach** | $O(n)$ | Avoided |\n| **Stream Processing** | $O(1)$ | Default Architecture |\n| **Associative Arrays**| $O(n)$ | Restricted to specific facts |\n\nFor a comprehensive breakdown of stream-based execution, see `education/book-02-bash-target/02_stream_compilation.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/02_stream_compilation.md", "topics": ["pipeline", "streaming", "memory-efficient", "unix"], "original_answer": "## Benefits of Stream-Based Compilation\n\n### Memory Efficiency\n\nStream processing avoids loading entire datasets into memory:\n\n```bash\n# Bad: Load everything first\nall_data=$(cat large_file.csv)\nprocess \"$all_data\"\n\n# Good: Stream processing (UnifyWeaver approach)\ncat large_file.csv | while read line; do\n    process \"$line\"\ndone\n```\n\n### Pipeline Patterns\n\n**Filter Pattern:**\n```bash\nstream | grep pattern\n```\n\n**Transform Pattern:**\n```bash\nstream | while IFS=: read a b; do\n    echo \"$b:$a\"  # Swap columns\ndone\n```\n\n**Join Pattern:**\n```bash\nstream1 | while read key; do\n    lookup_in_stream2 \"$key\"\ndone\n```\n\n**Aggregate Pattern:**\n```bash\nstream | sort | uniq -c  # Count occurrences\n```\n\n### Why Pipelines Work\n\n1. **Lazy evaluation** - Data processed as needed\n2. **Parallelism** - Pipeline stages run concurrently\n3. **Composability** - Small tools combine into complex operations\n4. **Standard tooling** - grep, awk, sort are battle-tested\n\n### Memory Comparison\n\n| Approach | Memory Use | UnifyWeaver |\n|----------|------------|-------------|\n| Load all data | O(n) | Not used |\n| Stream processing | O(1) | Default |\n| Associative arrays | O(n) | For facts only |", "answer_tailored": true}
