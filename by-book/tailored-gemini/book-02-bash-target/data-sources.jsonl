{"pair_id": "data-sources-overview_p0", "cluster_id": "data-sources-overview", "question": "What data sources does UnifyWeaver support?", "question_type": "medium", "answer": "UnifyWeaver supports several external data source types, primarily **CSV**, **HTTP**, **JSON**, and **Python**. These sources are defined using the `source/3` predicate in Prolog and can be compiled into executable target code (like Bash) for data ingestion and processing.\n\n### Supported Data Source Types\n\n| Type | Description | Key Options |\n|------|-------------|-------------|\n| **CSV** | Flat files (CSV/TSV) | `csv_file`, `has_header`, `delimiter` |\n| **HTTP** | External Web APIs | `url`, `headers`, `cache_duration` |\n| **JSON** | Structured JSON files | `json_file`, `json_path` |\n| **Python** | Complex logic/transforms | `python_inline`, `python_file` |\n\n### Defining a Source\n\nData sources are declared using the following syntax:\n```prolog\nsource(Type, Name, Options).\n```\n\nFor example, to define a CSV-based user list:\n```prolog\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n```\n\nOnce defined, these sources can be compiled into Bash functions using `compile_dynamic_source/3`, allowing them to be used in pipelines or looked up by key. The generated code handles row streaming and key-based lookups automatically.\n\nFor more details on configuring data pipelines and sources, refer to `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-overview_p1", "cluster_id": "data-sources-overview", "question": "How do I use external data in UnifyWeaver?", "question_type": "medium", "answer": "External data is integrated into UnifyWeaver by defining data sources in Prolog using the `source/3` predicate and compiling them into target-language scripts. This abstraction allows you to treat external files or APIs as standard Prolog predicates.\n\n### Available Source Types\n| Type | Description | Key Options |\n|------|-------------|-------------|\n| `csv` | Flat files (CSV/TSV) | `csv_file`, `delimiter`, `has_header` |\n| `http` | External Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | Structured JSON data | `json_file`, `json_path` |\n| `python` | Custom transformations | `python_inline`, `python_file` |\n\n### Configuration Example\nDefine your source by specifying its type, identifier, and configuration parameters. For a CSV file:\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% This defines the users/4 predicate (id, name, role, department)\n```\n\n### Compilation to Target Code\nTo generate the executable logic for your target environment (e.g., Bash), invoke the compiler:\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Executing the Data Source\nThe compiler generates optimized code (like `awk`-based filters for Bash) that handles both full streams and key-based lookups. You can then use these functions in your shell scripts:\n```bash\nsource output/users.sh\n# Stream data and apply filters\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\nDetailed documentation on configuring and compiling data pipelines can be found in `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-overview_p2", "cluster_id": "data-sources-overview", "question": "What is the source/3 predicate?", "question_type": "medium", "answer": "The `source/3` predicate is the primary mechanism in UnifyWeaver for defining external data sources in Prolog. It allows you to map external data\u2014such as files or web APIs\u2014into predicates that can then be compiled into target-specific code (like Bash functions).\n\n### Syntax\n```prolog\nsource(Type, Name, Options)\n```\n\n### Supported Source Types\n| Type | Usage | Key Options |\n|------|-------|-------------|\n| `csv` | Flat file ingestion | `csv_file`, `has_header`, `delimiter` |\n| `json` | Structured data | `json_file`, `json_path` |\n| `http` | REST APIs | `url`, `headers`, `cache_duration` |\n| `python` | Custom transformations | `python_inline`, `python_file` |\n\n### Usage Example\nTo define a CSV-backed predicate named `users`:\n```prolog\n:- use_module('src/unifyweaver/sources').\n\n:- source(csv, users, [\n    csv_file('examples/users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n```\nThis declaration creates a `users/N` predicate. When processed via `compile_dynamic_source/3`, UnifyWeaver generates optimized Bash code (typically using `awk`) to handle both streaming all records and performing keyed lookups.\n\n### Compilation to Bash\nThe compiler transforms the Prolog definition into a Bash function that manages data extraction:\n```bash\nusers_stream() {\n    # Generated awk logic to parse examples/users.csv\n    awk -F\",\" 'NR > 1 { print $1\":\"$2 }' examples/users.csv\n}\n```\n\nFor more implementation details and advanced pipeline configurations, see `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-overview_p3", "cluster_id": "data-sources-overview", "question": "How do I compile CSV data to Bash?", "question_type": "medium", "answer": "To compile CSV data into a Bash target, you define the data source in Prolog using the `source/3` predicate and then execute the UnifyWeaver compiler to generate a shell-compatible function.\n\n### 1. Define the CSV Source in Prolog\nFirst, import the necessary source modules and specify the CSV file path, header status, and delimiter:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% This automatically creates a users/4 predicate (id, name, role, department)\n```\n\n### 2. Compile to Bash\nInvoke `compile_dynamic_source/3` to transform the Prolog source definition into Bash code:\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### 3. Resulting Bash Implementation\nThe compiler generates a Bash function that uses `awk` for efficient data processing. It supports both streaming all records and looking up specific records by key:\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows, removing quotes and filtering by field count\n        awk -F\",\" 'NR > 1 { gsub(/\"/, \"\", $0); if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4 }' examples/demo_users.csv\n    else\n        # Lookup mode: filter by the first column (key)\n        awk -F\",\" -v key=\"$target_key\" 'NR > 1 { gsub(/\"/, \"\", $0); if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4 }' examples/demo_users.csv\n    fi\n}\n```\n\n### Execution\nAfter saving the output to a script, you can source it and pipeline the data into other shell utilities:\n\n```bash\nsource output_users.sh\nusers | awk -F: '$3 == \"Developer\"'\n```\n\nDetailed documentation on data pipelines and compilation options can be found in `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["data-sources", "csv", "http", "json", "python", "etl"], "original_answer": "## Data Sources in UnifyWeaver\n\nDefine external data sources in Prolog and compile to Bash.\n\n### Supported Source Types\n\n| Type | Use Case | Options |\n|------|----------|--------|\n| `csv` | CSV/TSV files | `csv_file`, `has_header`, `delimiter` |\n| `http` | Web APIs | `url`, `headers`, `cache_duration` |\n| `json` | JSON files | `json_file`, `json_path` |\n| `python` | Complex transforms | `python_inline`, `python_file` |\n\n### The source/3 Predicate\n\n```prolog\nsource(Type, Name, Options)\n```\n\n### Example: CSV Source\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n\n:- source(csv, users, [\n    csv_file('examples/demo_users.csv'),\n    has_header(true),\n    delimiter(',')\n]).\n% Creates: users/4 (id, name, role, department)\n```\n\n### Compilation\n\n```prolog\n?- compile_dynamic_source(users/4, [], BashCode).\n```\n\n### Generated Bash\n\n```bash\nusers() {\n    local target_key=\"$1\"\n    \n    if [[ -z \"$target_key\" ]]; then\n        # Stream all rows\n        awk -F\",\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    else\n        # Lookup mode\n        awk -F\",\" -v key=\"$target_key\" '\n        NR > 1 {\n            gsub(/\"/, \"\", $0)\n            if (NF >= 4 && $1 == key) print $1\":\"$2\":\"$3\":\"$4\n        }' examples/demo_users.csv\n    fi\n}\n\nusers_stream() {\n    users\n}\n```\n\n### Usage\n\n```bash\nsource output/users.sh\nusers_stream | awk -F: '$3 == \"Developer\"'\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p0", "cluster_id": "data-sources-pipeline", "question": "How do I build ETL pipelines with UnifyWeaver?", "question_type": "medium", "answer": "Building ETL pipelines in UnifyWeaver involves integrating its generated data streams with standard Unix utilities to extract, transform, and aggregate records. You define your data sources in Prolog, compile them into Bash functions, and then orchestrate the flow using standard shell pipes.\n\n### Core Pipeline Operations\n\n**Extraction (Streaming):**\nGenerate a stream of all records from a source:\n```bash\nusers_stream\n```\n\n**Filtering:**\nUse `awk` to filter records based on specific field criteria:\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\nApply logic to individual records using shell loops:\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\nSummarize data sets using `awk` for counting or grouping:\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Implementation Example\n\n1. **Define the Source:**\n```prolog\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n```\n\n2. **Compile and Orchestrate:**\nUse Prolog to generate the shell code and wrap it in a pipeline script:\n```prolog\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    % Write BashCode to output/users.sh and create a \n    % master script that invokes users_stream.\n    create_pipeline(BashCode).\n\ncreate_pipeline(BashCode) :-\n    Pipeline = '#!/bin/bash\n    source output/users.sh\n    echo \"Summary by Department:\"\n    users_stream | awk -F: \\'{dept[$4]++} END {for (d in dept) print d \": \" dept[d]}\\'',\n    % Save Pipeline to output/pipeline.sh\n    ...\n```\n\n### Integrating Multiple Data Plugins\n\nTo build pipelines across different formats (CSV, JSON, HTTP), load the required plugins using `imports([])` to avoid predicate conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n```\n\nDetailed implementation patterns and examples can be found in `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p1", "cluster_id": "data-sources-pipeline", "question": "How do I filter and transform data sources?", "question_type": "medium", "answer": "To filter and transform data sources, you combine UnifyWeaver's compiled Bash streams with standard Unix pipeline operations like `awk`, `grep`, and `while` loops. This allows for efficient ETL processing directly within the shell.\n\n### Filtering Records\nFiltering is best handled by piping the stream into `awk` to match specific field values. For example, to filter a user stream for those in a \"Developer\" role:\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n### Transforming Records\nYou can transform data by piping the stream into a `while read` loop for reformatting or using `awk` for field manipulations:\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in the $dept department\"\ndone\n```\n\n### Data Aggregation\nTo aggregate or summarize data, use `awk` to maintain counters or arrays:\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Implementing the Pipeline in Prolog\nIn your Prolog code, define the source and use `compile_dynamic_source/3` to generate the underlying Bash streaming logic. You can then wrap this in a shell script that performs the filtering and transformation.\n\n```prolog\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    % Save BashCode to a script and execute the pipeline\n    create_pipeline(BashCode).\n\ncreate_pipeline(BashCode) :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n# Example: Filter for Developers and transform output\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' | while IFS=: read id name role dept; do\n    echo \"Developer: $name\"\ndone',\n    setup_pipeline_script(Pipeline).\n```\n\n### Loading Multiple Source Plugins\nIf your pipeline uses various formats (CSV, JSON, HTTP), use `load_files/2` with `imports([])` to prevent predicate name conflicts:\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```\n\nDetailed guides on pipeline construction can be found in `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p2", "cluster_id": "data-sources-pipeline", "question": "What pipeline operations are available?", "question_type": "medium", "answer": "UnifyWeaver supports a variety of standard Unix pipeline operations that can be combined with its data sources, including streaming, filtering, transformation, and aggregation.\n\n### Available Operations\n\n*   **Streaming:** Stream all records from a source using its generated Bash function (e.g., `users_stream`).\n*   **Filtering:** Use `awk` for field-specific filtering:\n    ```bash\n    users_stream | awk -F: '$3 == \"Developer\"'\n    ```\n*   **Transformation:** Apply custom formatting or logic with Bash loops:\n    ```bash\n    users_stream | while IFS=: read id name role dept; do\n        echo \"$name works in $dept\"\n    done\n    ```\n*   **Aggregation:** Count or group data using `awk` associative arrays:\n    ```bash\n    users_stream | awk -F: '{dept[$4]++} END {\n        for (d in dept) print d\": \"dept[d]\n    }'\n    ```\n\n### ETL Orchestration\nIn Prolog, you can define a source and compile it into these pipeline components using `compile_dynamic_source/3`. When loading multiple source plugins (like CSV, JSON, or HTTP), it is recommended to use `load_files/2` with `imports([])` to prevent symbol conflicts:\n\n```prolog\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```\n\nFor more detailed examples and advanced pipeline configurations, consult `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
{"pair_id": "data-sources-pipeline_p3", "cluster_id": "data-sources-pipeline", "question": "How do I aggregate data in Bash pipelines?", "question_type": "medium", "answer": "To aggregate data within Bash pipelines generated by UnifyWeaver, you typically pipe the output of a generated `_stream` function into standard Unix utilities like `awk` for stateful processing.\n\n### Bash Aggregation Pattern\nOnce you have defined and compiled a dynamic source, you can use `awk` to perform counts, sums, or other groupings on the colon-delimited stream.\n\n**Example: Aggregating by Category**\n```bash\n# Using a generated stream function\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d \": \" dept[d]\n}'\n```\n\n### Integrated ETL Pipeline\nIn your Prolog logic, you define the source and script the pipeline by calling `compile_dynamic_source/3` and writing the resulting Bash code to a file:\n\n```prolog\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\ncreate_pipeline :-\n    compile_dynamic_source(users/4, [], BashCode),\n    % Save BashCode to a helper script then construct the pipeline\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"Counts by department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Essential Pipeline Operations\n*   **Streaming:** Invoke the generated `[source_name]_stream` function.\n*   **Filtering:** Use `awk` for row selection (e.g., `awk -F: '$3 == \"Developer\"'`).\n*   **Aggregation:** Use `awk` arrays in the `END` block to output summarized results.\n*   **Conflict Prevention:** When loading multiple plugins (CSV, JSON, etc.), use `load_files/2` with `imports([])` to avoid predicate conflicts.\n\nFor more detailed examples on building data pipelines, refer to `education/book-02-bash-target/data_sources_pipeline_guide.md`.", "answer_variant": "default", "answer_source": "education/book-02-bash-target/data_sources_pipeline_guide.md", "topics": ["pipeline", "etl", "data-processing", "awk"], "original_answer": "## Building ETL Pipelines\n\nCombine UnifyWeaver sources with Unix pipeline operations.\n\n### Pipeline Operations\n\n**Streaming:**\n```bash\nusers_stream  # Stream all records\n```\n\n**Filtering:**\n```bash\nusers_stream | awk -F: '$3 == \"Developer\"'\n```\n\n**Transformation:**\n```bash\nusers_stream | while IFS=: read id name role dept; do\n    echo \"$name works in $dept\"\ndone\n```\n\n**Aggregation:**\n```bash\nusers_stream | awk -F: '{dept[$4]++} END {\n    for (d in dept) print d\": \"dept[d]\n}'\n```\n\n### Complete ETL Example\n\n```prolog\n% Define source\n:- source(csv, users, [\n    csv_file('data/users.csv'),\n    has_header(true)\n]).\n\n% Compile and create pipeline\nmain :-\n    compile_dynamic_source(users/4, [], BashCode),\n    open('output/users.sh', write, S),\n    write(S, BashCode),\n    close(S),\n    create_pipeline.\n\ncreate_pipeline :-\n    Pipeline = '#!/bin/bash\nsource output/users.sh\n\necho \"All users:\"\nusers_stream | while IFS=: read id name role dept; do\n    echo \"  $name ($role) in $dept\"\ndone\n\necho \"\"\necho \"Developers:\"\nusers_stream | awk -F: \\'$3 == \"Developer\"\\' |\n    while IFS=: read id name role dept; do\n        echo \"  $name\"\n    done\n\necho \"\"\necho \"By department:\"\nusers_stream | awk -F: \\'{dept[$4]++} END {\n    for (d in dept) print \"  \" d \": \" dept[d]\n}\\'',\n    open('output/pipeline.sh', write, S),\n    write(S, Pipeline),\n    close(S).\n```\n\n### Loading Multiple Plugins\n\nUse `load_files` with `imports([])` to avoid conflicts:\n\n```prolog\n:- use_module('src/unifyweaver/sources').\n:- load_files('src/unifyweaver/sources/csv_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/http_source', [imports([])]).\n:- load_files('src/unifyweaver/sources/json_source', [imports([])]).\n```", "answer_tailored": true}
