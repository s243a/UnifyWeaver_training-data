{"pair_id": "ai-distill-001_p0", "cluster_id": "ai-distill-001", "question": "What is the H^L = N equivalence conjecture?", "question_type": "medium", "answer": "The $H^L = N$ equivalence conjecture states that the routing capacity of a transformer with $H$ attention heads per layer and $L$ layers is mathematically equivalent to $N$ independent \"flat\" LDA heads.\n\nThis capacity is achieved through sequential composition, where each layer routes through a new set of attention patterns based on the previous layer's output:\n* **Layer 1:** Offers $H$ routing choices.\n* **Layer 2:** Each of the $H$ choices from Layer 1 can route through $H$ new patterns ($H \\times H = H^2$).\n* **Layer L:** Continuing this composition across $L$ layers results in $H^L$ total routing combinations.\n\n**Capacity Equivalencies:**\n* **16 flat heads:** Equivalent to a 2-layer transformer with 4 heads per layer ($4^2$).\n* **64 flat heads:** Equivalent to a 3-layer transformer with 4 heads per layer ($4^3$).\n* **256 flat heads:** Equivalent to a 4-layer transformer with 4 heads per layer ($4^4$).\n\nResearch suggests that $H \\approx 4$ is the optimal head count for maximizing practical efficiency in these architectures. For a detailed analysis of this conjecture and its role in transformer distillation, see `education/book-14-ai-training/06_transformer_distillation.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/06_transformer_distillation.md", "topics": [], "original_answer": "A transformer with H attention heads per layer and L layers has routing capacity equivalent to H^L flat LDA heads.\n\n```\nFlat LDA:    N independent heads\nTransformer: H heads \u00d7 L layers \u2192 H^L effective routing patterns\n```\n\n**Why H^L?**\nEach layer routes through H attention patterns. Layers compose sequentially:\n```\nLayer 1: H choices\nLayer 2: H choices (for each Layer 1 choice)\n...\nLayer L: H choices\n\nTotal combinations: H \u00d7 H \u00d7 ... \u00d7 H = H^L\n```\n\n**Examples:**\n| Flat LDA Heads (N) | Transformer H | Transformer L | Equivalent |\n|-------------------|---------------|---------------|------------|\n| 16 | 4 | 2 | 4\u00b2 = 16 |\n| 64 | 4 | 3 | 4\u00b3 = 64 |\n| 256 | 4 | 4 | 4\u2074 = 256 |\n\n**Optimal H \u2248 4** for practical efficiency.", "answer_tailored": true}
{"pair_id": "ai-distill-001_p0", "cluster_id": "ai-distill-001", "question": "How do you train a transformer via distillation from LDA?", "question_type": "medium", "answer": "To train a transformer via distillation from LDA in the UnifyWeaver framework, you must optimize the model to replicate LDA outputs using a hybrid objective of **MSE and cosine loss**.\n\nThe loss function is defined as:\n`L = (1 - \u03bb) \u00d7 MSE(pred, target) + \u03bb \u00d7 (1 - cosine_sim(pred, target))`\n\nA weight of **\u03bb = 0.7** (cosine-weighted) is essential. While MSE reduces absolute error, the cosine loss ensures directional alignment, which is critical for semantic retrieval accuracy.\n\nYou can implement this distillation process using the `ProjectionTransformer` as follows:\n\n```python\nfrom projection_transformer import ProjectionTransformer, train_distillation\n\n# Initialize transformer to match the LDA's projection capacity\ntransformer = ProjectionTransformer(\n    embed_dim=384,\n    num_heads=4,      # Equivalent to 16 heads over 2 layers (L=4\u00b2)\n    num_layers=2,     \n    ff_dim=512\n)\n\n# Execute distillation from the LDA teacher\ntrain_distillation(\n    transformer=transformer,\n    lda_projection=lda,           # The multi-head LDA teacher\n    query_embeddings=train_data,\n    num_epochs=200,\n    cosine_weight=0.7             # Required for directional similarity\n)\n```\n\n**Performance Metrics:**\nTraining typically achieves a **Mean Cosine Similarity of 0.9928 \u00b1 0.0027** against 18 LDA heads. This high similarity validates the equivalence of the transformer architecture to the LDA projection. Detailed methodology and mathematical proofs can be found in `education/book-14-ai-training/06_transformer_distillation.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/06_transformer_distillation.md", "topics": [], "original_answer": "Train the transformer to match LDA outputs using **MSE + cosine loss**:\n\n```\n# Loss function (mathematical notation)\nL = (1 - \u03bb) \u00d7 MSE(pred, target) + \u03bb \u00d7 (1 - cosine_sim(pred, target))\n\n# Recommended: \u03bb = 0.7 (cosine-weighted)\n```\n\n**CRITICAL:** MSE alone achieves low error but wrong direction. Cosine loss ensures directional alignment.\n\n```python\nfrom projection_transformer import ProjectionTransformer, train_distillation\n\n# Create transformer matching LDA capacity\ntransformer = ProjectionTransformer(\n    embed_dim=384,\n    num_heads=4,      # H\n    num_layers=2,     # L \u2192 4\u00b2 = 16 equivalent heads\n    ff_dim=512\n)\n\n# Train via distillation\ntrain_distillation(\n    transformer=transformer,\n    lda_projection=lda,           # Teacher (multi-head LDA)\n    query_embeddings=train_data,\n    num_epochs=200,\n    cosine_weight=0.7             # Essential!\n)\n```\n\n**Validation results (N=18 LDA heads):**\n- Mean Cosine Similarity: 0.9928 \u00b1 0.0027\n- **99.28% cosine similarity** validates the equivalence conjecture.", "answer_tailored": true}
{"pair_id": "ai-distill-001_p0", "cluster_id": "ai-distill-001", "question": "When should I use transformer distillation vs flat LDA?", "question_type": "medium", "answer": "Choosing between transformer distillation and flat LDA depends primarily on your query scale (number of heads), batch size, and operational constraints like interpretability or memory.\n\n**Opt for Transformer Distillation when:**\n- Working with high-dimensional head counts (N > 400 for single queries, or N > 30 for batched processing).\n- Operating in memory-constrained environments, such as mobile or edge devices.\n- Your domain structure naturally favors hierarchical routing.\n\n**Stick with Flat LDA when:**\n- Your scale is smaller (N < 400 heads), which covers most standard use cases.\n- Model interpretability is a priority, as LDA weights offer direct meaning.\n- Data clusters shift frequently, making the high cost of transformer retraining prohibitive.\n\n**Latency and Crossover Analysis (N=18 heads):**\n| Method | Single Query | Batch (32) |\n|--------|-------------|------------|\n| LDA (NumPy CPU) | 0.046 ms | 0.046 ms |\n| Transformer (CUDA) | 1.110 ms | 0.069 ms |\n\nThe crossover point where transformers become more efficient is approximately 400 heads for single queries and 27 heads for batches of 32. At smaller scales, LDA on the CPU typically outperforms Transformers on CUDA because the GPU kernel overhead outweighs the benefits of the transformer's architecture. Transformers gain the advantage only when the O(N) softmax cost becomes more expensive than the overhead.\n\nFor a detailed breakdown of distillation benchmarks, see `education/book-14-ai-training/06_transformer_distillation.md`.", "answer_variant": "default", "answer_source": "book-14-ai-training/06_transformer_distillation.md", "topics": [], "original_answer": "**Use transformer when:**\n- N > 400 heads (single query) or N > 30 (batched)\n- Memory constrained (mobile/edge)\n- Natural domain structure for hierarchical routing\n\n**Stay with LDA when:**\n- N < 400 heads (most cases)\n- Interpretability matters (LDA weights are meaningful)\n- Clusters change frequently (retraining is expensive)\n\n**Latency analysis (N=18 heads, 1000 queries):**\n| Method | Single Query | Batch (32) |\n|--------|-------------|------------|\n| LDA (NumPy CPU) | 0.046 ms | 0.046 ms |\n| Transformer (CUDA) | 1.110 ms | 0.069 ms |\n\n**Crossover points:**\n- Single query: >400 heads (LDA wins below)\n- Batched (32): ~27 heads\n\n**Key insight:** At small scale, LDA wins due to GPU kernel overhead. Transformer benefits emerge at larger scale where O(N) softmax becomes expensive.", "answer_tailored": true}
