{"pair_id": "ai-advanced-001_p0", "cluster_id": "ai-advanced-001", "question": "What is smoothness regularization in LDA?", "question_type": "medium", "answer": "Smoothness regularization is a technique that prevents LDA projection overfitting when clusters contain limited training examples. It works by encouraging clusters with similar semantic content to learn similar projection matrices.\n\n**Core mechanism:**\n\nThe approach uses a graph Laplacian to encode cluster similarity:\n```\n# Build similarity matrix from cluster centroids\nS[i,j] = cosine_sim(centroid_i, centroid_j)\n\n# Construct graph Laplacian\nD = diag(sum(S, axis=1))  # Degree matrix\nL = D - S                  # Laplacian matrix\n\n# Add regularization term to loss function\nLoss = MSE(predictions, targets) + \u03bb * trace(W^T L W)\n     = MSE(predictions, targets) + \u03bb * sum_ij S[i,j] * ||W_i - W_j||\u00b2\n```\n\nThe regularization term penalizes large differences between projection matrices (W_i and W_j) for clusters that are close in embedding space, promoting smoother transitions across the topic landscape.\n\n**Best applied when:**\n- Training clusters have fewer than 5 examples\n- Per-cluster metrics show high variance\n- Domain structure suggests related topics should yield similar answers\n\nFor implementation details, see education/book-14-ai-training/08_advanced_topics.md.", "answer_variant": "default", "answer_source": "book-14-ai-training/08_advanced_topics.md", "topics": [], "original_answer": "When clusters have few training examples, projections can overfit. **Smoothness regularization** encourages similar clusters to have similar projections.\n\n**Graph Laplacian approach:**\n```\n# Cluster similarity graph (mathematical notation)\nS[i,j] = cosine_sim(centroid_i, centroid_j)\n\n# Graph Laplacian\nD = diag(sum(S, axis=1))  # Degree matrix\nL = D - S                  # Laplacian\n\n# Regularized loss\nLoss = MSE(pred, target) + \u03bb * sum_ij S[i,j] * ||W_i - W_j||\u00b2\n     = MSE(pred, target) + \u03bb * trace(W^T L W)\n```\n\nThis encourages nearby clusters (in embedding space) to have similar answer projections.\n\n**When to use:**\n- Clusters with < 5 training questions\n- High variance in per-cluster performance\n- Domain knowledge suggests cluster similarity", "answer_tailored": true}
{"pair_id": "ai-advanced-001_p0", "cluster_id": "ai-advanced-001", "question": "What are hierarchical transformer ensembles?", "question_type": "medium", "answer": "Hierarchical transformer ensembles are a multi-level architecture for handling very large cluster counts (500+). Instead of a single transformer projecting queries across all clusters, the system uses multiple specialized transformers, each responsible for a subset of clusters (e.g., Domain A handles clusters 1-100, Domain B handles 101-200). A small meta-router MLP sits above these transformers and dynamically routes each query by computing domain weights via softmax, then combining the domain-specific projections weighted by relevance. This allows the system to scale efficiently while maintaining specialization.\n\nDomains can be assigned automatically using K-means clustering on cluster centroids, manually by labeling semantic groups (like \"auth\", \"data\", \"performance\"), or through hierarchical clustering with tree cuts.\n\nThe meta-router implementation computes weighted projections: `projected = sum(domain_weights[d] * transformer_d(query) for d in domains)`, where `domain_weights = softmax(meta_router(query))` might produce values like [0.8, 0.15, 0.05] indicating the query's relevance to each domain.\n\nFor implementation details, see education/book-14-ai-training/08_advanced_topics.md in the UnifyWeaver repository.", "answer_variant": "default", "answer_source": "book-14-ai-training/08_advanced_topics.md", "topics": [], "original_answer": "For very large cluster counts (500+), use multiple transformers with a meta-router:\n\n```\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502  Meta-Router    \u2502\n                \u2502  (small MLP)    \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u25bc                 \u25bc                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Transformer  \u2502  \u2502 Transformer  \u2502  \u2502 Transformer  \u2502\n\u2502   Domain A   \u2502  \u2502   Domain B   \u2502  \u2502   Domain C   \u2502\n\u2502 (clusters    \u2502  \u2502 (clusters    \u2502  \u2502 (clusters    \u2502\n\u2502  1-100)      \u2502  \u2502  101-200)    \u2502  \u2502  201-300)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Domain assignment via:**\n1. K-means on centroids (automatic)\n2. Manual labels (\"auth\", \"data\", \"performance\")\n3. Hierarchical clustering (tree cut)\n\n**Meta-router:**\n```python\ndomain_weights = softmax(meta_router(query))  # [0.8, 0.15, 0.05]\nprojected = sum(domain_weights[d] * transformer_d(query) for d in domains)\n```", "answer_tailored": true}
{"pair_id": "ai-advanced-001_p0", "cluster_id": "ai-advanced-001", "question": "What scaling strategies exist for different LDA head counts?", "question_type": "medium", "answer": "The scaling strategy depends on your head count:\n\n- **<100 heads**: Use flat LDA multi-head projection (current approach)\n- **100-500 heads**: Consider transformer distillation to reduce computational overhead\n- **500-2000 heads**: Deploy hierarchical ensemble architectures\n- **2000+ heads**: Combine ANN indexing with hierarchical routing and sparse selection\n\nFor systems requiring extreme scale, future enhancements include:\n- **Online learning** for incremental projection updates\n- **Mixture of Experts (MoE)** with top-k sparse routing\n- **Contrastive training** to learn Q-A matching embeddings directly\n- **Cross-lingual projection** using multilingual embeddings\n- **Graph-aware routing** leveraging knowledge graph structure\n\nSee `education/book-14-ai-training/08_advanced_topics.md` for implementation details on each scaling approach.", "answer_variant": "default", "answer_source": "book-14-ai-training/08_advanced_topics.md", "topics": [], "original_answer": "| Scale | Strategy |\n|-------|---------|\n| Moderate (<100 heads) | Flat LDA multi-head |\n| Large (100-500 heads) | Consider transformer distillation |\n| Very Large (500-2000) | Hierarchical ensemble |\n| Massive (2000+) | ANN + hierarchical + sparse routing |\n\n**Future directions:**\n\n1. **Online Learning:** Update projections incrementally\n2. **Mixture of Experts (MoE):** Sparse routing with top-k selection\n3. **Contrastive Training:** Learn embeddings directly for Q-A matching\n4. **Cross-Lingual Projection:** Work across languages with multilingual embeddings\n5. **Graph-Aware Routing:** Use knowledge graph structure for routing", "answer_tailored": true}
