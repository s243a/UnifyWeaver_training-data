{"pair_id": "ai-pipeline-001_p0", "cluster_id": "ai-pipeline-001", "question": "What's the database schema for LDA training?", "question_type": "medium", "answer": "The LDA training database uses SQLite with separate NPY files for vector storage. The schema consists of four main tables:\n\n**models** - Tracks embedding models (e.g., 'all-MiniLM-L6-v2' with 384 dimensions)\n**clusters** - Groups related Q&A pairs, storing answer text and linking to the model\n**questions** - Individual questions linked to their parent cluster\n**embeddings** - Maps entities (questions/answers/centroids) to their vector files stored as .npy\n\nThe physical layout separates metadata (lda.db) from vectors (embeddings/ directory):\n\n```\nplaybooks/lda-training-data/\n\u251c\u2500\u2500 lda.db\n\u2514\u2500\u2500 embeddings/all-MiniLM-L6-v2/\n    \u251c\u2500\u2500 q_001.npy\n    \u251c\u2500\u2500 a_001.npy\n    \u2514\u2500\u2500 mh_1_cluster_0_centroid.npy\n```\n\nEach embedding record in the database points to its corresponding .npy file via the vector_path field, with entity_type distinguishing between 'question', 'answer', and 'centroid' vectors.", "answer_variant": "default", "answer_source": "book-14-ai-training/05_training_pipeline.md", "topics": [], "original_answer": "SQLite + NPY files for vectors:\n\n```\nplaybooks/lda-training-data/\n\u251c\u2500\u2500 lda.db                    # SQLite database\n\u2514\u2500\u2500 embeddings/\n    \u2514\u2500\u2500 all-MiniLM-L6-v2/\n        \u251c\u2500\u2500 q_001.npy         # Question embeddings\n        \u251c\u2500\u2500 a_001.npy         # Answer embeddings\n        \u2514\u2500\u2500 mh_1_cluster_0_centroid.npy  # Cluster centroids\n```\n\n**Core tables:**\n```sql\nCREATE TABLE models (\n    model_id INTEGER PRIMARY KEY,\n    name TEXT UNIQUE,           -- 'all-MiniLM-L6-v2'\n    dimensions INTEGER          -- 384\n);\n\nCREATE TABLE clusters (\n    cluster_id INTEGER PRIMARY KEY,\n    answer_text TEXT,\n    answer_record_id TEXT,\n    model_id INTEGER REFERENCES models\n);\n\nCREATE TABLE questions (\n    question_id INTEGER PRIMARY KEY,\n    cluster_id INTEGER REFERENCES clusters,\n    question_text TEXT\n);\n\nCREATE TABLE embeddings (\n    embedding_id INTEGER PRIMARY KEY,\n    entity_type TEXT,           -- 'question', 'answer', 'centroid'\n    entity_id INTEGER,\n    model_id INTEGER REFERENCES models,\n    vector_path TEXT            -- Path to .npy file\n);\n```", "answer_tailored": true}
{"pair_id": "ai-pipeline-001_p0", "cluster_id": "ai-pipeline-001", "question": "How does batch tracking work for incremental training?", "question_type": "medium", "answer": "Batch tracking for incremental training uses a `training_batches` table in SQLite to monitor which data files have been processed:\n\n```sql\nCREATE TABLE training_batches (\n    batch_id INTEGER PRIMARY KEY,\n    file_path TEXT,\n    file_hash TEXT,             -- SHA256 for change detection\n    status TEXT,                -- 'pending', 'importing', 'embedding', 'completed', 'failed'\n    error_message TEXT,\n    created_at TIMESTAMP\n);\n```\n\nThe system tracks file processing through status transitions. On the initial run, `migrate_to_lda_db.py --scan` identifies all files in the input directory and marks them as 'pending'. The `--process-pending` flag then processes only those pending batches, updating their status to 'completed' or 'failed'.\n\nWhen new data arrives, running `--scan` again detects new files (via file path and SHA256 hash) and adds only those as pending batches. This allows `--process-pending` to handle just the incremental data without reprocessing existing files.\n\nAfter importing new batches, `train_multi_head_projection.py` retrains the LDA model using the complete dataset (old + new). Failed batches can be retried using `migrate_to_lda_db.py --retry-failed`.\n\nFor implementation details, see education/book-14-ai-training/05_training_pipeline.md.", "answer_variant": "default", "answer_source": "book-14-ai-training/05_training_pipeline.md", "topics": [], "original_answer": "Track which data files have been processed:\n\n```sql\nCREATE TABLE training_batches (\n    batch_id INTEGER PRIMARY KEY,\n    file_path TEXT,\n    file_hash TEXT,             -- SHA256 for change detection\n    status TEXT,                -- 'pending', 'importing', 'embedding', 'completed', 'failed'\n    error_message TEXT,\n    created_at TIMESTAMP\n);\n```\n\n**Workflow:**\n```bash\n# Day 1: Initial training\npython3 scripts/migrate_to_lda_db.py --scan --input raw/\npython3 scripts/migrate_to_lda_db.py --process-pending\npython3 scripts/train_multi_head_projection.py --db lda.db\n\n# Day 2: New data added\npython3 scripts/migrate_to_lda_db.py --scan --input raw/\n# Only new files are marked pending\n\npython3 scripts/migrate_to_lda_db.py --process-pending\n# Only new batches processed\n\npython3 scripts/train_multi_head_projection.py --db lda.db\n# Re-trains with all data\n```\n\n**Failed batch handling:**\n```bash\npython3 scripts/migrate_to_lda_db.py --retry-failed\n```", "answer_tailored": true}
