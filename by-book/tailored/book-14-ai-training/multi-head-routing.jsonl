{"pair_id": "ai-multihead-001_p0", "cluster_id": "ai-multihead-001", "question": "What is multi-head routing in LDA?", "question_type": "medium", "answer": "Multi-head routing in LDA is an approach where you use **multiple specialized projection heads** instead of a single global projection. Each head focuses on a specific cluster of related questions (like authentication, data sources, or performance).\n\nWhen a query comes in, it's processed by all heads in parallel:\n\n```\n                    Query\n                      \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc             \u25bc             \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Head 1  \u2502   \u2502 Head 2  \u2502   \u2502 Head 3  \u2502\n   \u2502 (Auth)  \u2502   \u2502 (Data)  \u2502   \u2502 (Perf)  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n        \u2502             \u2502             \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n              Weighted Combination\n                      \u2502\n                      \u25bc\n               Projected Query\n```\n\nEach head maintains two key components:\n1. **Centroid**: The mean embedding of all questions in its cluster\n2. **Answer embedding**: The representative answer for that cluster\n\n```python\nhead = {\n    'centroid': np.mean(question_embeddings, axis=0),  # (d,)\n    'answer_emb': answer_embedding                      # (d,)\n}\n```\n\nThe outputs from all heads are then combined through weighted aggregation to produce the final projected query. This specialization improves retrieval accuracy by allowing each head to optimize for its specific question domain.", "answer_variant": "default", "answer_source": "book-14-ai-training/04_multi_head_routing.md", "topics": [], "original_answer": "Instead of one global projection, **multiple heads** each specialized for a cluster of related questions:\n\n```\n                    Query\n                      \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc             \u25bc             \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Head 1  \u2502   \u2502 Head 2  \u2502   \u2502 Head 3  \u2502\n   \u2502 (Auth)  \u2502   \u2502 (Data)  \u2502   \u2502 (Perf)  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n        \u2502             \u2502             \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n              Weighted Combination\n                      \u2502\n                      \u25bc\n               Projected Query\n```\n\n**Each head stores:**\n1. **Centroid**: Mean embedding of questions in this cluster\n2. **Answer embedding**: The answer for this cluster\n\n```python\nhead = {\n    'centroid': np.mean(question_embeddings, axis=0),  # (d,)\n    'answer_emb': answer_embedding                      # (d,)\n}\n```", "answer_tailored": true}
{"pair_id": "ai-multihead-001_p0", "cluster_id": "ai-multihead-001", "question": "How does temperature control routing sharpness?", "question_type": "medium", "answer": "The temperature parameter \u03c4 in the softmax function determines how concentrated or distributed the routing weights become across heads:\n\n**Lower temperature (\u03c4 \u2192 0):** Creates sharper, more concentrated routing where the best-matching head receives most of the weight. At \u03c4 = 0.01, you get nearly winner-take-all behavior (e.g., weights = [0.95, 0.03, 0.02]).\n\n**Higher temperature (\u03c4 \u2192 \u221e):** Produces softer, more uniform routing where weights spread more evenly across heads. At \u03c4 = 10.0, you approach uniform distribution regardless of similarity scores.\n\n**Recommended setting:** \u03c4 = 0.1 provides sharp but soft routing, giving clear preference to the best head while still allowing meaningful contributions from others (e.g., weights = [0.85, 0.10, 0.05]).\n\nThe temperature scaling works by dividing similarity scores before applying softmax:\n\n```python\nscaled_similarities = similarities / temperature\nweights = softmax(scaled_similarities)\n```\n\nFor overlapping topic clusters, use higher temperatures (\u03c4 = 1.0) to blend multiple heads' results. For well-separated clusters, use lower temperatures (\u03c4 = 0.1) to route decisively. See education/book-14-ai-training/04_multi_head_routing.md for implementation details.", "answer_variant": "default", "answer_source": "book-14-ai-training/04_multi_head_routing.md", "topics": [], "original_answer": "Temperature \u03c4 controls how sharply queries route to heads:\n\n| Temperature | Behavior | Use Case |\n|-------------|----------|----------|\n| \u03c4 = 0.01 | Nearly hard routing (winner-take-all) | Distinct clusters |\n| \u03c4 = 0.1 | Sharp but soft | **Recommended default** |\n| \u03c4 = 1.0 | Soft blending | Overlapping clusters |\n| \u03c4 = 10.0 | Nearly uniform | Regularization |\n\n**Example weights:**\n```python\n# Sharp routing (\u03c4=0.1): weights = [0.85, 0.10, 0.05]\n# Soft routing (\u03c4=1.0):  weights = [0.45, 0.30, 0.25]\n```\n\n**Temperature-controlled softmax:**\n```python\ndef route_query(query_emb, heads, temperature=0.1):\n    similarities = [np.dot(query_norm, centroid_norm) for head in heads]\n    scaled = similarities / temperature\n    exp_scaled = np.exp(scaled - np.max(scaled))  # stability\n    weights = exp_scaled / np.sum(exp_scaled)\n    return weights\n```", "answer_tailored": true}
{"pair_id": "ai-multihead-001_p0", "cluster_id": "ai-multihead-001", "question": "How is multi-head LDA related to transformer attention?", "question_type": "medium", "answer": "The connection is structural: multi-head LDA implements the same computation pattern as transformer attention mechanisms.\n\n**Direct mapping:**\n\n| LDA Multi-Head | Transformer Attention |\n|----------------|----------------------|\n| Centroids | Keys (K) |\n| Answer embeddings | Values (V) |\n| Query embedding | Query (Q) |\n| softmax(sim/\u03c4) | softmax(QK^T/\u221ad) |\n| \u03a3 weights \u00d7 answers | Attention output |\n\nBoth are learned routing mechanisms that compute similarity-weighted combinations. In LDA, centroids act as keys that route queries to answer embeddings (values). In transformers, queries attend to keys and retrieve their corresponding values.\n\n**The multi-head projection mirrors attention:**\n```python\ndef project_multi_head(query_emb, heads, temperature=0.1):\n    weights = route_query(query_emb, heads, temperature)\n    \n    # Weighted combination of answer embeddings (like attention output)\n    projected = np.zeros_like(query_emb)\n    for i, head in enumerate(heads):\n        projected += weights[i] * head['answer_emb']\n    \n    return projected\n```\n\nThe key difference: LDA uses learned topic centroids rather than dynamically computed key projections, but the routing mathematics are equivalent.", "answer_variant": "default", "answer_source": "book-14-ai-training/04_multi_head_routing.md", "topics": [], "original_answer": "Multi-head LDA is structurally identical to attention:\n\n| LDA Multi-Head | Transformer Attention |\n|----------------|----------------------|\n| Centroids | Keys (K) |\n| Answer embeddings | Values (V) |\n| Query embedding | Query (Q) |\n| softmax(sim/\u03c4) | softmax(QK^T/\u221ad) |\n| \u03a3 weights \u00d7 answers | Attention output |\n\nThis isn't coincidence\u2014both are learned routing mechanisms!\n\n**Multi-head projection formula:**\n```python\ndef project_multi_head(query_emb, heads, temperature=0.1):\n    weights = route_query(query_emb, heads, temperature)\n    \n    # Weighted combination of answer embeddings\n    projected = np.zeros_like(query_emb)\n    for i, head in enumerate(heads):\n        projected += weights[i] * head['answer_emb']\n    \n    return projected\n```", "answer_tailored": true}
