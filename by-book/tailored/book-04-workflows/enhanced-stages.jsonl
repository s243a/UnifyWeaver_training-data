{"pair_id": "b4-c6-enhanced-pipeline-stages_p0", "cluster_id": "b4-c6-enhanced-pipeline-stages", "question": "What are enhanced pipeline stages?", "question_type": "medium", "answer": "Enhanced pipeline stages are powerful building blocks for data processing within a single target language. Unlike cross-target pipelines that chain different languages together (like AWK \u2192 Python \u2192 Go covered in Chapter 3), enhanced stages operate entirely within one target language for streamlined processing.\n\nThese stages fall into six categories:\n\n**Stream combination**: interleave, concat, zip\n**Flow control**: fan_out, parallel, merge  \n**Filtering**: filter_by, batch, unbatch\n**Aggregation**: group_by, reduce, distinct\n**Error handling**: try_catch, retry, timeout\n**Throughput management**: rate_limit, throttle, buffer\n\nYou can compose these stages using `compile_enhanced_pipeline/3`:\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```\n\nFor comprehensive coverage of enhanced pipeline stages, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["enhanced-stages", "stream-processing", "within-target"], "original_answer": "## Enhanced Pipeline Stages\n\nPowerful building blocks for within-target data processing.\n\n### vs Cross-Target Pipelines\n\n| Aspect | Cross-Target | Enhanced Stages |\n|--------|--------------|------------------|\n| Scope | AWK \u2192 Python \u2192 Go | Within one target |\n| Use | Different languages | Same language |\n| Chapter | Chapter 3 | Chapter 6 |\n\n### Stage Categories\n\n| Category | Stages |\n|----------|--------|\n| Stream combination | interleave, concat, zip |\n| Flow control | fan_out, parallel, merge |\n| Filtering | filter_by, batch, unbatch |\n| Aggregation | group_by, reduce, distinct |\n| Error handling | try_catch, retry, timeout |\n| Throughput | rate_limit, throttle, buffer |\n\n### Usage\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_tailored": true}
{"pair_id": "b4-c6-enhanced-pipeline-stages_p1", "cluster_id": "b4-c6-enhanced-pipeline-stages", "question": "How do enhanced stages differ from cross-target pipelines?", "question_type": "medium", "answer": "Enhanced stages differ from cross-target pipelines in their scope and execution context. While cross-target pipelines connect different languages together (like AWK \u2192 Python \u2192 Go), enhanced stages operate entirely within a single target language. Cross-target pipelines are designed for multi-language workflows where you need to leverage each language's strengths, whereas enhanced stages provide powerful data processing primitives\u2014like interleaving streams, fan-out/merge patterns, filtering, aggregation, and error handling\u2014all compiled to the same target language. For cross-target pipeline details, see book-04-workflows chapter 3; enhanced stages are covered in chapter 6.\n\nThe enhanced stage system includes stream combination (interleave, concat, zip), flow control (fan_out, parallel, merge), filtering (filter_by, batch, unbatch), aggregation (group_by, reduce, distinct), error handling (try_catch, retry, timeout), and throughput management (rate_limit, throttle, buffer). You compile them similarly to other pipelines:\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["enhanced-stages", "stream-processing", "within-target"], "original_answer": "## Enhanced Pipeline Stages\n\nPowerful building blocks for within-target data processing.\n\n### vs Cross-Target Pipelines\n\n| Aspect | Cross-Target | Enhanced Stages |\n|--------|--------------|------------------|\n| Scope | AWK \u2192 Python \u2192 Go | Within one target |\n| Use | Different languages | Same language |\n| Chapter | Chapter 3 | Chapter 6 |\n\n### Stage Categories\n\n| Category | Stages |\n|----------|--------|\n| Stream combination | interleave, concat, zip |\n| Flow control | fan_out, parallel, merge |\n| Filtering | filter_by, batch, unbatch |\n| Aggregation | group_by, reduce, distinct |\n| Error handling | try_catch, retry, timeout |\n| Throughput | rate_limit, throttle, buffer |\n\n### Usage\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_tailored": true}
{"pair_id": "b4-c6-enhanced-pipeline-stages_p2", "cluster_id": "b4-c6-enhanced-pipeline-stages", "question": "What types of enhanced stages exist?", "question_type": "medium", "answer": "UnifyWeaver provides several categories of enhanced pipeline stages for within-target data processing:\n\n**Stream Combination Stages:**\n- interleave - Merge multiple streams alternating between sources\n- concat - Combine streams sequentially\n- zip - Pair up elements from parallel streams\n\n**Flow Control Stages:**\n- fan_out - Distribute data to multiple processors\n- parallel - Execute operations concurrently\n- merge - Combine results from parallel branches\n\n**Filtering Stages:**\n- filter_by - Select elements matching criteria\n- batch - Group individual items into batches\n- unbatch - Split batches into individual items\n\n**Aggregation Stages:**\n- group_by - Organize data by key\n- reduce - Combine elements into single result\n- distinct - Remove duplicates\n\n**Error Handling Stages:**\n- try_catch - Handle exceptions in pipeline\n- retry - Attempt failed operations again\n- timeout - Set time limits on operations\n\n**Throughput Control Stages:**\n- rate_limit - Control processing speed\n- throttle - Regulate data flow\n- buffer - Store data temporarily\n\nThese stages work within a single target language, unlike cross-target pipelines (covered in education/book-04-workflows/03_cross_target_pipelines.md) which chain different languages together. For implementation details, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["enhanced-stages", "stream-processing", "within-target"], "original_answer": "## Enhanced Pipeline Stages\n\nPowerful building blocks for within-target data processing.\n\n### vs Cross-Target Pipelines\n\n| Aspect | Cross-Target | Enhanced Stages |\n|--------|--------------|------------------|\n| Scope | AWK \u2192 Python \u2192 Go | Within one target |\n| Use | Different languages | Same language |\n| Chapter | Chapter 3 | Chapter 6 |\n\n### Stage Categories\n\n| Category | Stages |\n|----------|--------|\n| Stream combination | interleave, concat, zip |\n| Flow control | fan_out, parallel, merge |\n| Filtering | filter_by, batch, unbatch |\n| Aggregation | group_by, reduce, distinct |\n| Error handling | try_catch, retry, timeout |\n| Throughput | rate_limit, throttle, buffer |\n\n### Usage\n\n```prolog\ncompile_enhanced_pipeline([\n    parse/1,\n    filter_by(is_valid),\n    distinct,\n    output/1\n], [pipeline_name(my_pipe)], Code).\n```", "answer_tailored": true}
{"pair_id": "b4-c6-stream-combination_p0", "cluster_id": "b4-c6-stream-combination", "question": "What is the difference between interleave and concat?", "question_type": "medium", "answer": "The key difference between `interleave` and `concat` is how they combine streams:\n\n**interleave(Stages)** merges streams in round-robin fashion, alternating between sources:\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)** appends streams sequentially, exhausting the first before the second:\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\nUse `interleave` when you need balanced, fair merging of multiple sources. Use `concat` for simple sequential union where order priority matters.\n\nFor more on pipeline stages, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["interleave", "concat", "zip", "stream-merging"], "original_answer": "## Stream Combination Stages\n\n### interleave vs concat\n\n**interleave(Stages)**: Round-robin alternating\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)**: Sequential append\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\n| Aspect | interleave | concat |\n|--------|------------|--------|\n| Order | Alternating | Sequential |\n| Fairness | Balanced | First priority |\n| Use case | Fair merge | Union/append |\n\n### zip(Stages)\n\nRuns multiple stages on same input, combines outputs:\n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach record gets all three enrichments.", "answer_tailored": true}
{"pair_id": "b4-c6-stream-combination_p1", "cluster_id": "b4-c6-stream-combination", "question": "How does the zip stage work?", "question_type": "medium", "answer": "The `zip` stage runs multiple pipeline stages in parallel on the same input data and combines their outputs together. \n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach input record is processed by all three stages (geocode, sentiment, and categorize), and the results are merged into a single enriched output record. This is useful when you need to apply multiple independent transformations or enrichments to the same data.\n\nFor more details on pipeline stages and workflows, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["interleave", "concat", "zip", "stream-merging"], "original_answer": "## Stream Combination Stages\n\n### interleave vs concat\n\n**interleave(Stages)**: Round-robin alternating\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)**: Sequential append\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\n| Aspect | interleave | concat |\n|--------|------------|--------|\n| Order | Alternating | Sequential |\n| Fairness | Balanced | First priority |\n| Use case | Fair merge | Union/append |\n\n### zip(Stages)\n\nRuns multiple stages on same input, combines outputs:\n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach record gets all three enrichments.", "answer_tailored": true}
{"pair_id": "b4-c6-stream-combination_p2", "cluster_id": "b4-c6-stream-combination", "question": "When should you use interleave vs concat?", "question_type": "medium", "answer": "Choose **interleave** when you need fair, balanced processing from multiple sources - it alternates items round-robin style (e.g., `[A1,A2,A3] + [B1,B2]` becomes `[A1,B1,A2,B2,A3]`). This ensures no single stream dominates.\n\nChoose **concat** when you want sequential processing - it processes the first stream completely before moving to the next (e.g., `[A1,A2] + [B1,B2,B3]` becomes `[A1,A2,B1,B2,B3]`). Use this for simple union operations or when order priority matters.\n\n**Key difference**: interleave provides fairness across streams, while concat prioritizes completing each stream in sequence.\n\nFor detailed pipeline examples, see `education/book-04-workflows/06_enhanced_pipeline_stages.md`.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["interleave", "concat", "zip", "stream-merging"], "original_answer": "## Stream Combination Stages\n\n### interleave vs concat\n\n**interleave(Stages)**: Round-robin alternating\n```\n[A1,A2,A3] + [B1,B2] \u2192 [A1,B1,A2,B2,A3]\n```\n\n**concat(Stages)**: Sequential append\n```\n[A1,A2] + [B1,B2,B3] \u2192 [A1,A2,B1,B2,B3]\n```\n\n| Aspect | interleave | concat |\n|--------|------------|--------|\n| Order | Alternating | Sequential |\n| Fairness | Balanced | First priority |\n| Use case | Fair merge | Union/append |\n\n### zip(Stages)\n\nRuns multiple stages on same input, combines outputs:\n\n```prolog\nzip([\n    geocode/1,\n    sentiment/1,\n    categorize/1\n])\n```\n\nEach record gets all three enrichments.", "answer_tailored": true}
{"pair_id": "b4-c6-flow-control_p0", "cluster_id": "b4-c6-flow-control", "question": "How do fan_out and parallel stages work?", "question_type": "medium", "answer": "The `fan_out/1` and `parallel/1` stages control how records flow through pipeline branches:\n\n**fan_out(Stages)** broadcasts each incoming record to multiple processing stages simultaneously. For example:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\nThis sends every record through validation, enrichment, and auditing paths. You typically follow it with `merge` to recombine the results.\n\n**parallel(Stages)** executes multiple stages concurrently for performance optimization:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\nThis runs both transformations at the same time, useful when you have independent operations that can benefit from concurrent execution.\n\nBoth stages are typically paired with `merge` to collect outputs from all branches. The merge operation gathers results and maintains record order where feasible.\n\nFor complete pipeline examples, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["fan-out", "parallel", "route-by", "flow-control"], "original_answer": "## Flow Control Stages\n\n### fan_out(Stages)\n\nBroadcasts each record to multiple stages:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\n\nExecutes stages concurrently for performance:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### route_by(Pred, Routes)\n\nConditional routing based on record type:\n\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\n### merge\n\nCombines results from fan_out or parallel:\n- Collects outputs from all branches\n- Maintains record order where possible", "answer_tailored": true}
{"pair_id": "b4-c6-flow-control_p1", "cluster_id": "b4-c6-flow-control", "question": "What is conditional routing with route_by?", "question_type": "medium", "answer": "The `route_by(Pred, Routes)` stage enables conditional routing of records based on their properties. It evaluates a predicate against each record to determine which processing path to follow.\n\n**Syntax:**\n```prolog\nroute_by(Pred, Routes)\n```\n\nWhere `Pred` extracts a value from the record, and `Routes` is a list of `(Value, Handler)` pairs mapping extracted values to their corresponding pipeline stages.\n\n**Example:**\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\nThis routes records to different handlers based on their type\u2014errors go to `error_handler/1`, warnings to `warning_handler/1`, and informational records to `info_handler/1`.\n\n`route_by` is particularly useful when different record categories require distinct processing logic within the same pipeline. For more on pipeline stages and flow control, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["fan-out", "parallel", "route-by", "flow-control"], "original_answer": "## Flow Control Stages\n\n### fan_out(Stages)\n\nBroadcasts each record to multiple stages:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\n\nExecutes stages concurrently for performance:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### route_by(Pred, Routes)\n\nConditional routing based on record type:\n\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\n### merge\n\nCombines results from fan_out or parallel:\n- Collects outputs from all branches\n- Maintains record order where possible", "answer_tailored": true}
{"pair_id": "b4-c6-flow-control_p2", "cluster_id": "b4-c6-flow-control", "question": "How do you merge results from parallel stages?", "question_type": "medium", "answer": "The `merge` stage combines results from parallel or fan_out stages. It collects outputs from all branches and maintains record order where possible.\n\nUse it after `parallel(Stages)` or `fan_out(Stages)`:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\nOr with fan_out:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\nThe merge stage ensures all concurrent branches complete before continuing the pipeline. For more on pipeline stage composition, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["fan-out", "parallel", "route-by", "flow-control"], "original_answer": "## Flow Control Stages\n\n### fan_out(Stages)\n\nBroadcasts each record to multiple stages:\n\n```prolog\nfan_out([validate/1, enrich/1, audit/1]),\nmerge\n```\n\n### parallel(Stages)\n\nExecutes stages concurrently for performance:\n\n```prolog\nparallel([\n    slow_transform_a/1,\n    slow_transform_b/1\n]),\nmerge\n```\n\n### route_by(Pred, Routes)\n\nConditional routing based on record type:\n\n```prolog\nroute_by(get_type, [\n    (error, error_handler/1),\n    (warning, warning_handler/1),\n    (info, info_handler/1)\n])\n```\n\n### merge\n\nCombines results from fan_out or parallel:\n- Collects outputs from all branches\n- Maintains record order where possible", "answer_tailored": true}
{"pair_id": "b4-c6-error-throughput_p0", "cluster_id": "b4-c6-error-throughput", "question": "How do you handle errors in enhanced pipelines?", "question_type": "medium", "answer": "UnifyWeaver's enhanced pipelines provide several error handling mechanisms:\n\n**try_catch(Stage, Handler)** wraps a risky operation with a fallback handler:\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n**retry(Stage, N, Opts)** attempts an operation multiple times with configurable backoff:\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n**timeout(Stage, Ms, Fallback)** sets a time limit with a fallback if exceeded:\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\nThese can be composed for robust error handling:\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```\n\nThis production-ready pipeline retries failures up to 3 times, catches any remaining errors to log them, and continues processing. Additional throughput controls like `rate_limit/2`, `throttle/1`, `buffer/1`, and `debounce/1` help manage flow and prevent overwhelming downstream systems.\n\nFor complete details, see education/book-04-workflows/06_enhanced_pipeline_stages.md.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["error-handling", "retry", "rate-limit", "resilience"], "original_answer": "## Error Handling Stages\n\n### try_catch(Stage, Handler)\n\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n### retry(Stage, N, Opts)\n\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n### timeout(Stage, Ms, Fallback)\n\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\n## Throughput Control\n\n### rate_limit(N, Per)\n\n```prolog\nrate_limit(100, second)  % 100 records/sec\n```\n\n### Other Controls\n\n| Stage | Effect |\n|-------|--------|\n| `throttle(Ms)` | Fixed delay |\n| `buffer(N)` | Collect N records |\n| `debounce(Ms)` | Wait for quiet period |\n\n### Production Pipeline\n\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```", "answer_tailored": true}
{"pair_id": "b4-c6-error-throughput_p1", "cluster_id": "b4-c6-error-throughput", "question": "What retry and timeout options are available?", "question_type": "medium", "answer": "UnifyWeaver's pipeline stages provide `retry(Stage, N, Opts)` and `timeout(Stage, Ms, Fallback)` for handling unreliable operations.\n\n**Retry Options:**\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n- Retry a stage up to N times\n- `delay(Ms)` - wait between attempts\n- `backoff(exponential)` - increase delay exponentially\n\n**Timeout Options:**\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n- Abort stage after Ms milliseconds\n- Execute Fallback stage if timeout occurs\n\nThese can be combined for robust pipelines:\n```prolog\ntry_catch(retry(process/1, 3), log_error/1)\n```\n\nFor throughput control, additional stages include `rate_limit(N, Per)`, `throttle(Ms)`, `buffer(N)`, and `debounce(Ms)`. See education/book-04-workflows/06_enhanced_pipeline_stages.md for complete pipeline examples.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["error-handling", "retry", "rate-limit", "resilience"], "original_answer": "## Error Handling Stages\n\n### try_catch(Stage, Handler)\n\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n### retry(Stage, N, Opts)\n\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n### timeout(Stage, Ms, Fallback)\n\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\n## Throughput Control\n\n### rate_limit(N, Per)\n\n```prolog\nrate_limit(100, second)  % 100 records/sec\n```\n\n### Other Controls\n\n| Stage | Effect |\n|-------|--------|\n| `throttle(Ms)` | Fixed delay |\n| `buffer(N)` | Collect N records |\n| `debounce(Ms)` | Wait for quiet period |\n\n### Production Pipeline\n\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```", "answer_tailored": true}
{"pair_id": "b4-c6-error-throughput_p2", "cluster_id": "b4-c6-error-throughput", "question": "How do you control throughput in pipelines?", "question_type": "medium", "answer": "Throughput in pipelines is controlled through several stages:\n\n**Rate Limiting**: The primary control is `rate_limit(N, Per)`, which caps processing speed:\n```prolog\nrate_limit(100, second)  % Process 100 records per second\n```\n\n**Additional Throttle Controls**:\n- `throttle(Ms)` - Introduces a fixed delay between operations\n- `buffer(N)` - Accumulates N records before proceeding\n- `debounce(Ms)` - Waits for a quiet period before processing\n\n**Example Production Configuration**:\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```\n\nThis example limits throughput to 50 records/second while batching records in groups of 100 for efficient bulk processing. The `throttle`, `buffer`, and `debounce` stages provide additional fine-tuning for specific timing and accumulation requirements.\n\nSee education/book-04-workflows/06_enhanced_pipeline_stages.md for complete pipeline stage documentation.", "answer_variant": "default", "answer_source": "education/book-04-workflows/06_enhanced_pipeline_stages.md", "topics": ["error-handling", "retry", "rate-limit", "resilience"], "original_answer": "## Error Handling Stages\n\n### try_catch(Stage, Handler)\n\n```prolog\ntry_catch(risky_transform/1, error_fallback/1)\n```\n\n### retry(Stage, N, Opts)\n\n```prolog\nretry(call_api/1, 3, [\n    delay(1000),\n    backoff(exponential)\n])\n```\n\n### timeout(Stage, Ms, Fallback)\n\n```prolog\ntimeout(slow_op/1, 5000, use_cached/1)\n```\n\n## Throughput Control\n\n### rate_limit(N, Per)\n\n```prolog\nrate_limit(100, second)  % 100 records/sec\n```\n\n### Other Controls\n\n| Stage | Effect |\n|-------|--------|\n| `throttle(Ms)` | Fixed delay |\n| `buffer(N)` | Collect N records |\n| `debounce(Ms)` | Wait for quiet period |\n\n### Production Pipeline\n\n```prolog\n[\n    try_catch(retry(process/1, 3), log_error/1),\n    rate_limit(50, second),\n    batch(100),\n    bulk_output/1\n]\n```", "answer_tailored": true}
